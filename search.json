[{"path":[]},{"path":"https://mccarthy-m-g.github.io/alda/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"CC0 1.0 Universal","title":"CC0 1.0 Universal","text":"CREATIVE COMMONS CORPORATION LAW FIRM PROVIDE LEGAL SERVICES. DISTRIBUTION DOCUMENT CREATE ATTORNEY-CLIENT RELATIONSHIP. CREATIVE COMMONS PROVIDES INFORMATION “-” BASIS. CREATIVE COMMONS MAKES WARRANTIES REGARDING USE DOCUMENT INFORMATION WORKS PROVIDED HEREUNDER, DISCLAIMS LIABILITY DAMAGES RESULTING USE DOCUMENT INFORMATION WORKS PROVIDED HEREUNDER.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/LICENSE.html","id":"statement-of-purpose","dir":"","previous_headings":"","what":"Statement of Purpose","title":"CC0 1.0 Universal","text":"laws jurisdictions throughout world automatically confer exclusive Copyright Related Rights (defined ) upon creator subsequent owner(s) (, “owner”) original work authorship /database (, “Work”). Certain owners wish permanently relinquish rights Work purpose contributing commons creative, cultural scientific works (“Commons”) public can reliably without fear later claims infringement build upon, modify, incorporate works, reuse redistribute freely possible form whatsoever purposes, including without limitation commercial purposes. owners may contribute Commons promote ideal free culture production creative, cultural scientific works, gain reputation greater distribution Work part use efforts others. /purposes motivations, without expectation additional consideration compensation, person associating CC0 Work (“Affirmer”), extent owner Copyright Related Rights Work, voluntarily elects apply CC0 Work publicly distribute Work terms, knowledge Copyright Related Rights Work meaning intended legal effect CC0 rights. Copyright Related Rights. Work made available CC0 may protected copyright related neighboring rights (“Copyright Related Rights”). Copyright Related Rights include, limited , following: right reproduce, adapt, distribute, perform, display, communicate, translate Work; moral rights retained original author(s) /performer(s); publicity privacy rights pertaining person’s image likeness depicted Work; rights protecting unfair competition regards Work, subject limitations paragraph 4(), ; rights protecting extraction, dissemination, use reuse data Work; database rights (arising Directive 96/9/EC European Parliament Council 11 March 1996 legal protection databases, national implementation thereof, including amended successor version directive); similar, equivalent corresponding rights throughout world based applicable law treaty, national implementations thereof. Waiver. greatest extent permitted , contravention , applicable law, Affirmer hereby overtly, fully, permanently, irrevocably unconditionally waives, abandons, surrenders Affirmer’s Copyright Related Rights associated claims causes action, whether now known unknown (including existing well future claims causes action), Work () territories worldwide, (ii) maximum duration provided applicable law treaty (including future time extensions), (iii) current future medium number copies, (iv) purpose whatsoever, including without limitation commercial, advertising promotional purposes (“Waiver”). Affirmer makes Waiver benefit member public large detriment Affirmer’s heirs successors, fully intending Waiver shall subject revocation, rescission, cancellation, termination, legal equitable action disrupt quiet enjoyment Work public contemplated Affirmer’s express Statement Purpose. Public License Fallback. part Waiver reason judged legally invalid ineffective applicable law, Waiver shall preserved maximum extent permitted taking account Affirmer’s express Statement Purpose. addition, extent Waiver judged Affirmer hereby grants affected person royalty-free, non transferable, non sublicensable, non exclusive, irrevocable unconditional license exercise Affirmer’s Copyright Related Rights Work () territories worldwide, (ii) maximum duration provided applicable law treaty (including future time extensions), (iii) current future medium number copies, (iv) purpose whatsoever, including without limitation commercial, advertising promotional purposes (“License”). License shall deemed effective date CC0 applied Affirmer Work. part License reason judged legally invalid ineffective applicable law, partial invalidity ineffectiveness shall invalidate remainder License, case Affirmer hereby affirms () exercise remaining Copyright Related Rights Work (ii) assert associated claims causes action respect Work, either case contrary Affirmer’s express Statement Purpose. Limitations Disclaimers. trademark patent rights held Affirmer waived, abandoned, surrendered, licensed otherwise affected document. Affirmer offers Work -makes representations warranties kind concerning Work, express, implied, statutory otherwise, including without limitation warranties title, merchantability, fitness particular purpose, non infringement, absence latent defects, accuracy, present absence errors, whether discoverable, greatest extent permissible applicable law. Affirmer disclaims responsibility clearing rights persons may apply Work use thereof, including without limitation person’s Copyright Related Rights Work. , Affirmer disclaims responsibility obtaining necessary consents, permissions rights required use Work. Affirmer understands acknowledges Creative Commons party document duty obligation respect CC0 use Work.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-10.html","id":"the-life-table","dir":"Articles","previous_headings":"","what":"10.1 The Life Table","title":"Chapter 10: Describing discrete-time event occurrence data","text":"Table 10.1, page 327:","code":"# A life table is the tabular form of a survival curve, so begin by fitting a # Kaplan-Meir curve to the data. teachers_fit <- survfit(Surv(years, 1 - censor) ~ 1, data = teachers)  table_10.1 <- teachers_fit |>   # Add a starting time (time 0) for the table.   survfit0() |>   tidy() |>   # The summary of the fit gives most of what we want, but to match Table 10.1   # we need to do a little more wrangling.   select(-c(std.error:conf.low)) |>   mutate(     interval = paste0(\"[\", time, \", \", time + 1, \")\"),     haz.estimate = n.event / n.risk   ) |>   rename(year = time, surv.estimate = estimate) |>   relocate(     year, interval, n.risk, n.event, n.censor, haz.estimate, surv.estimate   )  table_10.1 #> # A tibble: 13 × 7 #>     year interval n.risk n.event n.censor haz.estimate surv.estimate #>    <dbl> <chr>     <dbl>   <dbl>    <dbl>        <dbl>         <dbl> #>  1     0 [0, 1)     3941       0        0       0              1     #>  2     1 [1, 2)     3941     456        0       0.116          0.884 #>  3     2 [2, 3)     3485     384        0       0.110          0.787 #>  4     3 [3, 4)     3101     359        0       0.116          0.696 #>  5     4 [4, 5)     2742     295        0       0.108          0.621 #>  6     5 [5, 6)     2447     218        0       0.0891         0.566 #>  7     6 [6, 7)     2229     184        0       0.0825         0.519 #>  8     7 [7, 8)     2045     123      280       0.0601         0.488 #>  9     8 [8, 9)     1642      79      307       0.0481         0.464 #> 10     9 [9, 10)    1256      53      255       0.0422         0.445 #> 11    10 [10, 11)    948      35      265       0.0369         0.428 #> 12    11 [11, 12)    648      16      241       0.0247         0.418 #> 13    12 [12, 13)    391       5      386       0.0128         0.412"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-10.html","id":"a-framework-for-characterizing-the-distribution-of-discrete-time-event-occurrence-data","dir":"Articles","previous_headings":"","what":"10.2 A Framework for Characterizing the Distribution of Discrete-Time Event Occurrence Data","title":"Chapter 10: Describing discrete-time event occurrence data","text":"Figure 10.1, page 333:","code":"ggplot(table_10.1, aes(x = year, y = haz.estimate)) +   geom_line() +   scale_x_continuous(breaks = 0:13, limits = c(1, 13)) +   scale_y_continuous(breaks = c(0, .05, .1, .15), limits = c(0, .15)) +   coord_cartesian(xlim = c(0, 13)) #> Warning: Removed 1 row containing missing values or values outside the scale range #> (`geom_line()`). # First interpolate median lifetime median_lifetime <- table_10.1 |>   # Get the row indices for the first survival estimate immediately below and   # immediately above 0.5. This will only work correctly if the values are in   # descending order, otherwise min() and max() must be swapped. By default, the   # survival estimates are in descending order, however, I've added the   # redundant step of ensuring they are here for demonstration purposes.   arrange(desc(surv.estimate)) |>   slice(min(which(surv.estimate <= .5)), max(which(surv.estimate >= .5))) |>   select(year, surv.estimate) |>   # Linearly interpolate between the two values of the survival estimates that   # bracket .5 following Miller's (1981) equation.   summarise(     year =       min(year) +       ((max(surv.estimate) - .5) /        (max(surv.estimate) - min(surv.estimate))) *       ((min(year) + 1) - min(year)),     surv.estimate = .5   )  ggplot(table_10.1, aes(x = year, y = surv.estimate)) +   geom_line() +   geom_segment(     aes(xend = year, y = 0, yend = .5), data = median_lifetime, linetype = 2   ) +   geom_segment(     aes(xend = 0, yend = .5), data = median_lifetime, linetype = 2   ) +   scale_x_continuous(breaks = 0:13) +   scale_y_continuous(breaks = c(0, .5, 1), limits = c(0, 1)) +   coord_cartesian(xlim = c(0, 13))"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-10.html","id":"developing-intuition-about-hazard-functions-survivor-functions-and-median-lifetimes","dir":"Articles","previous_headings":"","what":"10.3 Developing Intuition About Hazard Functions, Survivor Functions, and Median Lifetimes","title":"Chapter 10: Describing discrete-time event occurrence data","text":"Figure 10.2, page 340:","code":"relapse_fit <- survfit(Surv(weeks, 1 - censor) ~ 1, data = cocaine_relapse_1) relapse_tidy <- tidy(relapse_fit) relapse_summary <- glance(relapse_fit)"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-10.html","id":"quantifying-the-effects-of-sampling-variation","dir":"Articles","previous_headings":"","what":"10.4 Quantifying the Effects of Sampling Variation","title":"Chapter 10: Describing discrete-time event occurrence data","text":"Table 10.2, page 349:","code":"summary(teachers_fit) #> Call: survfit(formula = Surv(years, 1 - censor) ~ 1, data = teachers) #>  #>  time n.risk n.event survival std.err lower 95% CI upper 95% CI #>     1   3941     456    0.884 0.00510        0.874        0.894 #>     2   3485     384    0.787 0.00652        0.774        0.800 #>     3   3101     359    0.696 0.00733        0.682        0.710 #>     4   2742     295    0.621 0.00773        0.606        0.636 #>     5   2447     218    0.566 0.00790        0.550        0.581 #>     6   2229     184    0.519 0.00796        0.504        0.535 #>     7   2045     123    0.488 0.00796        0.472        0.504 #>     8   1642      79    0.464 0.00800        0.449        0.480 #>     9   1256      53    0.445 0.00811        0.429        0.461 #>    10    948      35    0.428 0.00827        0.412        0.445 #>    11    648      16    0.418 0.00848        0.401        0.435 #>    12    391       5    0.412 0.00870        0.396        0.430  teachers_fit |>   tidy() |>   mutate(     # The tidy() method for survfit objects returns the standard error for the     # cumulative hazard instead of the survival probability. Multiplying the     # survival estimate with the cumulative hazard's standard error will return     # the standard error for the survival probability. Note that it is unlikely     # the tidy() method will ever change to return the the standard error for     # the survival probability instead. See:     # - https://github.com/tidymodels/broom/pull/1162     # Other transformations of the survival probability can be found here:     # - https://stat.ethz.ch/pipermail/r-help/2014-June/376247.html     surv.std.error = estimate * std.error,     haz.estimate = n.event / n.risk,     haz.std.error = sqrt(haz.estimate * (1 - haz.estimate) / n.risk),     sqrt = (std.error)^2 / (estimate)^2   ) |>   select(     year = time,     n.risk,     haz.estimate,     haz.std.error,     surv.estimate = estimate,     sqrt,     surv.std.error   ) #> # A tibble: 12 × 7 #>     year n.risk haz.estimate haz.std.error surv.estimate     sqrt surv.std.error #>    <dbl>  <dbl>        <dbl>         <dbl>         <dbl>    <dbl>          <dbl> #>  1     1   3941       0.116        0.00510         0.884  4.25e-5        0.00510 #>  2     2   3485       0.110        0.00530         0.787  1.11e-4        0.00652 #>  3     3   3101       0.116        0.00575         0.696  2.29e-4        0.00733 #>  4     4   2742       0.108        0.00592         0.621  4.02e-4        0.00773 #>  5     5   2447       0.0891       0.00576         0.566  6.09e-4        0.00790 #>  6     6   2229       0.0825       0.00583         0.519  8.74e-4        0.00796 #>  7     7   2045       0.0601       0.00526         0.488  1.12e-3        0.00796 #>  8     8   1642       0.0481       0.00528         0.464  1.38e-3        0.00800 #>  9     9   1256       0.0422       0.00567         0.445  1.68e-3        0.00811 #> 10    10    948       0.0369       0.00612         0.428  2.03e-3        0.00827 #> 11    11    648       0.0247       0.00610         0.418  2.36e-3        0.00848 #> 12    12    391       0.0128       0.00568         0.412  2.62e-3        0.00870"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-10.html","id":"a-simple-and-useful-strategy-for-constructing-the-life-table","dir":"Articles","previous_headings":"","what":"10.5 A Simple and Useful Strategy for Constructing the Life Table","title":"Chapter 10: Describing discrete-time event occurrence data","text":"Figure 10.4, page 353: Table 10.3, page 355:","code":"filter(teachers, id %in% c(20, 126, 129)) #> # A tibble: 3 × 3 #>   id    years censor #>   <fct> <dbl>  <dbl> #> 1 20        3      0 #> 2 126      12      0 #> 3 129      12      1  teachers_pp <- teachers |>   reframe(     year = 1:max(years),     event = if_else(year == years & censor == 0, 1, 0),     .by = id   )  teachers_pp |>   filter(id %in% c(20, 126, 129)) |>   print(n = 27) #> # A tibble: 27 × 3 #>    id     year event #>    <fct> <int> <dbl> #>  1 20        1     0 #>  2 20        2     0 #>  3 20        3     1 #>  4 126       1     0 #>  5 126       2     0 #>  6 126       3     0 #>  7 126       4     0 #>  8 126       5     0 #>  9 126       6     0 #> 10 126       7     0 #> 11 126       8     0 #> 12 126       9     0 #> 13 126      10     0 #> 14 126      11     0 #> 15 126      12     1 #> 16 129       1     0 #> 17 129       2     0 #> 18 129       3     0 #> 19 129       4     0 #> 20 129       5     0 #> 21 129       6     0 #> 22 129       7     0 #> 23 129       8     0 #> 24 129       9     0 #> 25 129      10     0 #> 26 129      11     0 #> 27 129      12     0 teachers_pp |>   group_by(year) |>   count(event) |>   pivot_wider(names_from = event, names_prefix = \"event_\", values_from = n) |>   mutate(     total = event_0 + event_1,     p.event_1 = event_1 / total   ) #> # A tibble: 12 × 5 #> # Groups:   year [12] #>     year event_0 event_1 total p.event_1 #>    <int>   <int>   <int> <int>     <dbl> #>  1     1    3485     456  3941    0.116  #>  2     2    3101     384  3485    0.110  #>  3     3    2742     359  3101    0.116  #>  4     4    2447     295  2742    0.108  #>  5     5    2229     218  2447    0.0891 #>  6     6    2045     184  2229    0.0825 #>  7     7    1922     123  2045    0.0601 #>  8     8    1563      79  1642    0.0481 #>  9     9    1203      53  1256    0.0422 #> 10    10     913      35   948    0.0369 #> 11    11     632      16   648    0.0247 #> 12    12     386       5   391    0.0128"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-11.html","id":"toward-a-statistical-model-for-discretetime-hazard","dir":"Articles","previous_headings":"","what":"11.1 Toward a Statistical Model for DiscreteTime Hazard","title":"Chapter 11: Fitting basic discrete-time hazard models","text":"Several examples chapter rely following: Figure 11.1, page 359:  Table 11.1, page 360: Figure 11.2, page 363:  Figure 11.3, page 366:","code":"first_sex_fit <- survfit(Surv(grade, 1 - censor) ~ 1, data = first_sex)  first_sex_pt <- c(0, 1) |>   map_dfr(     \\(.x) {       first_sex_fit_subset <- update(         first_sex_fit, subset = (parental_transition == .x)       )              first_sex_fit_subset |>         survfit0(start.time = 6) |>         tidy() |>         rename(survival_probability = estimate) |>         mutate(           hazard_probability = n.event / n.risk,           odds = hazard_probability / (1 - hazard_probability),           log_odds = log(odds)         ) |>         select(-starts_with(\"conf\"), -std.error) |>         rename(grade = time) |>         pivot_longer(           cols = c(survival_probability, hazard_probability, odds, log_odds),           values_to = \"estimate\"         ) |>         # The figure doesn't include data for grade 6 in the hazard function.         filter(           !(name %in% c(\"hazard_probability\", \"odds\", \"log_odds\") & grade == 6)         )     },     .id = \"parental_transition\"   ) first_sex_pt |>   filter(name %in% c(\"survival_probability\", \"hazard_probability\")) |>   ggplot(aes(x = grade, y = estimate, colour = parental_transition)) +     geom_hline(       aes(yintercept = .5),       data = tibble(name = \"survival_probability\"),       alpha = .25,       linetype = 2     ) +     geom_line() +     scale_x_continuous(breaks = 6:12) +     coord_cartesian(xlim = c(6, 12)) +     facet_wrap(vars(name), ncol = 1, scales = \"free_y\") +     ggh4x::facetted_pos_scales(       y = list(         name == \"hazard_probability\" ~ scale_y_continuous(limits = c(0, .5)),         name == \"survival_probability\" ~           scale_y_continuous(breaks = c(0, .5, 1), limits = c(0, 1))       )     ) # First two sections of the table first_sex_pt |>   filter(grade != 6, !(name %in% c(\"odds\", \"log_odds\"))) |>   pivot_wider(names_from = name, values_from = estimate) |>   select(everything(), -n.censor, hazard_probability, survival_probability) #> # A tibble: 12 × 6 #>    parental_transition grade n.risk n.event survival_probability #>    <chr>               <dbl>  <dbl>   <dbl>                <dbl> #>  1 1                       7     72       2                0.972 #>  2 1                       8     70       2                0.944 #>  3 1                       9     68       8                0.833 #>  4 1                      10     60       8                0.722 #>  5 1                      11     52      10                0.583 #>  6 1                      12     42       8                0.472 #>  7 2                       7    108      13                0.880 #>  8 2                       8     95       5                0.833 #>  9 2                       9     90      16                0.685 #> 10 2                      10     74      21                0.491 #> 11 2                      11     53      15                0.352 #> 12 2                      12     38      18                0.185 #> # ℹ 1 more variable: hazard_probability <dbl>  # Last section first_sex_fit |>   tidy() |>   rename(survival_probability = estimate) |>   mutate(     hazard_probability = n.event / n.risk,     .before = survival_probability   ) |>   select(-starts_with(\"conf\"), -std.error, -n.censor) |>   rename(grade = time) #> # A tibble: 6 × 5 #>   grade n.risk n.event hazard_probability survival_probability #>   <dbl>  <dbl>   <dbl>              <dbl>                <dbl> #> 1     7    180      15             0.0833                0.917 #> 2     8    165       7             0.0424                0.878 #> 3     9    158      24             0.152                 0.744 #> 4    10    134      29             0.216                 0.583 #> 5    11    105      25             0.238                 0.444 #> 6    12     80      26             0.325                 0.3 first_sex_pt |>   filter(name %in% c(\"hazard_probability\", \"odds\", \"log_odds\")) |>   mutate(     name = factor(name, levels = c(\"hazard_probability\", \"odds\", \"log_odds\"))   ) |>   ggplot(aes(x = grade, y = estimate, colour = parental_transition)) +     geom_line() +     scale_x_continuous(breaks = 6:12) +     coord_cartesian(xlim = c(6, 12)) +     facet_wrap(vars(name), ncol = 1, scales = \"free_y\") +     ggh4x::facetted_pos_scales(       y = list(         name %in% c(\"hazard_probability\", \"odds\") ~           scale_y_continuous(limits = c(0, 1)),         name == \"log_odds\" ~ scale_y_continuous(limits = c(-4, 0))       )     ) # Transform to person-period format. first_sex_pp <- first_sex |>   rename(grades = grade) |>   reframe(     grade = 7:max(grades),     event = if_else(grade == grades & censor == 0, 1, 0),     parental_transition,     parental_antisociality,     .by = id   )  # Fit models for each panel. first_sex_fit_11.3a <- glm(   event ~ parental_transition, family = \"binomial\", data = first_sex_pp ) first_sex_fit_11.3b <- update(first_sex_fit_11.3a, . ~ . + grade) first_sex_fit_11.3c <- update(first_sex_fit_11.3a, . ~ . + factor(grade))  # Plot: map_df(   list(a = first_sex_fit_11.3a, b = first_sex_fit_11.3b, c = first_sex_fit_11.3c),   \\(.x) augment(.x, newdata = first_sex_pp),   .id = \"model\" ) |>   ggplot(aes(x = grade, y = .fitted, colour = factor(parental_transition))) +   geom_line() +   geom_point(     aes(y = estimate),     data = first_sex_pt |>       mutate(parental_transition = as.numeric(parental_transition) - 1) |>       filter(name == \"log_odds\")   ) +   coord_cartesian(ylim = c(-4, 0)) +   facet_wrap(vars(model), ncol = 1, labeller = label_both) +   labs(     y = \"logit(hazard)\",     colour = \"parental_transition\"   )"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-11.html","id":"a-formal-representation-of-the-population-discrete-time-hazard-model","dir":"Articles","previous_headings":"","what":"11.2 A Formal Representation of the Population Discrete-Time Hazard Model","title":"Chapter 11: Fitting basic discrete-time hazard models","text":"Figure 11.4, page 374:","code":"# Panel A: first_sex_fit_11.3c |>   augment(newdata = first_sex_pp) |>   ggplot(aes(x = grade, y = .fitted, colour = factor(parental_transition))) +   geom_line() +   coord_cartesian(ylim = c(-4, 0)) # Panel B: first_sex_fit_11.4b <- update(   first_sex_fit_11.3c,   . ~ . + parental_transition * factor(grade) )  first_sex_fit_11.4b |>   augment(newdata = first_sex_pp) |>   ggplot(aes(x = grade, y = exp(.fitted), colour = factor(parental_transition))) +   geom_line() +   coord_cartesian(ylim = c(0, 1)) # Panel C: first_sex_fit_11.4b |>   augment(newdata = first_sex_pp, type.predict = \"response\") |>   ggplot(aes(x = grade, y = .fitted, colour = factor(parental_transition))) +   geom_line()"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-11.html","id":"fitting-a-discrete-time-hazard-model-to-data","dir":"Articles","previous_headings":"","what":"11.3 Fitting a Discrete-Time Hazard Model to Data","title":"Chapter 11: Fitting basic discrete-time hazard models","text":"Figure 11.5: Table 11.3, page 386:","code":"model_A <- glm(   event ~ factor(grade) - 1, family = \"binomial\", data = first_sex_pp ) model_B <- update(model_A, . ~ . + parental_transition) model_C <- update(model_A, . ~ . + parental_antisociality) model_D <- update(model_B, . ~ . + parental_antisociality)  anova(model_B) #> Analysis of Deviance Table #>  #> Model: binomial, link: logit #>  #> Response: event #>  #> Terms added sequentially (first to last) #>  #>  #>                     Df Deviance Resid. Df Resid. Dev  Pr(>Chi)     #> NULL                                  822    1139.53               #> factor(grade)        6   487.58       816     651.96 < 2.2e-16 *** #> parental_transition  1    17.29       815     634.66 3.203e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 anova(model_C) #> Analysis of Deviance Table #>  #> Model: binomial, link: logit #>  #> Response: event #>  #> Terms added sequentially (first to last) #>  #>  #>                        Df Deviance Resid. Df Resid. Dev  Pr(>Chi)     #> NULL                                     822    1139.53               #> factor(grade)           6   487.58       816     651.96 < 2.2e-16 *** #> parental_antisociality  1    14.79       815     637.17 0.0001204 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 # Deviance tests are sequential so the order of terms matters. To test # parental_transition and parental_antisociality, the model needs to be # fit twice, once with each as the last term. anova(update(model_C, . ~ . + parental_transition)) #> Analysis of Deviance Table #>  #> Model: binomial, link: logit #>  #> Response: event #>  #> Terms added sequentially (first to last) #>  #>  #>                        Df Deviance Resid. Df Resid. Dev  Pr(>Chi)     #> NULL                                     822    1139.53               #> factor(grade)           6   487.58       816     651.96 < 2.2e-16 *** #> parental_antisociality  1    14.79       815     637.17 0.0001204 *** #> parental_transition     1     8.02       814     629.15 0.0046222 **  #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 anova(model_D) #> Analysis of Deviance Table #>  #> Model: binomial, link: logit #>  #> Response: event #>  #> Terms added sequentially (first to last) #>  #>  #>                        Df Deviance Resid. Df Resid. Dev  Pr(>Chi)     #> NULL                                     822    1139.53               #> factor(grade)           6   487.58       816     651.96 < 2.2e-16 *** #> parental_transition     1    17.29       815     634.66 3.203e-05 *** #> parental_antisociality  1     5.51       814     629.15   0.01886 *   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-11.html","id":"interpreting-parameter-estimates","dir":"Articles","previous_headings":"","what":"11.4 Interpreting Parameter Estimates","title":"Chapter 11: Fitting basic discrete-time hazard models","text":"Table 11.4, page 388:","code":"model_A |>   tidy() |>   select(term, estimate) |>   mutate(     odds = exp(estimate),     hazard = 1 / (1 + exp(-estimate))   ) #> # A tibble: 6 × 4 #>   term            estimate   odds hazard #>   <chr>              <dbl>  <dbl>  <dbl> #> 1 factor(grade)7    -2.40  0.0909 0.0833 #> 2 factor(grade)8    -3.12  0.0443 0.0424 #> 3 factor(grade)9    -1.72  0.179  0.152  #> 4 factor(grade)10   -1.29  0.276  0.216  #> 5 factor(grade)11   -1.16  0.313  0.238  #> 6 factor(grade)12   -0.731 0.481  0.325"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-11.html","id":"displaying-fitted-hazard-and-survivor-functions","dir":"Articles","previous_headings":"","what":"11.5 Displaying Fitted Hazard and Survivor Functions","title":"Chapter 11: Fitting basic discrete-time hazard models","text":"Table 11.5, page 392: Figure 11.6, page 393:  Figure 11.7, page 395:","code":"model_B_tidy <- model_B |>   augment(     newdata = expand_grid(grade = 7:12, parental_transition = 0:1)   ) |>   mutate(     hazard = 1 / (1 + exp(-.fitted)),     survival = cumprod(1 - hazard),     .by = parental_transition   )  model_B_tidy #> # A tibble: 12 × 5 #>    grade parental_transition .fitted hazard survival #>    <int>               <int>   <dbl>  <dbl>    <dbl> #>  1     7                   0  -2.99  0.0477    0.952 #>  2     7                   1  -2.12  0.107     0.893 #>  3     8                   0  -3.70  0.0241    0.929 #>  4     8                   1  -2.83  0.0559    0.843 #>  5     9                   0  -2.28  0.0927    0.843 #>  6     9                   1  -1.41  0.197     0.677 #>  7    10                   0  -1.82  0.139     0.726 #>  8    10                   1  -0.949 0.279     0.488 #>  9    11                   0  -1.65  0.161     0.609 #> 10    11                   1  -0.781 0.314     0.335 #> 11    12                   0  -1.18  0.235     0.466 #> 12    12                   1  -0.305 0.424     0.193 # FIXME: should use survfit0() for the survival panel so time starts at 6. model_B_tidy |>   pivot_longer(cols = .fitted:survival) |>   ggplot(aes(x = grade, y = value, colour = factor(parental_transition))) +   geom_line() +   facet_wrap(vars(name), ncol = 1, scales = \"free_y\") prototypical_males <- tibble(   id = rep(1:6, times = length(7:12)),   expand_grid(     grade = 7:12,     parental_transition = c(0, 1),     parental_antisociality = -1:1   ) )  prototypical_first_sex <- tibble(   log_odds = predict(     model_D,     prototypical_males   ),   hazard = 1 / (1 + exp(-log_odds)) )  grade_six <- tibble(   id = 1:6,   grade = 6,   expand_grid(     parental_transition = c(0, 1),     parental_antisociality = -1:1   ),   log_odds = NA,   hazard = NA,   survival = 1 )  prototypical_males |>   bind_cols(prototypical_first_sex) |>   mutate(survival = cumprod(1 - hazard), .by = id) |>   add_row(grade_six) |>   pivot_longer(cols = c(hazard, survival)) |>   ggplot(aes(x = grade, y = value, group = id)) +     geom_line(       aes(         colour = factor(parental_antisociality),         linetype = factor(parental_transition)       )     ) +     scale_colour_grey(start = 0, end = 0.75) +     facet_wrap(       vars(name),       ncol = 1,       scales = \"free_y\"     ) #> Warning: Removed 6 rows containing missing values or values outside the scale range #> (`geom_line()`)."},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-12.html","id":"alternative-specifications-for-the-main-effect-of-time","dir":"Articles","previous_headings":"","what":"12.1 Alternative Specifications for the “Main Effect of TIME”","title":"Chapter 12: Extending the discrete-time hazard model","text":"Table 12.2, page 413: Figure 12.1, page 414:","code":"# Convert to person-period format tenure_pp <- tenure |>   reframe(     year = 1:max(years),     event = if_else(year == years & censor == 0, 1, 0),     .by = id   ) |>   mutate(     temp_year = year,     temp_dummy = 1   ) |>   pivot_wider(     names_from = temp_year,     names_prefix = \"year_\",     values_from = temp_dummy,     values_fill = 0   )  # Fit models tenure_fit_general <- glm(   event ~ factor(year), family = \"binomial\", data = tenure_pp )  tenure_fit_constant <- glm(   event ~ 1, family = \"binomial\", data = tenure_pp )  tenure_fit_linear    <- update(tenure_fit_constant, . ~ year) tenure_fit_quadratic <- update(tenure_fit_linear, . ~ . + I(year^2)) tenure_fit_cubic     <- update(tenure_fit_quadratic, . ~ . + I(year^3)) tenure_fit_order_4   <- update(tenure_fit_cubic, . ~ . + I(year^4)) tenure_fit_order_5   <- update(tenure_fit_order_4, . ~ . + I(year^5))  # Compare anova(   tenure_fit_constant,   tenure_fit_linear,   tenure_fit_quadratic,   tenure_fit_cubic,   tenure_fit_order_4,   tenure_fit_order_5 ) #> Analysis of Deviance Table #>  #> Model 1: event ~ 1 #> Model 2: event ~ year #> Model 3: event ~ year + I(year^2) #> Model 4: event ~ year + I(year^2) + I(year^3) #> Model 5: event ~ year + I(year^2) + I(year^3) + I(year^4) #> Model 6: event ~ year + I(year^2) + I(year^3) + I(year^4) + I(year^5) #>   Resid. Df Resid. Dev Df Deviance  Pr(>Chi)     #> 1      1473    1037.57                           #> 2      1472     867.46  1  170.103 < 2.2e-16 *** #> 3      1471     836.30  1   31.158 2.379e-08 *** #> 4      1470     833.17  1    3.132   0.07679 .   #> 5      1469     832.74  1    0.430   0.51208     #> 6      1468     832.73  1    0.011   0.91831     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 tenure_fit_trajectories <- map_df(   list(     constant = tenure_fit_constant,     linear = tenure_fit_linear,     quadratic = tenure_fit_quadratic,     cubic = tenure_fit_cubic,     general = tenure_fit_general   ),   \\(.x) {     augment(.x, newdata = tibble(year = 1:9))    },   .id = \"model\" )  tenure_fit_trajectories |>   mutate(     model = factor(       model, levels = c(\"constant\", \"linear\", \"quadratic\", \"cubic\", \"general\")     ),     hazard = if_else(       model %in% c(\"quadratic\", \"general\"), 1 / (1 + exp(-.fitted)), NA     ),     survival = if_else(       model %in% c(\"quadratic\", \"general\"), cumprod(1 - hazard), NA     ),     .by = model   ) |>   rename(logit_hazard = .fitted) |>   pivot_longer(cols = logit_hazard:survival, names_to = \"estimate\") |>   mutate(estimate = factor(     estimate, levels = c(\"logit_hazard\", \"hazard\", \"survival\"))   ) |>   ggplot(aes(x = year, y = value, colour = model)) +     geom_line() +     scale_color_brewer(type = \"qual\", palette = \"Dark2\") +     scale_x_continuous(breaks = 1:9) +     facet_wrap(vars(estimate), scales = \"free_y\", labeller = label_both) #> Warning: Removed 54 rows containing missing values or values outside the scale range #> (`geom_line()`)."},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-12.html","id":"using-the-complementary-log-log-link-to-specify-a-discrete-time-hazard-model","dir":"Articles","previous_headings":"","what":"12.2 Using the Complementary Log-Log Link to Specify a Discrete-Time Hazard Model","title":"Chapter 12: Extending the discrete-time hazard model","text":"Figure 12.2: Figure 12.3, page 423:  Table 12.3, page 424:","code":"first_sex_pp <- first_sex |>   rename(grades = grade) |>   reframe(     grade = 7:max(grades),     event = if_else(grade == grades & censor == 0, 1, 0),     parental_transition,     parental_antisociality,     .by = id   )  # The nested map_() is used here so we can get an ID column for both the # link function and the subset. map_dfr(   list(logit = \"logit\", cloglog = \"cloglog\"),   \\(.x) {     map_dfr(       list(`0` = 0, `1` = 1),       \\(.y) {         first_sex_fit <- glm(           event ~ factor(grade),           family = binomial(link = .x),           data = first_sex_pp,           subset = c(parental_transition == .y)         )                  augment(first_sex_fit, newdata = tibble(grade = 7:12))       },       .id = \"parental_transition\"     )   },   .id = \"link\" ) |>   ggplot(     aes(x = grade, y = .fitted, colour = parental_transition, linetype = link)   ) +     geom_line() map_dfr(   list(cloglog = \"cloglog\", logit = \"logit\"),   \\(.x) {     first_sex_fit <- glm(       event ~ -1 + factor(grade) + parental_transition,       family = binomial(link = .x),       data = first_sex_pp     )          first_sex_fit |>       tidy() |>       select(term, estimate) |>       mutate(         base_hazard = case_when(           .x == \"logit\" & term != \"parental_transition\" ~             1 / (1 + exp(-estimate)),           .x == \"cloglog\" & term != \"parental_transition\" ~             1 - exp(-exp(estimate))         )       )   },   .id = \"link\" ) |>   pivot_wider(names_from = link, values_from = c(estimate, base_hazard)) #> # A tibble: 7 × 5 #>   term     estimate_cloglog estimate_logit base_hazard_cloglog base_hazard_logit #>   <chr>               <dbl>          <dbl>               <dbl>             <dbl> #> 1 factor(…           -2.97          -2.99               0.0498            0.0477 #> 2 factor(…           -3.66          -3.70               0.0254            0.0241 #> 3 factor(…           -2.32          -2.28               0.0940            0.0927 #> 4 factor(…           -1.90          -1.82               0.139             0.139  #> 5 factor(…           -1.76          -1.65               0.158             0.161  #> 6 factor(…           -1.34          -1.18               0.230             0.235  #> 7 parenta…            0.785          0.874             NA                NA"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-12.html","id":"time-varying-predictors","dir":"Articles","previous_headings":"","what":"12.3 Time-Varying Predictors","title":"Chapter 12: Extending the discrete-time hazard model","text":"Figure 12.4, page 432:   Figure 12.5, page 437:","code":"first_depression_fit <- glm(   depressive_episode ~ poly(I(period - 18), 3, raw = TRUE) + parental_divorce,   family = binomial(link = \"logit\"),   data = first_depression_1 )  # When a predictor enters the model as part of a matrix of covariates, such as # with stats::poly(), it is represented in augment() as a matrix column. A simple # workaround to get the predictor on its original scale as a vector is to pass # the original data to augment(). first_depression_predictions <- first_depression_fit |>   augment(data = first_depression_1) |>   mutate(hazard = 1 / (1 + exp(-.fitted)))  # Proportions of the risk set at each age who experienced an initial depressive # episode at that age, as function of their parental divorce status at that age. first_depression_proportions <- first_depression_1 |>   group_by(period, parental_divorce) |>   summarise(     total = n(),     event = sum(depressive_episode),     proportion = event / total,     proportion = if_else(proportion == 0, NA, proportion),     logit = log(proportion / (1 - proportion))   ) #> `summarise()` has grouped output by 'period'. You can override using the #> `.groups` argument.  # Top plot ggplot(mapping = aes(x = period, colour = factor(parental_divorce))) +   geom_line(     aes(y = hazard), data = first_depression_predictions   ) +   geom_point(     aes(y = proportion), data = first_depression_proportions   ) +   scale_x_continuous(breaks = seq(0, 40, by = 5), limits = c(0, 40)) +   scale_y_continuous(limits = c(0, 0.06)) #> Warning: Removed 14 rows containing missing values or values outside the scale range #> (`geom_point()`). # Bottom plot ggplot(mapping = aes(x = period, colour = factor(parental_divorce))) +   geom_line(     aes(y = .fitted), data = first_depression_predictions   ) +   geom_point(     aes(y = logit), data = first_depression_proportions   ) +   scale_x_continuous(breaks = seq(0, 40, by = 5), limits = c(0, 40)) +   scale_y_continuous(breaks = seq(-8, -2, by = 1), limits = c(-8, -2)) #> Warning: Removed 14 rows containing missing values or values outside the scale range #> (`geom_point()`). first_depression_fit_2 <- update(first_depression_fit, . ~ . + female)  first_depression_fit_2 |>   augment(     newdata = expand_grid(       period = 4:39, parental_divorce = c(0, 1), female = c(0, 1)     )   ) |>   mutate(     female = factor(female),     parental_divorce = factor(parental_divorce),     hazard = 1 / (1 + exp(-.fitted)),     survival = cumprod(1 - hazard),     .by = c(female, parental_divorce)   ) |>   pivot_longer(cols = c(hazard, survival), names_to = \"estimate\") |>   ggplot(aes(x = period, y = value, linetype = female, colour = parental_divorce)) +     geom_line() +     facet_wrap(vars(estimate), ncol = 1, scales = \"free_y\") +     scale_x_continuous(breaks = seq(0, 40, by = 5), limits = c(0, 40))  +     ggh4x::facetted_pos_scales(       y = list(         estimate == \"hazard\" ~ scale_y_continuous(limits = c(0, .04)),         estimate == \"survival\" ~ scale_y_continuous(limits = c(0, 1))       )     )"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-12.html","id":"the-linear-additivity-assumption-uncovering-violations-and-simple-solutions","dir":"Articles","previous_headings":"","what":"12.4 The Linear Additivity Assumption: Uncovering Violations and Simple Solutions","title":"Chapter 12: Extending the discrete-time hazard model","text":"Figure 12.6, page 445:   Table 12.4, page 449:","code":"# Raw first_arrest |>   group_by(period, abused, black) |>   summarise(     total = n(),     event = sum(event),     proportion = event / total,     proportion = if_else(proportion == 0, NA, proportion),     logit = log(proportion / (1 - proportion))   ) |>   ungroup() |>   mutate(across(c(abused, black), factor)) |>   na.omit() |>   ggplot(aes(x = period, y = logit, colour = abused, group = abused)) +     geom_line() +     scale_x_continuous(breaks = 7:19, limits = c(7, 19)) +     scale_y_continuous(limits = c(-7, -2)) +     facet_wrap(vars(black), labeller = label_both) #> `summarise()` has grouped output by 'period', 'abused'. You can override using #> the `.groups` argument. # Model first_arrest_fit <- glm(   event ~ factor(period) + abused + black + abused:black,   family = binomial(link = \"logit\"),   data = first_arrest )  first_arrest_fit |>   augment(     newdata = expand_grid(period = 8:18, abused = c(0, 1), black = c(0, 1))   ) |>   ggplot(     aes(       x = period, y = .fitted, colour = factor(abused), linetype = factor(black)     )   ) +   geom_line() +   scale_x_continuous(breaks = 7:19, limits = c(7, 19)) +   scale_y_continuous(limits = c(-8, -2)) model_A <- update(first_depression_fit_2, . ~ . + siblings) model_B <- update(   first_depression_fit_2,    . ~ . +     between(siblings, 1, 2) +     between(siblings, 3, 4) +     between(siblings, 5, 6) +     between(siblings, 7, 8) +     between(siblings, 9, Inf) ) model_C <- update(first_depression_fit_2, . ~ . + bigfamily)  tidy(model_A) #> # A tibble: 7 × 5 #>   term                                  estimate std.error statistic   p.value #>   <chr>                                    <dbl>     <dbl>     <dbl>     <dbl> #> 1 (Intercept)                          -4.36     0.122        -35.8  2.23e-281 #> 2 poly(I(period - 18), 3, raw = TRUE)1  0.0611   0.0117         5.24 1.64e-  7 #> 3 poly(I(period - 18), 3, raw = TRUE)2 -0.00731  0.00122       -5.97 2.34e-  9 #> 4 poly(I(period - 18), 3, raw = TRUE)3  0.000182 0.0000790      2.30 2.14e-  2 #> 5 parental_divorce                      0.373    0.162          2.29 2.18e-  2 #> 6 female                                0.559    0.109          5.10 3.34e-  7 #> 7 siblings                             -0.0814   0.0223        -3.66 2.57e-  4 tidy(model_B) #> # A tibble: 11 × 5 #>    term                                  estimate std.error statistic   p.value #>    <chr>                                    <dbl>     <dbl>     <dbl>     <dbl> #>  1 (Intercept)                          -4.50     0.207      -21.8    4.22e-105 #>  2 poly(I(period - 18), 3, raw = TRUE)1  0.0615   0.0117       5.27   1.37e-  7 #>  3 poly(I(period - 18), 3, raw = TRUE)2 -0.00729  0.00122     -5.96   2.56e-  9 #>  4 poly(I(period - 18), 3, raw = TRUE)3  0.000181 0.0000790    2.30   2.17e-  2 #>  5 parental_divorce                      0.373    0.162        2.29   2.18e-  2 #>  6 female                                0.560    0.110        5.11   3.24e-  7 #>  7 between(siblings, 1, 2)TRUE           0.0209   0.198        0.106  9.16e-  1 #>  8 between(siblings, 3, 4)TRUE           0.0108   0.210        0.0512 9.59e-  1 #>  9 between(siblings, 5, 6)TRUE          -0.494    0.255       -1.94   5.22e-  2 #> 10 between(siblings, 7, 8)TRUE          -0.775    0.344       -2.26   2.41e-  2 #> 11 between(siblings, 9, Inf)TRUE        -0.658    0.344       -1.91   5.56e-  2 tidy(model_C) #> # A tibble: 7 × 5 #>   term                                  estimate std.error statistic     p.value #>   <chr>                                    <dbl>     <dbl>     <dbl>       <dbl> #> 1 (Intercept)                          -4.48     0.109        -41.2      0       #> 2 poly(I(period - 18), 3, raw = TRUE)1  0.0614   0.0117         5.27     1.40e-7 #> 3 poly(I(period - 18), 3, raw = TRUE)2 -0.00729  0.00122       -5.96     2.54e-9 #> 4 poly(I(period - 18), 3, raw = TRUE)3  0.000182 0.0000790      2.30     2.15e-2 #> 5 parental_divorce                      0.371    0.162          2.29     2.22e-2 #> 6 female                                0.558    0.109          5.10     3.44e-7 #> 7 bigfamily                            -0.611    0.145         -4.22     2.39e-5"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-12.html","id":"the-proportionality-assumption-uncovering-violations-and-simple-solutions","dir":"Articles","previous_headings":"","what":"12.5 The proportionality assumption: Uncovering violations and simple solutions","title":"Chapter 12: Extending the discrete-time hazard model","text":"Figure 12.8, page 458:   Table 12.5, page 459:","code":"# Raw math_dropout |>   group_by(term, woman) |>   summarise(     total = n(),     event = sum(event),     proportion = event / total,     proportion = if_else(proportion == 0, NA, proportion),     logit = log(proportion / (1 - proportion))   ) |>   ungroup() |>   mutate(across(c(woman), factor)) |>   na.omit() |>   ggplot(aes(x = term, y = logit, colour = woman)) +     geom_line() #> `summarise()` has grouped output by 'term'. You can override using the #> `.groups` argument. # Models model_A <- glm(   event ~ -1 + factor(term) + woman,   family = binomial(link = \"logit\"),   data = math_dropout ) model_B <- glm(   event ~ -1 + factor(term) + factor(term):woman,   family = binomial(link = \"logit\"),   data = math_dropout ) model_C <- update(model_A, . ~ . + woman:I(term - 1))  map_df(   list(model_A = model_A, model_B = model_B, model_C = model_C),   \\(.x) {     .x |>       augment(newdata = expand_grid(term = 1:5, woman = c(0, 1))) |>       mutate(hazard = 1 / (1 + exp(-.fitted)))   },   .id = \"model\" ) |>   ggplot(aes(x = term, y = hazard, colour = factor(woman))) +     geom_line() +     facet_wrap(vars(model)) tidy(model_A) #> # A tibble: 6 × 5 #>   term          estimate std.error statistic   p.value #>   <chr>            <dbl>     <dbl>     <dbl>     <dbl> #> 1 factor(term)1   -2.13     0.0567    -37.6  0         #> 2 factor(term)2   -0.942    0.0479    -19.7  3.14e- 86 #> 3 factor(term)3   -1.45     0.0634    -22.8  1.66e-115 #> 4 factor(term)4   -0.618    0.0757     -8.16 3.42e- 16 #> 5 factor(term)5   -0.772    0.143      -5.40 6.54e-  8 #> 6 woman            0.379    0.0501      7.55 4.33e- 14 tidy(model_B) #> # A tibble: 10 × 5 #>    term                estimate std.error statistic   p.value #>    <chr>                  <dbl>     <dbl>     <dbl>     <dbl> #>  1 factor(term)1         -2.01     0.0715    -28.1  1.40e-173 #>  2 factor(term)2         -0.964    0.0585    -16.5  5.98e- 61 #>  3 factor(term)3         -1.48     0.0847    -17.5  1.45e- 68 #>  4 factor(term)4         -0.710    0.101      -7.05 1.81e- 12 #>  5 factor(term)5         -0.869    0.191      -4.56 5.23e-  6 #>  6 factor(term)1:woman    0.157    0.0978      1.60 1.09e-  1 #>  7 factor(term)2:woman    0.419    0.0792      5.28 1.27e-  7 #>  8 factor(term)3:woman    0.441    0.116       3.81 1.42e-  4 #>  9 factor(term)4:woman    0.571    0.145       3.95 7.86e-  5 #> 10 factor(term)5:woman    0.601    0.286       2.10 3.55e-  2 tidy(model_C) #> # A tibble: 7 × 5 #>   term              estimate std.error statistic   p.value #>   <chr>                <dbl>     <dbl>     <dbl>     <dbl> #> 1 factor(term)1       -2.05     0.0646    -31.6  7.80e-220 #> 2 factor(term)2       -0.926    0.0482    -19.2  3.96e- 82 #> 3 factor(term)3       -1.50     0.0665    -22.5  3.54e-112 #> 4 factor(term)4       -0.718    0.0861     -8.34 7.34e- 17 #> 5 factor(term)5       -0.917    0.156      -5.89 3.94e-  9 #> 6 woman                0.227    0.0774      2.94 3.31e-  3 #> 7 woman:I(term - 1)    0.120    0.0470      2.55 1.08e-  2"},{"path":[]},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-12.html","id":"residual-analysis","dir":"Articles","previous_headings":"","what":"12.7 Residual Analysis","title":"Chapter 12: Extending the discrete-time hazard model","text":"Table 12.6, page 465: Figure 12.8, page 467:","code":"first_sex_fit <- glm(   event ~ -1 + factor(grade) + parental_transition + parental_antisociality,   family = binomial(link = \"logit\"),   data = first_sex_pp )  first_sex_fit |>   augment(data = first_sex_pp, type.residuals = \"deviance\") |>   select(id:parental_antisociality, .resid) |>   filter(id %in% c(22, 112, 166, 89, 102, 87, 67, 212)) |>   pivot_wider(     id_cols = id,     names_from = grade,     names_prefix = \"grade_\",     values_from = .resid   ) #> # A tibble: 8 × 7 #>   id    grade_7 grade_8 grade_9 grade_10 grade_11 grade_12 #>   <fct>   <dbl>   <dbl>   <dbl>    <dbl>    <dbl>    <dbl> #> 1 22     -0.412  -0.294  -0.584   -0.718   -0.775    1.41  #> 2 67     -0.618  -0.448  -0.856   -1.03    -1.10     1.04  #> 3 87      1.82   NA      NA       NA       NA       NA     #> 4 89     -0.325  -0.231  -0.464   -0.575    1.86    NA     #> 5 102    -0.491   2.37   NA       NA       NA       NA     #> 6 112    -0.411  -0.294  -0.583   -0.717   -0.774   -0.956 #> 7 166    -0.661  -0.481  -0.911   -1.09     1.19    NA     #> 8 212    -0.286  -0.203  -0.410   -0.509   -0.552   -0.696 first_sex_fit |>   augment(data = first_sex_pp, type.residuals = \"deviance\") |>   ggplot(aes(x = id, y = .resid)) +     geom_point() +     geom_hline(yintercept = 0) first_sex_fit |>   augment(data = first_sex_pp, type.residuals = \"deviance\") |>   group_by(id) |>   summarise(ss.deviance = sum(.resid^2)) |>   ggplot(aes(x = id, y = ss.deviance)) +     geom_point()"},{"path":[]},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-13.html","id":"grouped-methods-for-estimating-continuous-time-survivor-and-hazard-functions","dir":"Articles","previous_headings":"","what":"13.2 Grouped Methods for Estimating Continuous-Time Survivor and Hazard Functions","title":"Chapter 13: Describing continuous-time event occurrence data","text":"Table 13.2, page 477: Figure 13.1, page 479:","code":"# Adding discrete-time intervals honking_discrete <- honking |>   mutate(     event = if_else(censor == 0, 1, 0),     time_interval = cut(seconds, breaks = c(1:8, 18), right = FALSE),     time_start = str_extract(time_interval, \"[[:digit:]]+(?=,)\"),     time_end = str_extract(time_interval, \"(?<=,)[[:digit:]]+\"),     across(c(time_start, time_end), as.numeric)   )  # Grouped life table honking_grouped <- honking_discrete |>   group_by(time_interval, time_start, time_end) |>   summarise(     total = n(),     n_event = sum(event),     n_censor = sum(censor),     # All grouping needs to be dropped in order to calculate the number at risk     # correctly.     .groups = \"drop\"   ) |>   mutate(n_risk = sum(total) - lag(cumsum(total), default = 0))  # The conditional probability can be estimated using the same discrete-time methods # from the previous chapter, using the grouped data. honking_grouped_fit <- glm(   cbind(n_event, n_risk - n_event) ~ 0 + time_interval,   family = binomial(link = \"logit\"),   data = honking_grouped )  honking_grouped_fit |>   # .fitted is the conditional probability   broom::augment(newdata = honking_grouped, type.predict = \"response\") |>   mutate(     survival = cumprod(1 - .fitted),     hazard = .fitted / (time_end - time_start)   ) #> # A tibble: 8 × 10 #>   time_interval time_start time_end total n_event n_censor n_risk .fitted #>   <fct>              <dbl>    <dbl> <int>   <dbl>    <dbl>  <int>   <dbl> #> 1 [1,2)                  1        2     6       5        1     57  0.0877 #> 2 [2,3)                  2        3    17      14        3     51  0.275  #> 3 [3,4)                  3        4    11       9        2     34  0.265  #> 4 [4,5)                  4        5    10       6        4     23  0.261  #> 5 [5,6)                  5        6     4       2        2     13  0.154  #> 6 [6,7)                  6        7     4       2        2      9  0.222  #> 7 [7,8)                  7        8     1       1        0      5  0.2    #> 8 [8,18)                 8       18     4       3        1      4  0.75   #> # ℹ 2 more variables: survival <dbl>, hazard <dbl>  # Estimates by hand honking_discrete_fit <- honking_grouped |>   mutate(     conditional_probability = n_event / n_risk,     discrete.s = cumprod(1 - conditional_probability),     discrete.h = conditional_probability / (time_end - time_start),     # The actuarial method redefines the number of individuals to be at risk of     # event occurrence for both the survival and hazard functions, and thus has     # different conditional probabilities from the discrete method.     n_risk.s = n_risk - (n_censor / 2),     conditional_probability.s = n_event / n_risk.s,     actuarial.s = cumprod(1 - conditional_probability.s),     n_risk.h = n_risk.s - (n_event / 2),     conditional_probability.h = n_event / n_risk.h,     actuarial.h = conditional_probability.h / (time_end - time_start)   ) |>   select(     -c(conditional_probability.s, conditional_probability.h, n_risk.s, n_risk.h)   ) |>   add_row(time_end = 0:1, discrete.s = 1, actuarial.s = 1)  honking_discrete_fit #> # A tibble: 10 × 12 #>    time_interval time_start time_end total n_event n_censor n_risk #>    <fct>              <dbl>    <dbl> <int>   <dbl>    <dbl>  <int> #>  1 [1,2)                  1        2     6       5        1     57 #>  2 [2,3)                  2        3    17      14        3     51 #>  3 [3,4)                  3        4    11       9        2     34 #>  4 [4,5)                  4        5    10       6        4     23 #>  5 [5,6)                  5        6     4       2        2     13 #>  6 [6,7)                  6        7     4       2        2      9 #>  7 [7,8)                  7        8     1       1        0      5 #>  8 [8,18)                 8       18     4       3        1      4 #>  9 NA                    NA        0    NA      NA       NA     NA #> 10 NA                    NA        1    NA      NA       NA     NA #> # ℹ 5 more variables: conditional_probability <dbl>, discrete.s <dbl>, #> #   discrete.h <dbl>, actuarial.s <dbl>, actuarial.h <dbl> honking_discrete_fit |>   pivot_longer(     cols = c(discrete.h, discrete.s, actuarial.h, actuarial.s),     names_to = \"estimate\"   ) |>   mutate(     estimate = factor(       estimate,       levels = c(\"discrete.s\", \"actuarial.s\", \"discrete.h\", \"actuarial.h\")     )   ) |>   ggplot(aes(x = time_end, y = value)) +     geom_line(data = \\(x) filter(x, str_detect(estimate, \"discrete\"))) +     geom_step(       data = \\(x) filter(x, str_detect(estimate, \"actuarial\")),       direction = \"vh\"     ) +     scale_x_continuous(limits = c(0, 20)) +     facet_wrap(vars(estimate), scales = \"free_y\") +     ggh4x::facetted_pos_scales(       y = list(         str_detect(estimate, \"s$\") ~ scale_y_continuous(limits = c(0, 1)),         str_detect(estimate, \"h$\") ~           scale_y_continuous(limits = c(0, .35), breaks = seq(0, .35, by = .05))       )     )"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-13.html","id":"the-kaplan-meier-method-of-estimating-the-continuous-time-survivor-function","dir":"Articles","previous_headings":"","what":"13.3 The Kaplan-Meier Method of Estimating the Continuous-Time Survivor Function","title":"Chapter 13: Describing continuous-time event occurrence data","text":"Table 13.3, page 484: Figure 13.2, page 485:","code":"honking_continuous_fit <- survfit(Surv(seconds, 1 - censor) ~ 1, data = honking)  honking_continuous_fit_tidy <- honking_continuous_fit |>   survfit0() |>   tidy() |>   select(-starts_with(\"conf\")) |>   mutate(     # tidy() returns the standard error for the cumulative hazard, so we need to     # transform it into the standard error for the survival.      std.error = estimate * std.error,     conditional_probability = n.event / n.risk,     time_interval = 1:n(),     time_end = lead(time, default = Inf),     width = time_end - time,     hazard = conditional_probability / width   ) |>   relocate(     time_interval,     time_start = time,     time_end,     n.risk:n.censor,     conditional_probability,     survival = estimate   )  honking_continuous_fit_tidy #> # A tibble: 57 × 11 #>    time_interval time_start time_end n.risk n.event n.censor #>            <int>      <dbl>    <dbl>  <dbl>   <dbl>    <dbl> #>  1             1       0        1.41     57       0        0 #>  2             2       1.41     1.51     57       1        1 #>  3             3       1.51     1.67     55       1        0 #>  4             4       1.67     1.68     54       1        0 #>  5             5       1.68     1.86     53       1        0 #>  6             6       1.86     2.12     52       1        0 #>  7             7       2.12     2.19     51       1        0 #>  8             8       2.19     2.36     50       1        0 #>  9             9       2.36     2.48     49       0        1 #> 10            10       2.48     2.5      48       1        0 #> # ℹ 47 more rows #> # ℹ 5 more variables: conditional_probability <dbl>, survival <dbl>, #> #   std.error <dbl>, width <dbl>, hazard <dbl> honking_continuous_fit_tidy |>   add_row(time_end = 0:1, survival = 1) |>   # The largest event time was censored, so we extend the last step out to that   # largest censored value rather than going to infinity.   mutate(time_end = if_else(time_end == Inf, time_start, time_end)) |>    ggplot() +     geom_step(       aes(x = time_end, y = survival, linetype = \"1: Kaplan Meier\"),       direction = \"vh\"     ) +     geom_line(       aes(x = time_end, y = discrete.s, linetype = \"2: Discrete-time\"),       data = honking_discrete_fit     ) +     geom_step(       aes(x = time_end, y = actuarial.s, linetype = \"3: Actuarial\"),       data = honking_discrete_fit,       direction = \"vh\"     ) +     scale_x_continuous(limits = c(0, 20)) +     labs(x = \"time\")"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-13.html","id":"the-cumulative-hazard-function","dir":"Articles","previous_headings":"","what":"13.4 The Cumulative Hazard Function","title":"Chapter 13: Describing continuous-time event occurrence data","text":"Figure 13.4, page 493:","code":"honking_continuous_fit_tidy |>   mutate(time_end = if_else(time_end == Inf, time_start, time_end)) |>    ggplot(aes(x = time_end)) +     geom_step(       aes(y = -log(survival), linetype = \"Negative log\"),       direction = \"vh\"     ) +     geom_step(       aes(y = cumsum(hazard * width), linetype = \"Nelson-Aalen\"),       direction = \"vh\"     ) #> Warning: Removed 1 row containing missing values or values outside the scale range #> (`geom_step()`)."},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-13.html","id":"kernel-smoothed-estimates-of-the-hazard-function","dir":"Articles","previous_headings":"","what":"13.5 Kernel-Smoothed Estimates of the Hazard Function","title":"Chapter 13: Describing continuous-time event occurrence data","text":"Figure 13.5, page 496:","code":"kernel_smoothed_hazards <- map_df(   set_names(1:3),   \\(bandwidth) {     # muhaz() estimates the hazard function from right-censored data using     # kernel-based methods, using the vector of survival and event times.     kernel_smoothed_hazard <- muhaz(       honking$seconds,       1 - honking$censor,       # Narrow the temporal region the smoothed function describes, given the       # bandwidth and the minimum and maximum observed event times.       min.time = min(honking$seconds[honking$censor == 0]) + bandwidth,       max.time = max(honking$seconds[honking$censor == 0]) - bandwidth,       bw.grid = bandwidth,       bw.method = \"global\",       b.cor = \"none\",       kern = \"epanechnikov\"     )          tidy(kernel_smoothed_hazard)   },   .id = \"bandwidth\" ) #> Warning in muhaz(honking$seconds, 1 - honking$censor, min.time = min(honking$seconds[honking$censor == : minimum time > minimum Survival Time #> Warning in muhaz(honking$seconds, 1 - honking$censor, min.time = min(honking$seconds[honking$censor == : minimum time > minimum Survival Time #> Warning in muhaz(honking$seconds, 1 - honking$censor, min.time = min(honking$seconds[honking$censor == : minimum time > minimum Survival Time  ggplot(kernel_smoothed_hazards, aes(x = time, y = estimate)) +   geom_line() +   scale_x_continuous(limits = c(0, 20)) +   facet_wrap(vars(bandwidth), ncol = 1, labeller = label_both)"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-13.html","id":"developing-an-intuition-about-continuous-time-survivor-cumulative-hazard-and-kernel-smoothed-hazard-functions","dir":"Articles","previous_headings":"","what":"13.6 Developing an Intuition about Continuous-Time Survivor, Cumulative Hazard, and Kernel-Smoothed Hazard Functions","title":"Chapter 13: Describing continuous-time event occurrence data","text":"Figure 13.6, page 499:","code":"# TODO: Check that models are correct, then tidy up code.  # Fit survival models alcohol_relapse_fit <- survfit(   Surv(weeks, 1 - censor) ~ 1, data = alcohol_relapse )  judges_fit <- survfit(   Surv(tenure, dead) ~ 1, data = judges )  first_depression_fit <- survfit(   Surv(age, 1 - censor) ~ 1, data = first_depression_2 )  health_workers_fit <- survfit(   Surv(weeks, 1 - censor) ~ 1, data = health_workers )  # Tidy survival models survival_models <- list(   alcohol_relapse = alcohol_relapse_fit,   judges = judges_fit,   first_depression = first_depression_fit,   health_workers = health_workers_fit )  survival_models_tidy <- map(   survival_models,   \\(.x) {     .x |>       survfit0() |>       tidy() |>       mutate(cumulative_hazard = -log(estimate)) |>       select(time, survival = estimate, cumulative_hazard) |>       pivot_longer(         cols = c(survival, cumulative_hazard),         names_to = \"statistic\",         values_to = \"estimate\"       )   } )  # Estimate and tidy smoothed hazards kernel_smoothed_hazards_tidy <- pmap(   list(     list(       alcohol_relapse = alcohol_relapse$weeks,       judges = judges$tenure,       first_depression = first_depression_2$age,       health_workers = health_workers$weeks     ),     list(       1 - alcohol_relapse$censor,       judges$dead,       1 - first_depression_2$censor,       1 - health_workers$censor     ),     list(12, 5, 7, 7)   ),   \\(survival_time, event, bandwidth) {     kernel_smoothed_hazard <- muhaz(       survival_time,       event,       min.time = min(survival_time[1 - event == 0]) + bandwidth,       max.time = max(survival_time[1 - event == 0]) - bandwidth,       bw.grid = bandwidth,       bw.method = \"global\",       b.cor = \"none\",       kern = \"epanechnikov\"     )          kernel_smoothed_hazard |>       tidy() |>       mutate(statistic = \"hazard\")   } ) #> Warning in muhaz(survival_time, event, min.time = min(survival_time[1 - : minimum time > minimum Survival Time #> Warning in muhaz(survival_time, event, min.time = min(survival_time[1 - : minimum time > minimum Survival Time #> Warning in muhaz(survival_time, event, min.time = min(survival_time[1 - : minimum time > minimum Survival Time #> Warning in muhaz(survival_time, event, min.time = min(survival_time[1 - : minimum time > minimum Survival Time  # Combine estimates estimates_tidy <- map2(   survival_models_tidy, kernel_smoothed_hazards_tidy,   \\(.x, .y) {     bind_rows(.x, .y) |>       mutate(statistic = factor(         statistic, levels = c(\"survival\", \"cumulative_hazard\", \"hazard\"))       )   } )  plots <- map2(   estimates_tidy, names(estimates_tidy),   \\(.x, .y) {     ggplot(.x, aes(x = time, y = estimate)) +       geom_step(data = \\(.x) filter(.x, statistic != \"hazard\")) +       geom_line(data = \\(.x) filter(.x, statistic == \"hazard\")) +       facet_wrap(vars(statistic), ncol = 1, scales = \"free_y\") +       labs(title = .y)   } )  patchwork::wrap_plots(plots, ncol = 4)"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-14.html","id":"toward-a-statistical-model-for-continuous-time-hazard","dir":"Articles","previous_headings":"","what":"14.1 Toward a Statistical Model for Continuous-Time Hazard","title":"Chapter 14: Fitting the Cox regression model","text":"Figure 14.1, page 505:  Figure 14.2, page 508:","code":"# Fit survival models rearrest_fit <- survfit(   Surv(months, abs(censor - 1)) ~ 1, data = rearrest )  person_crime_0_fit <- update(rearrest_fit, subset = person_crime == 0) person_crime_1_fit <- update(rearrest_fit, subset = person_crime == 1)  # Tidy survival models survival_models <- list(   person_crime_0 = person_crime_0_fit,   person_crime_1 = person_crime_1_fit )  survival_models_tidy <- map(   survival_models,   \\(.x) {     .x |>       survfit0() |>       tidy() |>       mutate(cumulative_hazard = -log(estimate)) |>       select(time, survival = estimate, cumulative_hazard) |>       pivot_longer(         cols = c(survival, cumulative_hazard),         names_to = \"statistic\",         values_to = \"estimate\"       )   } )  # Estimate and tidy smoothed hazards kernel_smoothed_hazards_tidy <- map2(   list(     person_crime_0 = filter(rearrest, person_crime == 0)$months,     person_crime_1 = filter(rearrest, person_crime == 1)$months   ),   list(     abs(filter(rearrest, person_crime == 0)$censor - 1),     abs(filter(rearrest, person_crime == 1)$censor - 1)   ),   \\(survival_time, event) {     kernel_smoothed_hazard <- muhaz(       survival_time,       event,       min.time = min(survival_time[1 - event == 0]) + 8,       max.time = max(survival_time[1 - event == 0]) - 8,       bw.grid = 8,       bw.method = \"global\",       b.cor = \"none\",       kern = \"epanechnikov\"     )          kernel_smoothed_hazard |>       tidy() |>       mutate(statistic = \"hazard\")   } ) #> Warning in muhaz(survival_time, event, min.time = min(survival_time[1 - : minimum time > minimum Survival Time #> Warning in muhaz(survival_time, event, min.time = min(survival_time[1 - : minimum time > minimum Survival Time  # Combine estimates estimates_tidy <- map2(   survival_models_tidy, kernel_smoothed_hazards_tidy,   \\(.x, .y) {     bind_rows(.x, .y) |>       mutate(statistic = factor(         statistic, levels = c(\"survival\", \"cumulative_hazard\", \"hazard\"))       )   } ) |>   list_rbind(names_to = \"person_crime\")  # Plot ggplot(estimates_tidy, aes(x = time, y = estimate, linetype = person_crime)) +   geom_step(data = \\(.x) filter(.x, statistic != \"hazard\")) +   geom_line(data = \\(.x) filter(.x, statistic == \"hazard\")) +   facet_wrap(vars(statistic), ncol = 1, scales = \"free_y\") # Top plot log_cumulative_hazards <- estimates_tidy |>   filter(statistic == \"cumulative_hazard\") |>   mutate(estimate = log(estimate)) |>   filter(!is.infinite(estimate))    ggplot(   log_cumulative_hazards, aes(x = time, y = estimate, linetype = person_crime) ) +   geom_hline(yintercept = 0) +   geom_step() +   coord_cartesian(xlim = c(0, 30), ylim = c(-6, 1)) # Middle and bottom plots ---- rearrest_fit_2 <- coxph(   Surv(months, abs(censor - 1)) ~ person_crime, data = rearrest, method = \"efron\" )  rearrest_fit_2_curves <- map_df(   list(person_crime_0 = 0, person_crime_1 = 1),   \\(.x) {     rearrest_fit_2 |>       survfit(         newdata = data.frame(person_crime = .x),         type = \"kaplan-meier\"       ) |>       tidy() |>       mutate(         cumulative_hazard = -log(estimate),         log_cumulative_hazard = log(cumulative_hazard)       )   },   .id = \"person_crime\" )  # Middle plot rearrest_fit_2 |>   augment(data = rearrest, type.predict = \"survival\") |>   mutate(     cumulative_hazard = -log(.fitted),     log_cumulative_hazard = log(cumulative_hazard)   ) |>   ggplot(mapping = aes(x = months, y = log_cumulative_hazard)) +     geom_step(aes(x = time, linetype = person_crime), data = rearrest_fit_2_curves)+     geom_point(       aes(shape = person_crime, x = time, y = estimate),       data = log_cumulative_hazards     ) +     scale_y_continuous(breaks = -6:1) +     coord_cartesian(xlim = c(0, 30), ylim = c(-6, 1)) # Bottom plot rearrest_fit_2 |>   augment(data = rearrest, type.predict = \"survival\") |>   mutate(     cumulative_hazard = -log(.fitted),     log_cumulative_hazard = log(cumulative_hazard)   ) |>   ggplot(mapping = aes(x = months, y = cumulative_hazard)) +     geom_step(aes(x = time, linetype = person_crime), data = rearrest_fit_2_curves) +     geom_point(       aes(shape = person_crime, x = time, y = estimate),       data = filter(estimates_tidy, statistic == \"cumulative_hazard\")     ) +     coord_cartesian(xlim = c(0, 30))"},{"path":[]},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-14.html","id":"interpreting-the-results-of-fitting-the-cox-regression-model-to-data","dir":"Articles","previous_headings":"","what":"14.3 Interpreting the Results of Fitting the Cox Regression Model to Data","title":"Chapter 14: Fitting the Cox regression model","text":"Table 14.1, page 525: Table 14.2, page 533:","code":"# TODO: Make table model_A <- coxph(Surv(months, abs(censor - 1)) ~ person_crime, data = rearrest)  model_B <- coxph(Surv(months, abs(censor - 1)) ~ property_crime, data = rearrest)  model_C <- coxph(Surv(months, abs(censor - 1)) ~ age, data = rearrest)  model_D <- coxph(   Surv(months, abs(censor - 1)) ~ person_crime + property_crime + age,   data = rearrest ) # TODO"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-14.html","id":"nonparametric-strategies-for-displaying-the-results-of-model-fitting","dir":"Articles","previous_headings":"","what":"14.4 Nonparametric Strategies for Displaying the Results of Model Fitting","title":"Chapter 14: Fitting the Cox regression model","text":"Figure 14.4, page 538:  Figure 14.5, page 541:","code":"pmap(   list(     list(baseline = 0, average = mean(rearrest$person_crime)),     list(0, mean(rearrest$property_crime)),     list(0, mean(rearrest$age))   ),   \\(.person_crime, .property_crime, .age) {     model_D_baseline <- model_D |>       survfit(         newdata = tibble(           person_crime = .person_crime,           property_crime = .property_crime,           age = .age)         ) |>       survfit0() |>       tidy()          survival <- ggplot(model_D_baseline, aes(x = time, y = estimate)) +       geom_line() +       geom_point() +       scale_x_continuous(limits = c(0, 29)) +       coord_cartesian(xlim = c(0, 36), ylim = c(0, 1))          cumulative_hazard <- ggplot(         model_D_baseline, aes(x = time, y = -log(estimate))       ) +       geom_line() +       geom_point() +       scale_x_continuous(limits = c(0, 29)) +       coord_cartesian(xlim = c(0, 36), ylim = c(0, 1.5))          #TODO: Not sure if muhaz can deal with this situation with newdata     hazard <- muhaz(       model_D_baseline$time,       1 - model_D_baseline$n.censor,       min.time = min(rearrest$months[rearrest$censor == 0]) + 8,       max.time = max(rearrest$months[rearrest$censor == 0]) - 8,       bw.grid = 8,       bw.method = \"global\",       b.cor = \"none\",       kern = \"epanechnikov\"     ) |>       tidy() |>       ggplot(aes(x = time, y = estimate)) +         geom_line() +         coord_cartesian(xlim = c(0, 36), ylim = c(0, 0.08))          survival + cumulative_hazard + hazard +       plot_layout(ncol = 1)   } ) |>   patchwork::wrap_plots() #> Warning in muhaz(model_D_baseline$time, 1 - model_D_baseline$n.censor, min.time = min(rearrest$months[rearrest$censor == : minimum time > minimum Survival Time #> Warning in muhaz(model_D_baseline$time, 1 - model_D_baseline$n.censor, min.time = min(rearrest$months[rearrest$censor == : minimum time > minimum Survival Time #> Warning: Removed 6 rows containing missing values or values outside the scale range #> (`geom_line()`). #> Warning: Removed 6 rows containing missing values or values outside the scale range #> (`geom_point()`). #> Warning: Removed 6 rows containing missing values or values outside the scale range #> (`geom_line()`). #> Warning: Removed 6 rows containing missing values or values outside the scale range #> (`geom_point()`). #> Warning: Removed 6 rows containing missing values or values outside the scale range #> (`geom_line()`). #> Warning: Removed 6 rows containing missing values or values outside the scale range #> (`geom_point()`). #> Warning: Removed 6 rows containing missing values or values outside the scale range #> (`geom_line()`). #> Warning: Removed 6 rows containing missing values or values outside the scale range #> (`geom_point()`). #TODO: Not sure if muhaz can deal with this situation with newdata, not sure if the # estimates can be modified after fitting to get the desired values hazard_fit <- muhaz(   rearrest$months,   1 - rearrest$censor,   min.time = min(rearrest$months[rearrest$censor == 0]) + 8,   max.time = max(rearrest$months[rearrest$censor == 0]) - 8,   bw.grid = 8,   bw.method = \"global\",   b.cor = \"none\",   kern = \"epanechnikov\" ) #> Warning in muhaz(rearrest$months, 1 - rearrest$censor, min.time = min(rearrest$months[rearrest$censor == : minimum time > minimum Survival Time hazard_fit |> str() #> List of 7 #>  $ pin      :List of 13 #>   ..$ times      : num [1:194] 0.0657 0.1314 0.23 0.2957 0.2957 ... #>   ..$ delta      : num [1:194] 1 1 1 1 1 1 1 0 1 1 ... #>   ..$ nobs       : int 194 #>   ..$ min.time   : num 8.07 #>   ..$ max.time   : num 21 #>   ..$ n.min.grid : num 51 #>   ..$ min.grid   : num [1:51] 8.07 8.32 8.58 8.84 9.1 ... #>   ..$ n.est.grid : num 101 #>   ..$ bw.pilot   : num 1.03 #>   ..$ bw.smooth  : num 5.16 #>   ..$ method     : int 1 #>   ..$ b.cor      : num 0 #>   ..$ kernel.type: num 1 #>  $ est.grid : num [1:101] 8.07 8.19 8.32 8.45 8.58 ... #>  $ haz.est  : num [1:101] 0.0418 0.0419 0.042 0.0421 0.0421 ... #>  $ imse.opt : num 0 #>  $ bw.glob  : num 8 #>  $ glob.imse: num 0 #>  $ bw.grid  : num 8 #>  - attr(*, \"class\")= chr \"muhaz\" prototypical_individuals <- map2_df(   # .person_crime   list(neither = 0, personal_only = 1, property_only = 0, both = 1),   # .property_crime   list(0, 0, 1, 1),   \\(.person_crime, .property_crime) {      model_D |>       survfit(         newdata = tibble(           person_crime = .person_crime,           property_crime = .property_crime,           age = mean(rearrest$age)         )       ) |>       survfit0() |>       tidy()   },   .id = \"prototypical_individual\" )  prototypical_individuals_survival <- ggplot(   prototypical_individuals,   aes(x = time, y = estimate, colour = prototypical_individual)) +   geom_line() +   scale_x_continuous(limits = c(0, 29)) +   coord_cartesian(xlim = c(0, 36), ylim = c(0, 1)) +   labs(     y = \"Survival\"   )  prototypical_individuals_cumhaz <- ggplot(   prototypical_individuals,   aes(x = time, y = -log(estimate), colour = prototypical_individual)) +   geom_line() +   scale_x_continuous(limits = c(0, 29)) +   coord_cartesian(xlim = c(0, 36), ylim = c(0, 2)) +   labs(     y = \"Cumulative hazard\"   )  prototypical_individuals_logcumhaz <- ggplot(   filter(prototypical_individuals, time != 0),   aes(x = time, y = log(-log(estimate)), colour = prototypical_individual)) +   geom_line() +   scale_x_continuous(limits = c(0, 29)) +   scale_y_continuous(breaks = -7:1) +   coord_cartesian(xlim = c(0, 36), ylim = c(-7, 1)) +   labs(     y = \"log Cumulative hazard\"   )  prototypical_individuals_survival + prototypical_individuals_cumhaz +   prototypical_individuals_logcumhaz + plot_layout(ncol = 1, guides = \"collect\") #> Warning: Removed 24 rows containing missing values or values outside the scale range #> (`geom_line()`). #> Removed 24 rows containing missing values or values outside the scale range #> (`geom_line()`). #> Removed 24 rows containing missing values or values outside the scale range #> (`geom_line()`)."},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-15.html","id":"time-varying-predictors","dir":"Articles","previous_headings":"","what":"15.1 Time-Varying Predictors","title":"Chapter 15: Extending the Cox regression model","text":"Table 15.1, page 548:","code":"# TODO: Clean up code and make table model_A <- coxph(   Surv(used_cocaine_age, 1 - censor) ~     birthyr + early_marijuana_use + early_drug_use,   data = first_cocaine )  # Model B ---- first_cocaine_pp <- first_cocaine |>   group_by(id) |>   reframe(     # {survival} uses the counting process method for time-varying predictors,     # so we need to construct intervals for the ages at which different events     # occurred. These intervals are left-censored, so we start with the end     # time; we also only require unique intervals, so duplicate ages should be     # removed.     age_end = sort(unique(c(used_cocaine_age, used_marijuana_age, used_drugs_age))),     age_start = lag(age_end, default = 0),     # Time-varying predictors should be lagged so that they describe an individual's     # status in the immediately prior year.     used_cocaine = if_else(       age_end == used_cocaine_age & censor == 0, true = 1, false = 0, missing = 0     ),     used_marijuana = if_else(       age_end > used_marijuana_age, true = 1, false = 0, missing = 0     ),     used_drugs = if_else(       age_end > used_drugs_age, true = 1, false = 0, missing = 0     ),     # Keep time-invariant predictors from the person-level data     birthyr   ) |>   relocate(age_start, .before = age_end)  model_B <- coxph(   Surv(age_start, age_end, used_cocaine) ~     birthyr + used_marijuana + used_drugs,   data = first_cocaine_pp,   ties = \"efron\" )  ## This method with tmerge() also works tmerge(   first_cocaine, first_cocaine,   id = id,   used_cocaine = event(used_cocaine_age, 1 - censor),   used_marijuana = tdc(used_marijuana_age),   used_drugs = tdc(used_drugs_age),   options = list(     tstartname = \"age_start\",     tstopname = \"age_end\"   ) ) |> as_tibble() |> arrange(id) #> Warning: Unknown or uninitialised column: `tstop`. #> Warning: Unknown or uninitialised column: `tstart`. #> Warning in tmerge(first_cocaine, first_cocaine, id = id, used_cocaine = #> event(used_cocaine_age, : replacement of variable 'used_marijuana' #> Warning in tmerge(first_cocaine, first_cocaine, id = id, used_cocaine = #> event(used_cocaine_age, : replacement of variable 'used_drugs' #> # A tibble: 3,086 × 18 #>    id    used_cocaine_age censor birthyr early_marijuana_use early_drug_use #>    <fct>            <dbl>  <dbl>   <dbl>               <dbl>          <dbl> #>  1 5                   41      1       0                   0              0 #>  2 5                   41      1       0                   0              0 #>  3 5                   41      1       0                   0              0 #>  4 8                   32      1      10                   0              0 #>  5 9                   36      1       5                   0              0 #>  6 9                   36      1       5                   0              0 #>  7 11                  41      1       0                   0              0 #>  8 12                  32      0       4                   0              0 #>  9 12                  32      0       4                   0              0 #> 10 13                  39      1       3                   0              0 #> # ℹ 3,076 more rows #> # ℹ 12 more variables: used_marijuana <int>, used_marijuana_age <dbl>, #> #   sold_marijuana <dbl>, sold_marijuana_age <dbl>, used_drugs <int>, #> #   used_drugs_age <dbl>, sold_drugs <dbl>, sold_drugs_age <dbl>, rural <dbl>, #> #   age_start <dbl>, age_end <dbl>, used_cocaine <dbl>  coxph(   Surv(age_start, age_end, used_cocaine) ~     birthyr + used_marijuana + used_drugs,   data = tmerge(     first_cocaine, first_cocaine,     id = id,     used_cocaine = event(used_cocaine_age, 1 - censor),     used_marijuana = tdc(used_marijuana_age),     used_drugs = tdc(used_drugs_age),     options = list(       tstartname = \"age_start\",       tstopname = \"age_end\"     )   ),   ties = \"efron\" ) |>   summary() #> Warning: Unknown or uninitialised column: `tstop`. #> Warning: Unknown or uninitialised column: `tstart`. #> Warning in tmerge(first_cocaine, first_cocaine, id = id, used_cocaine = #> event(used_cocaine_age, : replacement of variable 'used_marijuana' #> Warning in tmerge(first_cocaine, first_cocaine, id = id, used_cocaine = #> event(used_cocaine_age, : replacement of variable 'used_drugs' #> Warning: Unknown or uninitialised column: `tstop`. #> Warning: Unknown or uninitialised column: `tstart`. #> Warning in tmerge(first_cocaine, first_cocaine, id = id, used_cocaine = #> event(used_cocaine_age, : replacement of variable 'used_marijuana' #> Warning in tmerge(first_cocaine, first_cocaine, id = id, used_cocaine = #> event(used_cocaine_age, : replacement of variable 'used_drugs' #> Call: #> coxph(formula = Surv(age_start, age_end, used_cocaine) ~ birthyr +  #>     used_marijuana + used_drugs, data = tmerge(first_cocaine,  #>     first_cocaine, id = id, used_cocaine = event(used_cocaine_age,  #>         1 - censor), used_marijuana = tdc(used_marijuana_age),  #>     used_drugs = tdc(used_drugs_age), options = list(tstartname = \"age_start\",  #>         tstopname = \"age_end\")), ties = \"efron\") #>  #>   n= 3086, number of events= 382  #>  #>                    coef exp(coef) se(coef)      z Pr(>|z|)     #> birthyr         0.10741   1.11340  0.02145  5.008  5.5e-07 *** #> used_marijuana  2.55176  12.82972  0.28095  9.082  < 2e-16 *** #> used_drugs      1.85387   6.38446  0.12921 14.347  < 2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #>                exp(coef) exp(-coef) lower .95 upper .95 #> birthyr            1.113    0.89815     1.068     1.161 #> used_marijuana    12.830    0.07794     7.397    22.252 #> used_drugs         6.384    0.15663     4.956     8.225 #>  #> Concordance= 0.876  (se = 0.008 ) #> Likelihood ratio test= 856  on 3 df,   p=<2e-16 #> Wald test            = 451.1  on 3 df,   p=<2e-16 #> Score (logrank) test = 1039  on 3 df,   p=<2e-16  # Model C and D ---- first_cocaine_pp_C <- first_cocaine |>   group_by(id) |>   reframe(     age_end = sort(       unique(         c(           used_cocaine_age,           used_marijuana_age,           used_drugs_age,           sold_marijuana_age,           sold_drugs_age         )       )     ),     age_start = lag(age_end, default = 0),     # Time-varying predictors should be lagged so that they describe an individual's     # status in the immediately prior year.     used_cocaine = if_else(       age_end == used_cocaine_age & censor == 0, true = 1, false = 0, missing = 0     ),     used_marijuana = if_else(       age_end > used_marijuana_age, true = 1, false = 0, missing = 0     ),     used_drugs = if_else(       age_end > used_drugs_age, true = 1, false = 0, missing = 0     ),     sold_marijuana = if_else(       age_end > sold_marijuana_age, true = 1, false = 0, missing = 0     ),     sold_drugs = if_else(       age_end > sold_drugs_age, true = 1, false = 0, missing = 0     ),     # Keep time-invariant predictors from the person-level data     birthyr,     early_marijuana_use,     early_drug_use,     rural   ) |>   relocate(age_start, .before = age_end)  first_cocaine_model_C <- coxph(   Surv(age_start, age_end, used_cocaine) ~     birthyr + used_marijuana + used_drugs + sold_marijuana + sold_drugs,   data = first_cocaine_pp_C,   ties = \"efron\" )  model_D <- update(first_cocaine_model_C, . ~ . + early_marijuana_use + early_drug_use)"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-15.html","id":"imputation-strategies-for-time-varying-predictors","dir":"Articles","previous_headings":"15.1 Time-Varying Predictors","what":"15.1.3 Imputation Strategies for Time-Varying Predictors","title":"Chapter 15: Extending the Cox regression model","text":"Section 15.1.3 Singer Willet (2003) discuss imputation strategies time-varying predictors using subset unpublished data Hall, Havassy, Wasserman (1990), measured relation number days relapse cocaine use several predictors might associated relapse sample 104 newly abstinent cocaine users recently completed abstinence-oriented treatment program. Former cocaine users followed 12 weeks post-treatment used cocaine 7 consecutive days. Self-reported abstinence confirmed interview absence cocaine urine specimens. example use cocaine_relapse_2 data set, person-period data frame 1248 rows 7 columns: id: Participant ID. days: Number days relapse cocaine use censoring. Relapse defined 4 days cocaine use week preceding interview. Study dropouts lost participants coded relapsing cocaine use, number days relapse coded occurring week last follow-interview attended. censor: Censoring status (0 = relapsed, 1 = censored). needle: Binary indicator whether cocaine ever used intravenously. base_mood: Total score positive mood subscales (Activity Happiness) Mood Questionnaire (Ryman, Biersner, & LaRocco, 1974), taken intake interview last week treatment. item used five point Likert score (ranging 0 = , 4 = extremely). followup: Week follow-interview. -mood: Total score positive mood subscales (Activity Happiness) Mood Questionnaire (Ryman, Biersner, & LaRocco, 1974), taken follow-interviews week post-treatment. item used five point Likert score (ranging 0 = , 4 = extremely). time relapse measured days follow-interviews conducted week, cocaine relapse data current form fails meet data requirement time-varying predictors: unique event time days know time-varying mood scores—everyone still risk —moments. Thus, order meet data requirement must generate predictor histories provide near-daily mood scores participant. proceeding steps develop three Cox regression models fitted cocaine relapse data illustrate compare different imputation strategies time-varying predictors. includes number days relapse cocaine use outcome variable; time-invariant predictor needle; different time-varying variable representing predictor total score positive mood subscales Mood Questionnaire (Ryman, Biersner, & LaRocco, 1974), explore following popular imputation strategies suggested Singer Willet (2003): Carry forward mood score next one available. Interpolate adjacent mood scores. Compute moving average based recent several past mood scores.","code":"glimpse(cocaine_relapse_2) #> Rows: 1,248 #> Columns: 7 #> $ id        <fct> 550, 550, 550, 550, 550, 550, 550, 550, 550, 550, 550, 550, … #> $ censor    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … #> $ days      <dbl> 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, … #> $ needle    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, … #> $ base_mood <dbl> 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 25, 25, 25, … #> $ followup  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, … #> $ mood      <dbl> 23, 27, 28, 31, 29, 32, 33, 28, 36, 33, 33, 24, 31, 19, 29, …"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-15.html","id":"exploratory-data-analysis","dir":"Articles","previous_headings":"15.1 Time-Varying Predictors > 15.1.3 Imputation Strategies for Time-Varying Predictors","what":"Exploratory Data Analysis","title":"Chapter 15: Extending the Cox regression model","text":"begin exploring time-invariant variables cocaine_relapse_2 data. convenient one Cox regression models fitted later , using person-level version cocaine_relapse_2 data. total 62 newly abstinent cocaine users (59.6%) relapsed cocaine use within 12 weeks completing abstinence-oriented treatment program. users relapsed early-follow-period.  Across sample 38 unique event times. total 69 participants (66.3%) reported previously used cocaine intravenously.","code":"cocaine_relapse_2_pl <- cocaine_relapse_2 |>   pivot_wider(     names_from = followup,     names_prefix = \"mood_\",     values_from = mood   )  glimpse(cocaine_relapse_2_pl) #> Rows: 104 #> Columns: 17 #> $ id        <fct> 550, 604, 608, 631, 513, 531, 533, 536, 599, 542, 564, 573, … #> $ censor    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … #> $ days      <dbl> 83, 83, 83, 83, 82, 82, 82, 82, 82, 81, 81, 81, 81, 81, 81, … #> $ needle    <dbl> 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, … #> $ base_mood <dbl> 29, 25, 37, 39, 33, 27, 10, 27, 28, 19, 35, 32, 32, 31, 31, … #> $ mood_1    <dbl> 23, 31, 40, 42, 43, 14, 16, 22, 29, 31, 30, 27, 27, 41, 40, … #> $ mood_2    <dbl> 27, 19, 37, 22, 25, 11, 26, 21, 28, 25, 33, 26, 27, NA, NA, … #> $ mood_3    <dbl> 28, 29, 36, 38, 34, 2, 37, 24, 25, 28, 33, 24, 23, 38, 31, 4… #> $ mood_4    <dbl> 31, 24, 36, 41, 42, 8, 17, 24, NA, 20, 33, 29, 22, 37, NA, N… #> $ mood_5    <dbl> 29, 22, 32, 41, 46, 3, 30, NA, NA, 23, 26, 21, 26, 24, 28, N… #> $ mood_6    <dbl> 32, 22, 35, 42, 42, 3, 15, 22, 16, 29, 35, 28, 28, 27, 28, 2… #> $ mood_7    <dbl> 33, NA, 34, 42, 46, 5, 16, NA, 22, 27, 35, 22, 24, 27, 31, 2… #> $ mood_8    <dbl> 28, 20, 35, 42, 46, 3, 15, 23, 23, NA, 33, 28, 17, 28, 22, 3… #> $ mood_9    <dbl> 36, 31, 29, 46, 47, 2, 21, 19, 24, NA, 29, 25, 14, 31, 24, 3… #> $ mood_10   <dbl> 33, 33, 36, NA, NA, 2, 14, 21, 16, 31, NA, NA, NA, 37, 33, N… #> $ mood_11   <dbl> 33, 30, 30, 47, 28, 0, 20, 15, 18, 23, 26, 25, 17, 38, 29, 2… #> $ mood_12   <dbl> 24, NA, 36, 43, 44, 0, 16, 18, 16, 21, NA, 25, 19, 34, 30, 2… cocaine_relapse_2_pl |>   group_by(relapsed = 1 - censor) |>   summarise(count = n()) |>   mutate(proportion = count / sum(count)) #> # A tibble: 2 × 3 #>   relapsed count proportion #>      <dbl> <int>      <dbl> #> 1        0    42      0.404 #> 2        1    62      0.596 ggplot(cocaine_relapse_2_pl, aes(x = days)) +   geom_histogram(binwidth = 7) +   scale_x_continuous(breaks = c(0, 1:12 * 7)) +   facet_wrap(vars(relapsed = 1 - censor), labeller = label_both) # We will use these event times later on during the imputation procedure for # Model B. It is important they are sorted in ascending order for this # procedure, so we do so here for convenience while creating the object. event_times <- cocaine_relapse_2_pl |>   filter(1 - censor == 1) |>   pull(days) |>   unique() |>   sort()  censor_times <- cocaine_relapse_2_pl |>   filter(censor == 1) |>   pull(days) |>   unique()  event_times |>   discard(\\(.x) .x %in% censor_times) |>   length() #> [1] 38 cocaine_relapse_2_pl |>   group_by(needle) |>   summarise(count = n()) |>   mutate(proportion = count / sum(count)) #> # A tibble: 2 × 3 #>   needle count proportion #>    <dbl> <int>      <dbl> #> 1      0    35      0.337 #> 2      1    69      0.663"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-15.html","id":"model-a-time-invariant-baseline","dir":"Articles","previous_headings":"15.1 Time-Varying Predictors > 15.1.3 Imputation Strategies for Time-Varying Predictors","what":"Model A: Time-Invariant Baseline","title":"Chapter 15: Extending the Cox regression model","text":"Model uses time-invariant predictor assessing respondent’s mood score just release treatment.","code":"model_A <- coxph(   Surv(days, 1 - censor) ~ needle + base_mood,    data = cocaine_relapse_2_pl,   ties = \"efron\" )  summary(model_A) #> Call: #> coxph(formula = Surv(days, 1 - censor) ~ needle + base_mood,  #>     data = cocaine_relapse_2_pl, ties = \"efron\") #>  #>   n= 104, number of events= 62  #>  #>                coef exp(coef)  se(coef)      z Pr(>|z|)    #> needle     1.020734  2.775232  0.314068  3.250  0.00115 ** #> base_mood -0.003748  0.996259  0.014709 -0.255  0.79886    #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #>           exp(coef) exp(-coef) lower .95 upper .95 #> needle       2.7752     0.3603    1.4996     5.136 #> base_mood    0.9963     1.0038    0.9679     1.025 #>  #> Concordance= 0.63  (se = 0.036 ) #> Likelihood ratio test= 12.51  on 2 df,   p=0.002 #> Wald test            = 10.6  on 2 df,   p=0.005 #> Score (logrank) test = 11.51  on 2 df,   p=0.003"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-15.html","id":"model-b","dir":"Articles","previous_headings":"15.1 Time-Varying Predictors > 15.1.3 Imputation Strategies for Time-Varying Predictors","what":"Model B:","title":"Chapter 15: Extending the Cox regression model","text":"Model B return person-period version cocaine_relapse_2 data explore first imputation strategy suggested Singer Willet (2003): Carrying forward mood score next one available. procedure also lag mood score predictor one week—associating, example, first followup baseline mood scores, second followup first followup’s mood scores, forth. Next, prepare cocaine_relapse_2_prevweek data modelling, want transform data set number days relapse counting process (start, stop) format, person-period format : row transformed data set represents “-risk” time interval (day_start, day_end], open left closed right. event variable row 1 time interval ends event 0 otherwise. Variable values row values apply time interval. start end points time interval determined vector unique event_times, defined earlier. censored data, end point final time interval determined time censorship—included vector unique event times—needs handled separately. Transforming cocaine_relapse_2_prevweek data counting process format two-step process. First create counting process structure, columns participant ID, start time, stop time, event status record. also add week variable indicating week record occurred , important second step process. Note step done using either person-period person-level versions cocaine_relapse_2 data however, readability use person-level data . result can obtained using person-period data wrapping calls days censor unique(). Second, join cocaine_relapse_2_prevweek data counting process structure id week, giving us counting process formatted data time-varying predictor’s values occurring appropriate time interval participant. Finally, match text, rename mood score variable week_mood. survival package also comes two utility functions, survSplit() tmerge(), can used transform data counting process format. discussion, see vignette(\"timedep\", package=\"survival\"). Now can fit Model B.","code":"cocaine_relapse_2_prevweek <- cocaine_relapse_2 |>   group_by(id) |>   mutate(     mood_previous_week = lag(mood, default = unique(base_mood)),     mood_previous_week_fill = vec_fill_missing(       mood_previous_week, direction = \"down\"     )   )  cocaine_relapse_2_prevweek #> # A tibble: 1,248 × 9 #> # Groups:   id [104] #>    id    censor  days needle base_mood followup  mood mood_previous_week #>    <fct>  <dbl> <dbl>  <dbl>     <dbl>    <int> <dbl>              <dbl> #>  1 550        1    83      1        29        1    23                 29 #>  2 550        1    83      1        29        2    27                 23 #>  3 550        1    83      1        29        3    28                 27 #>  4 550        1    83      1        29        4    31                 28 #>  5 550        1    83      1        29        5    29                 31 #>  6 550        1    83      1        29        6    32                 29 #>  7 550        1    83      1        29        7    33                 32 #>  8 550        1    83      1        29        8    28                 33 #>  9 550        1    83      1        29        9    36                 28 #> 10 550        1    83      1        29       10    33                 36 #> # ℹ 1,238 more rows #> # ℹ 1 more variable: mood_previous_week_fill <dbl> cocaine_relapse_2_prevweek_cp <- cocaine_relapse_2_pl |>   group_by(id) |>   reframe(     # For censored data the final day should be a participant's days value, so     # we need to concatenate their days to the vector of event times. The call     # to unique() around the vector removes the duplicate for uncensored data in     # the final time interval.     day_end = unique(c(event_times[event_times <= days], days)),     day_start = lag(day_end, default = 0),     event = if_else(day_end == days & censor == 0, true = 1, false = 0),     week = floor(day_end / 7) + 1   ) |>   relocate(day_start, .after = id)  cocaine_relapse_2_prevweek_cp #> # A tibble: 2,805 × 5 #>    id    day_start day_end event  week #>    <fct>     <dbl>   <dbl> <dbl> <dbl> #>  1 501           0       1     0     1 #>  2 501           1       2     0     1 #>  3 501           2       3     0     1 #>  4 501           3       4     0     1 #>  5 501           4       6     0     1 #>  6 501           6       7     0     2 #>  7 501           7       8     0     2 #>  8 501           8       9     0     2 #>  9 501           9      10     0     2 #> 10 501          10      11     0     2 #> # ℹ 2,795 more rows cocaine_relapse_2_prevweek_cp <- cocaine_relapse_2_prevweek_cp |>   left_join(     cocaine_relapse_2_prevweek,     by = join_by(id == id, week == followup)   ) |>   rename(week_mood = mood_previous_week_fill)  cocaine_relapse_2_prevweek_cp #> # A tibble: 2,805 × 12 #>    id    day_start day_end event  week censor  days needle base_mood  mood #>    <fct>     <dbl>   <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl>     <dbl> <dbl> #>  1 501           0       1     0     1      0    12      1        29    34 #>  2 501           1       2     0     1      0    12      1        29    34 #>  3 501           2       3     0     1      0    12      1        29    34 #>  4 501           3       4     0     1      0    12      1        29    34 #>  5 501           4       6     0     1      0    12      1        29    34 #>  6 501           6       7     0     2      0    12      1        29    19 #>  7 501           7       8     0     2      0    12      1        29    19 #>  8 501           8       9     0     2      0    12      1        29    19 #>  9 501           9      10     0     2      0    12      1        29    19 #> 10 501          10      11     0     2      0    12      1        29    19 #> # ℹ 2,795 more rows #> # ℹ 2 more variables: mood_previous_week <dbl>, week_mood <dbl> model_B <- coxph(   Surv(day_start, day_end, event) ~ needle + week_mood,    data = cocaine_relapse_2_prevweek_cp,   ties = \"efron\" )  summary(model_B) #> Call: #> coxph(formula = Surv(day_start, day_end, event) ~ needle + week_mood,  #>     data = cocaine_relapse_2_prevweek_cp, ties = \"efron\") #>  #>   n= 2805, number of events= 62  #>  #>               coef exp(coef) se(coef)      z Pr(>|z|)     #> needle     1.07959   2.94348  0.31574  3.419 0.000628 *** #> week_mood -0.03490   0.96570  0.01387 -2.517 0.011832 *   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #>           exp(coef) exp(-coef) lower .95 upper .95 #> needle       2.9435     0.3397    1.5853    5.4654 #> week_mood    0.9657     1.0355    0.9398    0.9923 #>  #> Concordance= 0.662  (se = 0.037 ) #> Likelihood ratio test= 18.61  on 2 df,   p=9e-05 #> Wald test            = 16.62  on 2 df,   p=2e-04 #> Score (logrank) test = 17.49  on 2 df,   p=2e-04"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-15.html","id":"model-c","dir":"Articles","previous_headings":"15.1 Time-Varying Predictors > 15.1.3 Imputation Strategies for Time-Varying Predictors","what":"Model C:","title":"Chapter 15: Extending the Cox regression model","text":"Model C also start lagged weekly mood scores, however, use different imputation strategy: Interpolating adjacent mood scores. Although Singer Willet (2003) suggest “resisting temptation design sophisticated imputation algorithms,” approach interpolating adjacent mood scores somewhat complex. Consequently, need create function suit purpose, rather using existing functions like zoo::na.approx() imputeTS::na_ma(). Singer Willet (2003) describe approach text, algorithm appears based following rules: Trailing NAs imputed consecutively using recent non-missing mood score. Internal NAs imputed using mean adjacent non-missing mood scores. consecutive internal NAs, following first imputed mood score sequence, every NA thereafter imputed using mean previous NA value’s imputed mood score next non-missing mood score. Imputed mood scores rounded nearest integer. Now can impute lagged weekly mood scores using na_adjacent() function. Next, prepare cocaine_relapse_2_adjacent data modelling, transform counting process format; however, Model C “-risk” time interval one day long. Following Singer Willet (2003), construct day_mood variable linearly interpolating adjacent weekly values yield daily values, assigning given day mood value imputed immediate prior day. Now can fit Model C. Table 15.2, page 555:","code":"na_adjacent <- function(x) {   # The while loop is used here to allow us to carry forward imputed mood scores   # for consecutive internal NAs.   x_avg <- x   while (any(is.na(x_avg[2:length(x)]))) {     x_avg <- pslide_dbl(       list(         x_avg,         vec_fill_missing(x_avg, direction = \"down\"),         vec_fill_missing(x_avg, direction = \"up\")       ),       \\(.x, .x_fill_down, .x_fill_up) {         case_when(           # Rule 1:           all(is.na(.x[3:length(.x)])) ~ .x_fill_down[2],           # Rule 2:           !is.na(.x[1]) & is.na(.x[2]) ~ mean(c(.x_fill_up[1], .x_fill_up[2])),           TRUE ~ .x[2]         )       },       .before = 1,       .after = Inf,       .complete = TRUE     )          # Rule 3. We are not using round() here because it goes to the even digit when     # rounding off a 5, rather than always going upward.     x_avg <- if_else(x_avg %% 1 < .5, floor(x_avg), ceiling(x_avg))          x_avg[1] <- x[1]   }      x_avg } cocaine_relapse_2_adjacent <- cocaine_relapse_2_prevweek |>   group_by(id) |>   mutate(     # It's important to include the final follow-up when imputing between     # adjacent mood scores, otherwise cases where the second last score is an     # internal NA will fill down instead of using the mean between adjacent mood     # scores. However, afterwards the final follow-up can be dropped.     mood_adjacent_lag = na_adjacent(c(mood_previous_week, last(mood)))[-13],     # We also want the non-lagged mood scores for later, which we impute using     # similar logic.     mood_adjacent = na_adjacent(c(first(mood_previous_week), mood))[-1]   )  # Here is a small preview of the difference between the imputation strategies # for Models B and C: cocaine_relapse_2_adjacent |>   filter(id == 544) |>   select(id, followup, mood_previous_week:mood_adjacent_lag) #> # A tibble: 12 × 5 #> # Groups:   id [1] #>    id    followup mood_previous_week mood_previous_week_fill mood_adjacent_lag #>    <fct>    <int>              <dbl>                   <dbl>             <dbl> #>  1 544          1                 40                      40                40 #>  2 544          2                 40                      40                40 #>  3 544          3                 38                      38                38 #>  4 544          4                 27                      27                27 #>  5 544          5                 NA                      27                25 #>  6 544          6                 22                      22                22 #>  7 544          7                 NA                      22                21 #>  8 544          8                 NA                      22                21 #>  9 544          9                 20                      20                20 #> 10 544         10                 NA                      20                25 #> 11 544         11                 30                      30                30 #> 12 544         12                 28                      28                28 cocaine_relapse_2_adjacent_cp <- cocaine_relapse_2_adjacent |>   group_by(id, followup) |>   reframe(     day_end = (followup - 1) * 7 + 1:7,     day_start = day_end - 1,     days = unique(days),     censor = unique(censor),     event = if_else(       day_end == days & censor == 0,       true = 1, false = 0     ),     needle = unique(needle),     mood_day = approx(c(mood_adjacent_lag, mood_adjacent), n = 8)[[2]][1:7],   ) |>   relocate(day_start, day_end, days, .after = id) |>   filter(day_end <= days)  cocaine_relapse_2_adjacent_cp #> # A tibble: 4,948 × 9 #>    id    day_start day_end  days followup censor event needle mood_day #>    <fct>     <dbl>   <dbl> <dbl>    <int>  <dbl> <dbl>  <dbl>    <dbl> #>  1 501           0       1    12        1      0     0      1     29   #>  2 501           1       2    12        1      0     0      1     29.7 #>  3 501           2       3    12        1      0     0      1     30.4 #>  4 501           3       4    12        1      0     0      1     31.1 #>  5 501           4       5    12        1      0     0      1     31.9 #>  6 501           5       6    12        1      0     0      1     32.6 #>  7 501           6       7    12        1      0     0      1     33.3 #>  8 501           7       8    12        2      0     0      1     34   #>  9 501           8       9    12        2      0     0      1     31.9 #> 10 501           9      10    12        2      0     0      1     29.7 #> # ℹ 4,938 more rows model_C <- coxph(   Surv(day_start, day_end, event) ~ needle + mood_day,    data = cocaine_relapse_2_adjacent_cp,   ties = \"efron\" )  summary(model_C) #> Call: #> coxph(formula = Surv(day_start, day_end, event) ~ needle + mood_day,  #>     data = cocaine_relapse_2_adjacent_cp, ties = \"efron\") #>  #>   n= 4948, number of events= 62  #>  #>              coef exp(coef) se(coef)      z Pr(>|z|)     #> needle    1.12077   3.06720  0.31700  3.536 0.000407 *** #> mood_day -0.05438   0.94707  0.01489 -3.651 0.000261 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #>          exp(coef) exp(-coef) lower .95 upper .95 #> needle      3.0672      0.326    1.6478    5.7091 #> mood_day    0.9471      1.056    0.9198    0.9751 #>  #> Concordance= 0.695  (se = 0.036 ) #> Likelihood ratio test= 25.52  on 2 df,   p=3e-06 #> Wald test            = 23.1  on 2 df,   p=1e-05 #> Score (logrank) test = 24.04  on 2 df,   p=6e-06 # TODO"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-15.html","id":"nonproportional-hazards-models-via-stratification","dir":"Articles","previous_headings":"","what":"15.2 Nonproportional Hazards Models via Stratification","title":"Chapter 15: Extending the Cox regression model","text":"Figure 15.2, page 559:  Table 15.3, page 560:","code":"# FIXME: The upper limit of the data doesn't match the textbook. survfit(Surv(used_cocaine_age, 1 - censor) ~ rural, data = first_cocaine) |>   tidy() |>   mutate(     strata = stringr::str_remove(strata, \"rural=\"),     cumulative_hazard = -log(estimate),     log_cumulative_hazard = log(cumulative_hazard)   ) |>   rename(rural = strata) |>   ggplot(aes(x = time, y = log_cumulative_hazard, linetype = rural)) +     geom_line() +     coord_cartesian(ylim = c(-6, -1)) # first_cocaine_model_C from earlier is the first model first_cocaine_model_C #> Call: #> coxph(formula = Surv(age_start, age_end, used_cocaine) ~ birthyr +  #>     used_marijuana + used_drugs + sold_marijuana + sold_drugs,  #>     data = first_cocaine_pp_C, ties = \"efron\") #>  #>                    coef exp(coef) se(coef)     z        p #> birthyr         0.08493   1.08864  0.02183 3.890    1e-04 #> used_marijuana  2.45920  11.69542  0.28357 8.672  < 2e-16 #> used_drugs      1.25110   3.49419  0.15656 7.991 1.34e-15 #> sold_marijuana  0.68989   1.99349  0.12263 5.626 1.84e-08 #> sold_drugs      0.76037   2.13908  0.13066 5.819 5.91e-09 #>  #> Likelihood ratio test=944.5  on 5 df, p=< 2.2e-16 #> n= 3312, number of events= 382  first_cocaine_model_stratified <- update(   first_cocaine_model_C, . ~ . + strata(rural) )  first_cocaine_model_nonrural <- update(   first_cocaine_model_C, subset = rural == 0 )  first_cocaine_model_rural <- update(   first_cocaine_model_C, subset = rural == 1 )  # TODO: Make table."},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-15.html","id":"nonproportional-hazards-models-via-interactions-with-time","dir":"Articles","previous_headings":"","what":"15.3 Nonproportional Hazards Models via Interactions with Time","title":"Chapter 15: Extending the Cox regression model","text":"Table 15.4 page 566: Figure 15.3 page 567:","code":"# TODO psychiatric_discharge #> # A tibble: 174 × 4 #>    id     days censor treatment_plan #>    <fct> <dbl>  <dbl>          <dbl> #>  1 2         3      0              1 #>  2 8        46      0              0 #>  3 73       30      0              0 #>  4 76       45      0              0 #>  5 78       22      0              0 #>  6 79       50      0              0 #>  7 81       59      0              0 #>  8 83       44      0              0 #>  9 95       44      0              1 #> 10 117      22      0              0 #> # ℹ 164 more rows # FIXME: The upper limit of the data doesn't match the textbook. survfit(Surv(days, 1 - censor) ~ treatment_plan, data = psychiatric_discharge) |>   tidy() |>   mutate(     strata = stringr::str_remove(strata, \"treatment_plan=\"),     cumulative_hazard = -log(estimate),     log_cumulative_hazard = log(cumulative_hazard)   ) |>   rename(treatment_plan = strata) |>   ggplot(aes(x = time, y = log_cumulative_hazard, linetype = treatment_plan)) +     geom_hline(yintercept = 0, linewidth = .25, linetype = 3) +     geom_line() +     coord_cartesian(xlim = c(0, 77), ylim = c(-4, 2)) # TODO: Bottom panel"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-15.html","id":"regression-diagnostics","dir":"Articles","previous_headings":"","what":"15.4 Regression Diagnostics","title":"Chapter 15: Extending the Cox regression model","text":"Figure 15.4 page 573:  Figure 15.5 page 577:  Figure 15.6 page 580:  Figure 15.7 page 583:","code":"rearrest <- rearrest |>   mutate(rank_time = rank(months, ties.method = \"average\"), .after = \"months\")  rearrest_null_model <- coxph(Surv(months, 1 - censor) ~ 1, data = rearrest)  rearrest_full_model <- update(   rearrest_null_model, . ~ . + person_crime + property_crime + age )  rearrest_models <- list(   null = rearrest_null_model,   full = rearrest_full_model )  rearrest_fits <- rearrest_models |>   map(     \\(.x) {       map_df(         list(martingale = \"martingale\", deviance = \"deviance\"),         \\(.y) augment(           .x, data = rearrest, type.predict = \"lp\", type.residuals = .y         ),         .id = \".resid_type\"       )     }   ) |>   list_rbind(names_to = \"model\") |>   mutate(     model = factor(model, levels = c(\"null\", \"full\")),     censored = as.logical(censor)   )  rearrest_fits |>   filter(.resid_type == \"martingale\") |>   ggplot(aes(x = age, y = .resid)) +     geom_hline(yintercept = 0, linewidth = .25, linetype = 3) +     geom_point(aes(shape = censored)) +     scale_shape_manual(values = c(16, 3)) +     geom_smooth(se = FALSE) +     facet_wrap(vars(model), ncol = 1, labeller = label_both) +     coord_cartesian(ylim = c(-3, 1)) #> `geom_smooth()` using method = 'loess' and formula = 'y ~ x' stem(resid(rearrest_full_model, type = \"deviance\"), scale = 2) #>  #>   The decimal point is 1 digit(s) to the left of the | #>  #>   -22 | 09 #>   -20 | 21 #>   -18 | 6491 #>   -16 | 969321 #>   -14 | 54216 #>   -12 | 87654741 #>   -10 | 208776542110 #>    -8 | 99852176544 #>    -6 | 876322098874400 #>    -4 | 444443332877644210 #>    -2 | 85009955411 #>    -0 | 997597551 #>     0 | 268979 #>     2 | 0318 #>     4 | 133678802337 #>     6 | 2688919 #>     8 | 1136690012233889999 #>    10 | 122334724789 #>    12 | 0115 #>    14 | 0228055578 #>    16 | 03795 #>    18 | 2336 #>    20 | 0735 #>    22 | 6 #>    24 | 00 #>    26 | 7  rearrest_fits |>   filter(model == \"full\" & .resid_type == \"deviance\") |>   ggplot(aes(x = .fitted, y = .resid)) +     geom_hline(yintercept = 0, linewidth = .25, linetype = 3) +     geom_point(aes(shape = censored)) +     scale_shape_manual(values = c(16, 3)) +     scale_x_continuous(breaks = -3:3) +     scale_y_continuous(breaks = -3:3) +     coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) # augment.coxph is bugged and won't return the .resid column when using # `newdata`, likely related to this issue: https://github.com/tidymodels/broom/issues/937 # So this code doesn't work: # augment( #   rearrest_full_model, #   newdata = filter(rearrest, censor == 0), #   type.predict = \"lp\", #   type.residuals = \"schoenfeld\" # ) # Likewise, `data` can't be used because it expects the full dataset; thus, it # will error out even when using the filtered data.  # However, updating the model first does work: # Schoenfeld residuals only pertain to those who experience the event, so we need # to update the model before retrieving them, and only use a subset of the data # when getting predictions. rearrest_full_model |>   update(subset = censor == 0) |>   augment(     data = filter(rearrest, censor == 0),     type.predict = \"lp\",     type.residuals = \"schoenfeld\"   ) |>   mutate(.resid = as.data.frame(.resid)) |>   unnest_wider(col = .resid, names_sep = \"_\") |>   pivot_longer(     cols = starts_with(\".resid\"),     names_to = \"predictor\",     values_to = \".resid\"   ) |>   mutate(     predictor = stringr::str_remove(predictor, \".resid_\"),     predictor = factor(       predictor, levels = c(\"person_crime\", \"property_crime\", \"age\")     )   ) |>   ggplot(aes(x = rank_time, y = .resid)) +     geom_hline(yintercept = 0, linewidth = .25, linetype = 3) +     geom_point() +     scale_shape_manual(values = c(16, 3)) +     geom_smooth(se = FALSE, span = 1) +     facet_wrap(       vars(predictor), ncol = 1, scales = \"free_y\", labeller = label_both     ) +     scale_x_continuous(n.breaks = 8) +     ggh4x::facetted_pos_scales(       y = list(         predictor == \"person_crime\" ~ scale_y_continuous(limits = c(-.5, 1)),         predictor == \"property_crime\" ~ scale_y_continuous(           n.breaks = 7, limits = c(-1, .2)         ),         predictor == \"age\" ~ scale_y_continuous(limits = c(-10, 20))       )     ) +     coord_cartesian(xlim = c(0, 175)) #> `geom_smooth()` using method = 'loess' and formula = 'y ~ x' # TODO: set y-axis scales to match textbook. rearrest_full_model |>   augment(     data = rearrest,     type.predict = \"lp\",     type.residuals = \"score\"   ) |>   mutate(.resid = as.data.frame(.resid)) |>   unnest_wider(col = .resid, names_sep = \"_\") |>   pivot_longer(     cols = starts_with(\".resid\"),     names_to = \"predictor\",     values_to = \".resid\"   ) |>   mutate(     predictor = stringr::str_remove(predictor, \".resid_\"),     predictor = factor(       predictor, levels = c(\"person_crime\", \"property_crime\", \"age\")     ),     censored = as.logical(censor)   ) |>   ggplot(aes(x = rank_time, y = .resid)) +     geom_hline(yintercept = 0, linewidth = .25, linetype = 3) +     geom_point(aes(shape = censored)) +     scale_shape_manual(values = c(16, 3)) +     facet_wrap(       vars(predictor), ncol = 1, scales = \"free_y\", labeller = label_both     )"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-15.html","id":"competing-risks","dir":"Articles","previous_headings":"","what":"15.5 Competing Risks","title":"Chapter 15: Extending the Cox regression model","text":"Figure 15.8 page 589:  Table 15.7 page 592:","code":"judges_null_models <- list(   dead = survfit(Surv(tenure, dead) ~ 1, data = judges),   retired = survfit(Surv(tenure, retired) ~ 1, data = judges) )  judges_null_models_tidy <- map(   judges_null_models,   \\(.x) {     .x |>       survfit0() |>       tidy() |>       mutate(cumulative_hazard = -log(estimate)) |>       select(time, survival = estimate, cumulative_hazard) |>       pivot_longer(         cols = c(survival, cumulative_hazard),         names_to = \"statistic\",         values_to = \"estimate\"       )   } )  # Estimate and tidy smoothed hazards judges_kernel_smoothed_hazards_tidy <- map(   list(     judges_dead = judges$dead,     judges_retired = judges$retired   ),   \\(event) {     kernel_smoothed_hazard <- muhaz(       judges$tenure,       event,       min.time = min(judges$tenure[event == 0]) + 6,       max.time = max(judges$tenure[event == 0]) - 6,       bw.grid = 6,       bw.method = \"global\",       b.cor = \"none\",       kern = \"epanechnikov\"     )          kernel_smoothed_hazard |>       tidy() |>       mutate(statistic = \"hazard\")   } ) #> Warning in muhaz(judges$tenure, event, min.time = min(judges$tenure[event == : minimum time > minimum Survival Time #> Warning in muhaz(judges$tenure, event, min.time = min(judges$tenure[event == : minimum time > minimum Survival Time  # Combine estimates estimates_tidy <- map2_df(   judges_null_models_tidy, judges_kernel_smoothed_hazards_tidy,   \\(.x, .y) {     bind_rows(.x, .y) |>       mutate(statistic = factor(         statistic, levels = c(\"survival\", \"cumulative_hazard\", \"hazard\"))       )   },   .id = \"event\" )  ggplot(estimates_tidy, aes(x = time, y = estimate, linetype = event)) +   geom_step(data = \\(.x) filter(.x, statistic != \"hazard\")) +   geom_line(data = \\(.x) filter(.x, statistic == \"hazard\")) +   facet_wrap(vars(statistic), ncol = 1, scales = \"free_y\") judges_model_A <- coxph(   Surv(tenure, dead) ~ appointment_age + appointment_year,   data = judges )  judges_model_B <- coxph(   Surv(tenure, retired) ~ appointment_age + appointment_year,   data = judges )  judges_model_C <- coxph(   Surv(tenure, left_appointment) ~ appointment_age + appointment_year,   data = judges )  # TODO: Make table."},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-15.html","id":"late-entry-into-the-risk-set","dir":"Articles","previous_headings":"","what":"15.6 Late Entry into the Risk Set","title":"Chapter 15: Extending the Cox regression model","text":"Table 15.8 page 601:","code":"# Model A ----  # First we need to transform to a counting process format. physicians_event_times_A <- physicians |>   filter(1 - censor == 1) |>   pull(exit) |>   unique() |>   sort()  # We'll use survSplit() this time around. physicians_cp_A <- physicians |>   mutate(event = 1 - censor) |>   survSplit(     Surv(entry, exit, event) ~ .,     data = _,     cut = physicians_event_times_A,     end = \"exit\"   ) |>   as_tibble()  # The warning message here can be ignored. physicians_model_A <- coxph(   Surv(entry, exit, event) ~ part_time + age + age:exit,   data = physicians_cp_A ) #> Warning in coxph(Surv(entry, exit, event) ~ part_time + age + age:exit, : a #> variable appears on both the left and right sides of the formula  # Model B ---- physicians_event_times_B <- physicians |>   filter(1 - censor == 1 & entry == 0) |>   pull(exit) |>   unique() |>   sort()  physicians_cp_B <- physicians |>   filter(entry == 0) |>   mutate(event = 1 - censor) |>   survSplit(     Surv(entry, exit, event) ~ .,     data = _,     cut = physicians_event_times_B,     end = \"exit\"   ) |>   as_tibble()  physicians_model_B <- coxph(   Surv(entry, exit, event) ~ part_time + age + age:exit,   data = physicians_cp_B ) #> Warning in coxph(Surv(entry, exit, event) ~ part_time + age + age:exit, : a #> variable appears on both the left and right sides of the formula  # Model C ---- physicians_cp_C <- physicians |>   mutate(     event = 1 - censor,     entry = 0   ) |>   survSplit(     Surv(entry, exit, event) ~ .,     data = _,     cut = physicians_event_times_A,     end = \"exit\"   ) |>   as_tibble()  physicians_model_C <- coxph(   Surv(entry, exit, event) ~ part_time + age + age:exit,   data = physicians_cp_C ) #> Warning in coxph(Surv(entry, exit, event) ~ part_time + age + age:exit, : a #> variable appears on both the left and right sides of the formula  # TODO: Make table and clean up code."},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-15.html","id":"using-late-entrants-to-introduce-alternative-metrics-for-clocking-time","dir":"Articles","previous_headings":"15.6 Late Entry into the Risk Set","what":"15.6.2 Using Late Entrants to Introduce Alternative Metrics for Clocking Time","title":"Chapter 15: Extending the Cox regression model","text":"Table 15.9 page 604:","code":"monkeys_model_A <- coxph(   Surv(sessions, 1 - censor) ~ initial_age + birth_weight + female,   data = monkeys )  monkeys_model_B <- update(monkeys_model_A, Surv(end_age, 1 - censor) ~ .)  # The warning message here can be ignored. monkeys_model_C <- update(   monkeys_model_A, Surv(initial_age, end_age, 1 - censor) ~ . ) #> Warning in coxph(formula = Surv(initial_age, end_age, 1 - censor) ~ initial_age #> + : a variable appears on both the left and right sides of the formula  # TODO: Make table."},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-2.html","id":"creating-a-longitudinal-data-set","dir":"Articles","previous_headings":"","what":"2.1 Creating a longitudinal data set","title":"Chapter 2: Exploring longitudinal data on change","text":"Section 2.1 Singer Willet (2003) introduce two distinct formats data organization longitudinal data—person-level format person-period format—using subset data National Youth Survey (NYS) measuring development tolerance towards deviant behaviour adolescents time relation self-reported sex exposure deviant peers (Raudenbush & Chan, 1992). Adolescents’ tolerance towards deviant behaviour based 9-item scale measuring attitudes tolerant deviant behaviour. scale administered year age 11 15 time-varying variable. However, adolescents’ self-reported sex exposure deviant peers recorded beginning study period time-invariant variables. example illustrate difference two formats using deviant_tolerance_pl deviant_tolerance_pp data sets, correspond adolescent tolerance deviant behaviour data organized person-level person-period formats, respectively.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-2.html","id":"the-person-level-data-set","dir":"Articles","previous_headings":"2.1 Creating a longitudinal data set","what":"The Person-Level Data Set","title":"Chapter 2: Exploring longitudinal data on change","text":"person-level format (also known wide multivariate format), person one row data multiple columns containing data measurement occasion time-varying variables. demonstrated deviant_tolerance_pl data set, person-level data frame 16 rows 8 columns: id: Participant ID. tolerance_11, tolerance_12, tolerance_13, tolerance_14, tolerance_15: Average score across 9-item scale assessing attitudes favourable deviant behaviour ages 11, 12, 13, 14, 15. item used four point scale (1 = wrong, 2 = wrong, 3 = little bit wrong, 4 = wrong ). male: Binary indicator whether adolescent male. exposure: Average score across 9-item scale assessing level exposure deviant peers. item used five point Likert score (ranging 0 = none, 4 = ). Although person-level format common cross-sectional research, four disadvantages make ill-suited longitudinal data analysis: restricts data analysis examining rank order wave--wave relationships, leading non-informative summaries tell us person changes time, even direction change. omits explicit time-indicator variable, rendering time unavailable data analysis. requires adding additional variable data set unique measurement occasion, making inefficient useless number spacing measurement occasions varies across individuals. requires adding additional set columns time-varying predictor (one column per measurement occasion), rendering unable easily handle presence time-varying predictors. Singer Willet (2003) exemplify first disadvantages postulating one might analyze person-level tolerance towards deviant behaviour data set. natural approach summarize wave--wave relationships among tolerance_11 tolerance_15 using bivariate correlations bivariate plots; however, tell us anything adolescent tolerance towards deviant behaviour changed time either individuals groups. Rather, weak positive correlation measurement occasions merely tells us rank order tolerance towards deviant behaviour remained relatively stable across occasions—, adolescents tolerant towards deviant behaviour one measurement occasion tended tolerant next. first disadvantage also apparent examining bivariate plots measurement occasions: way tell adolescent tolerance towards deviant behaviour changed time either individuals groups. Moreover, lack explicit time-indicator variable, isn’t possible plot person-level data set meaningful way, time series plot organized id.  Considered together, disadvantages make person-level format ill-suited longitudinal data analyses. Fortunately, disadvantages person-level format can addressed simple conversion person-period format.","code":"deviant_tolerance_pl #> # A tibble: 16 × 8 #>    id    tolerance_11 tolerance_12 tolerance_13 tolerance_14 tolerance_15  male #>    <fct>        <dbl>        <dbl>        <dbl>        <dbl>        <dbl> <dbl> #>  1 9             2.23         1.79         1.9          2.12         2.66     0 #>  2 45            1.12         1.45         1.45         1.45         1.99     1 #>  3 268           1.45         1.34         1.99         1.79         1.34     1 #>  4 314           1.22         1.22         1.55         1.12         1.12     0 #>  5 442           1.45         1.99         1.45         1.67         1.9      0 #>  6 514           1.34         1.67         2.23         2.12         2.44     1 #>  7 569           1.79         1.9          1.9          1.99         1.99     0 #>  8 624           1.12         1.12         1.22         1.12         1.22     1 #>  9 723           1.22         1.34         1.12         1            1.12     0 #> 10 918           1            1            1.22         1.99         1.22     0 #> 11 949           1.99         1.55         1.12         1.45         1.55     1 #> 12 978           1.22         1.34         2.12         3.46         3.32     1 #> 13 1105          1.34         1.9          1.99         1.9          2.12     1 #> 14 1542          1.22         1.22         1.99         1.79         2.12     0 #> 15 1552          1            1.12         2.23         1.55         1.55     0 #> 16 1653          1.11         1.11         1.34         1.55         2.12     0 #> # ℹ 1 more variable: exposure <dbl> # Table 2.1, page 20: deviant_tolerance_pl |>   select(starts_with(\"tolerance\")) |>   correlate(diagonal = 1) |>   shave() |>   fashion() #>           term tolerance_11 tolerance_12 tolerance_13 tolerance_14 tolerance_15 #> 1 tolerance_11         1.00                                                     #> 2 tolerance_12          .66         1.00                                        #> 3 tolerance_13          .06          .25         1.00                           #> 4 tolerance_14          .14          .21          .59         1.00              #> 5 tolerance_15          .26          .39          .57          .83         1.00 deviant_tolerance_pl |>   select(starts_with(\"tolerance\")) |>   pairs()"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-2.html","id":"the-person-period-data-set","dir":"Articles","previous_headings":"2.1 Creating a longitudinal data set","what":"The Person-Period Data Set","title":"Chapter 2: Exploring longitudinal data on change","text":"person-period format (also known long univariate format), person one row data measurement occasion, participant identifier variable person, time-indicator variable measurement occasion. format, time-invariant variables identical values across measurement occasion; whereas time-varying variables potentially differing values. demonstrated deviant_tolerance_pp data set, person-period data frame 80 rows 5 columns: id: Participant ID. age: Adolescent age years. tolerance: Average score across 9-item scale assessing attitudes favourable deviant behaviour. item used four point scale (1 = wrong, 2 = wrong, 3 = little bit wrong, 4 = wrong ). male: Binary indicator whether adolescent male. exposure: Average score across 9-item scale assessing level exposure deviant peers. item used five point Likert score (ranging 0 = none, 4 = ). Although person-period data set contains information person-level data set, format data organization makes amenable longitudinal data analysis, specifically: includes explicit participant identifier variable, enabling data sorted person-specific subsets. includes explicit time-indicator variable, rendering time available data analysis, accommodating research designs number spacing measurement occasions varies across individuals. needs single column variable data set—whether time-varying time-invariant, outcome predictor—making trivial handle number variables. Indeed, R functions designed work data person-period format—falls larger umbrella tidy data format—due R’s vectorized nature. Wickham, Çetinkaya-Rundel, Grolemund (2023) explain, three interrelated rules make data set tidy: variable must column. observation must row. value must cell. Thus, person-period format simply special case tidy data format, distinguishes longitudinal nature requirements explicit participant identifier time-indicator variables.","code":"deviant_tolerance_pp #> # A tibble: 80 × 5 #>    id      age tolerance  male exposure #>    <fct> <dbl>     <dbl> <dbl>    <dbl> #>  1 9        11      2.23     0     1.54 #>  2 9        12      1.79     0     1.54 #>  3 9        13      1.9      0     1.54 #>  4 9        14      2.12     0     1.54 #>  5 9        15      2.66     0     1.54 #>  6 45       11      1.12     1     1.16 #>  7 45       12      1.45     1     1.16 #>  8 45       13      1.45     1     1.16 #>  9 45       14      1.45     1     1.16 #> 10 45       15      1.99     1     1.16 #> # ℹ 70 more rows"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-2.html","id":"converting-between-person-level-and-person-period-data-sets","dir":"Articles","previous_headings":"2.1 Creating a longitudinal data set","what":"Converting Between Person-Level and Person-Period Data Sets","title":"Chapter 2: Exploring longitudinal data on change","text":"Unfortunately, longitudinal data often initially stored person-level data set, meaning real analyses require least little tidying get data person-period format. reasons : Many people aren’t familiar principles tidy data—special cases like person person-period format—’s hard derive without spending lot time working longitudinal data. person-level format closely resembles familiar cross-sectional data-set format, making seemingly sensible default inexperienced analysts. Data often organized facilitate non-analytical goals, data entry, rather data analysis. Thus, essential skill aspiring longitudinal data analyst able convert person-level data set person-period data set. tidyr package provides two functions can easily convert longitudinal data set one format : pivot_longer() pivot_wider(). convert person-level data set person-period data set use pivot_longer(): person-level data, five key arguments: cols specifies columns need pivoted longer format—longitudinal data, always columns corresponding time-varying variables. argument uses tidy selection, small data science language selecting columns data frame (?tidyr_tidy_select), making simple select column time-varying variable based naming pattern. names_to names new column (columns) create information stored column names specified cols. named new column age. names_prefix removes matching text start column name—longitudinal data, always prefix time-varying variables separating variable name measurement occasion. argument uses regular expression select matching text. names_transform applies function new column (columns). converted new column age type character type integer. values_to names new column (columns) create data stored cell values. named new column tolerance. Note \"age\" \"tolerance\" quoted call pivot_longer() represent column names new variables ’re creating, rather already-existing variables data. Although longitudinal data analyses begin getting data person-period format, can occasionally useful go opposite direction. computations can made easier using person-period data set, certain functions analyses expect person-period data set; therefore, ’s helpful know untidy, transform, re-tidy data needed. convert person-period data set person-level data set use dplyr::pivot_wider(): person-period data, three key arguments: names_from specifies column (columns) get name output columns —longitudinal data, always columns corresponding time-indicator variables. names_prefix adds specified string start output column name—longitudinal data, always prefix time-varying variables separating variable name measurement occasion. values_from specifies column (columns) get cell values —longitudinal data, always columns corresponding time-varying variables. learn principles tidy data pivoting works, see Data Tidying chapter R Data Science.","code":"# Figure 2.1, page 18: pivot_longer(   deviant_tolerance_pl,   cols = starts_with(\"tolerance_\"),   names_to = \"age\",   names_prefix = \"tolerance_\",   names_transform = as.integer,   values_to = \"tolerance\" ) #> # A tibble: 80 × 5 #>    id     male exposure   age tolerance #>    <fct> <dbl>    <dbl> <int>     <dbl> #>  1 9         0     1.54    11      2.23 #>  2 9         0     1.54    12      1.79 #>  3 9         0     1.54    13      1.9  #>  4 9         0     1.54    14      2.12 #>  5 9         0     1.54    15      2.66 #>  6 45        1     1.16    11      1.12 #>  7 45        1     1.16    12      1.45 #>  8 45        1     1.16    13      1.45 #>  9 45        1     1.16    14      1.45 #> 10 45        1     1.16    15      1.99 #> # ℹ 70 more rows pivot_wider(   deviant_tolerance_pp,   names_from = age,   names_prefix = \"tolerance_\",   values_from = tolerance ) #> # A tibble: 16 × 8 #>    id     male exposure tolerance_11 tolerance_12 tolerance_13 tolerance_14 #>    <fct> <dbl>    <dbl>        <dbl>        <dbl>        <dbl>        <dbl> #>  1 9         0     1.54         2.23         1.79         1.9          2.12 #>  2 45        1     1.16         1.12         1.45         1.45         1.45 #>  3 268       1     0.9          1.45         1.34         1.99         1.79 #>  4 314       0     0.81         1.22         1.22         1.55         1.12 #>  5 442       0     1.13         1.45         1.99         1.45         1.67 #>  6 514       1     0.9          1.34         1.67         2.23         2.12 #>  7 569       0     1.99         1.79         1.9          1.9          1.99 #>  8 624       1     0.98         1.12         1.12         1.22         1.12 #>  9 723       0     0.81         1.22         1.34         1.12         1    #> 10 918       0     1.21         1            1            1.22         1.99 #> 11 949       1     0.93         1.99         1.55         1.12         1.45 #> 12 978       1     1.59         1.22         1.34         2.12         3.46 #> 13 1105      1     1.38         1.34         1.9          1.99         1.9  #> 14 1542      0     1.44         1.22         1.22         1.99         1.79 #> 15 1552      0     1.04         1            1.12         2.23         1.55 #> 16 1653      0     1.25         1.11         1.11         1.34         1.55 #> # ℹ 1 more variable: tolerance_15 <dbl>"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-2.html","id":"descriptive-analysis-of-individual-change-over-time","dir":"Articles","previous_headings":"","what":"2.2 Descriptive analysis of individual change over time","title":"Chapter 2: Exploring longitudinal data on change","text":"Section 2.2 Singer Willet (2003) use deviant_tolerance_pp data set demonstrate person-period format facilitates exploratory analyses describe individuals data set change time, revealing nature idiosyncrasies person’s temporal pattern change.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-2.html","id":"empirical-growth-plots","dir":"Articles","previous_headings":"2.2 Descriptive analysis of individual change over time","what":"Empirical Growth Plots","title":"Chapter 2: Exploring longitudinal data on change","text":"Empirical growth plots show, individual, sequence change time-varying variable. change can evaluated either absolute terms scale variable interest, relative terms comparison sample members. Singer Willet (2003) identify several questions helpful answer examining empirical growth plots: increasing? decreasing? increasing ? least? decreasing ? least? anyone increase decrease? anyone decrease increase? construct empirical growth plot ggplot2 package, put time-indicator x-axis, time-varying variable y-axis, facet individual separate panel.  data set large, Singer Willet (2003) suggest constructing empirical growth plots randomly selected subsample individuals—perhaps stratified groups defined values important predictors—rather using entire sample. task can easily accomplished using filter() function dplyr package prior plotting. example, sample four random adolescents. Note use set.seed() function prior sampling, sets state R’s random number generator: results random sample reproducible. approach can also extended randomly select subsample individuals within different strata combining group_split() function dplyr package split data list different groups, map() function purrr package apply filter() call previous example group. example, sample two random adolescent males two random adolescent females, combine filtered data frames list back together using list_rbind() function purrr package.","code":"# Figure 2.2, page 25: deviant_tolerance_empgrowth <- deviant_tolerance_pp |>   ggplot(aes(x = age, y = tolerance)) +     geom_point() +     coord_cartesian(ylim = c(0, 4)) +     facet_wrap(vars(id), labeller = label_both)  deviant_tolerance_empgrowth set.seed(345)  deviant_tolerance_pp |>   filter(id %in% sample(unique(id), size = 4)) #> # A tibble: 20 × 5 #>    id      age tolerance  male exposure #>    <fct> <dbl>     <dbl> <dbl>    <dbl> #>  1 268      11      1.45     1     0.9  #>  2 268      12      1.34     1     0.9  #>  3 268      13      1.99     1     0.9  #>  4 268      14      1.79     1     0.9  #>  5 268      15      1.34     1     0.9  #>  6 442      11      1.45     0     1.13 #>  7 442      12      1.99     0     1.13 #>  8 442      13      1.45     0     1.13 #>  9 442      14      1.67     0     1.13 #> 10 442      15      1.9      0     1.13 #> 11 569      11      1.79     0     1.99 #> 12 569      12      1.9      0     1.99 #> 13 569      13      1.9      0     1.99 #> 14 569      14      1.99     0     1.99 #> 15 569      15      1.99     0     1.99 #> 16 1105     11      1.34     1     1.38 #> 17 1105     12      1.9      1     1.38 #> 18 1105     13      1.99     1     1.38 #> 19 1105     14      1.9      1     1.38 #> 20 1105     15      2.12     1     1.38 set.seed(123)  deviant_tolerance_pp |>   group_split(male) |>   map(\\(.group) filter(.group, id %in% sample(unique(id), size = 2))) |>   list_rbind() #> # A tibble: 20 × 5 #>    id      age tolerance  male exposure #>    <fct> <dbl>     <dbl> <dbl>    <dbl> #>  1 442      11      1.45     0     1.13 #>  2 442      12      1.99     0     1.13 #>  3 442      13      1.45     0     1.13 #>  4 442      14      1.67     0     1.13 #>  5 442      15      1.9      0     1.13 #>  6 918      11      1        0     1.21 #>  7 918      12      1        0     1.21 #>  8 918      13      1.22     0     1.21 #>  9 918      14      1.99     0     1.21 #> 10 918      15      1.22     0     1.21 #> 11 268      11      1.45     1     0.9  #> 12 268      12      1.34     1     0.9  #> 13 268      13      1.99     1     0.9  #> 14 268      14      1.79     1     0.9  #> 15 268      15      1.34     1     0.9  #> 16 514      11      1.34     1     0.9  #> 17 514      12      1.67     1     0.9  #> 18 514      13      2.23     1     0.9  #> 19 514      14      2.12     1     0.9  #> 20 514      15      2.44     1     0.9"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-2.html","id":"using-a-trajectory-to-summarize-each-persons-empirical-growth-record","dir":"Articles","previous_headings":"2.2 Descriptive analysis of individual change over time","what":"Using a Trajectory to Summarize Each Person’s Empirical Growth Record","title":"Chapter 2: Exploring longitudinal data on change","text":"person’s empirical growth record can summarized applying two standardized approaches: nonparametric approach uses nonparametric smooths summarize person’s pattern change time graphically without imposing specific functional form. primary advantage nonparametric approach requires assumptions. parametric approach uses separate parametric models fit person’s data summarize pattern change time. model uses common functional form trajectories (e.g., straight line, quadratic curve, etc.). primary advantage parametric approach provides numeric summaries trajectories can used exploration. Singer Willett (2003) recommend using approaches—beginning nonparametric approach—examining smoothed trajectories help select common functional form trajectories parametric approach.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-2.html","id":"the-nonparametric-approach","dir":"Articles","previous_headings":"2.2 Descriptive analysis of individual change over time > Using a Trajectory to Summarize Each Person’s Empirical Growth Record","what":"The Nonparametric Approach","title":"Chapter 2: Exploring longitudinal data on change","text":"stat_smooth() function can used add nonparametric smooth layer empirical growth record plot. choice particular smoothing algorithm primarily matter convenience, ’ll use default loess smoother. span argument controls amount smoothing default loess smoother—smaller numbers producing wigglier lines larger numbers producing smoother lines; choose value creates similar smooth textbook figure.  Singer Willet (2003) recommend focusing elevation, shape, tilt smoothed trajectories answering questions like: scores hover low, medium, high end scale? everyone change time people remain ? trajectories inflection point plateau? rate change steep shallow? overall functional form trajectory group-level? linear curvilinear? smooth step-like? Answering last question particularly important, help select common functional form trajectories parametric approach.","code":"# Figure 2.3, page 27: deviant_tolerance_empgrowth +   stat_smooth(method = \"loess\", se = FALSE, span = .9)"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-2.html","id":"the-parametric-approach","dir":"Articles","previous_headings":"2.2 Descriptive analysis of individual change over time > Using a Trajectory to Summarize Each Person’s Empirical Growth Record","what":"The Parametric Approach","title":"Chapter 2: Exploring longitudinal data on change","text":"parametric approach, Singer Willet (2003) suggest using following three-step process: Estimate within-person linear model person data set. Collect summary statistics within-person linear model single data set. Add person’s fitted trajectory empirical growth record plot. begin, ’ll use lmList() function lme4 package fit common linear model adolescent data set. model formula lmList() function takes form response ~ terms | group. select straight line common functional form trajectories, age centred age 11. Next ’ll collect summary statistics within-person linear model single data set using tidy() function broom package. However, lmList() returns list models, need apply tidy() call model prior collecting summary statistics single data set. Ironically, also need tidy result tidy() prepare data plotting. Finally, can add person’s fitted trajectory empirical growth record plot using geom_abline() function. However, centred age linear model, need transform scale x-axis empirical growth plot centred well—otherwise ggplot2 able align fitted trajectories correctly. , must create custom transformation object using new_transform() function scales package, defines transformation, inverse, methods generating breaks labels.  Alternatively, plan examine parametric trajectories graphically, three-step process suggested Singer Willet (2003) can skipped altogether using stat_smooth() function \"lm\" method. approach also fits within-person linear model person data set; drawback makes awkward (though impossible) access summary statistics model.","code":"deviant_tolerance_fit <- lmList(   tolerance ~ I(age - 11) | id,   pool = FALSE,   data = deviant_tolerance_pp )  # Table 2.2, page 30: summary(deviant_tolerance_fit) #> Call: #>   Model: tolerance ~ I(age - 11) | NULL  #>    Data: deviant_tolerance_pp  #>  #> Coefficients: #>    (Intercept)  #>      Estimate Std. Error   t value     Pr(>|t|) #> 9       1.902 0.25194841  7.549165 4.819462e-03 #> 45      1.144 0.13335666  8.578499 3.329579e-03 #> 268     1.536 0.26038049  5.899059 9.725771e-03 #> 314     1.306 0.15265648  8.555156 3.356044e-03 #> 442     1.576 0.20786534  7.581832 4.759898e-03 #> 514     1.430 0.13794927 10.366130 1.915399e-03 #> 569     1.816 0.02572936 70.580844 6.267530e-06 #> 624     1.120 0.04000000 28.000000 1.000014e-04 #> 723     1.268 0.08442748 15.018806 6.407318e-04 #> 918     1.000 0.30444376  3.284679 4.626268e-02 #> 949     1.728 0.24118043  7.164760 5.600382e-03 #> 978     1.028 0.31995000  3.213002 4.884420e-02 #> 1105    1.538 0.15115555 10.174949 2.022903e-03 #> 1542    1.194 0.18032748  6.621287 7.015905e-03 #> 1552    1.184 0.37355321  3.169562 5.049772e-02 #> 1653    0.954 0.13925516  6.850734 6.366647e-03 #>    I(age - 11)  #>      Estimate Std. Error    t value   Pr(>|t|) #> 9       0.119 0.10285751  1.1569404 0.33105320 #> 45      0.174 0.05444263  3.1960249 0.04948216 #> 268     0.023 0.10629989  0.2163690 0.84257784 #> 314    -0.030 0.06232175 -0.4813729 0.66318168 #> 442     0.058 0.08486067  0.6834733 0.54336337 #> 514     0.265 0.05631755  4.7054602 0.01816360 #> 569     0.049 0.01050397  4.6649040 0.01859462 #> 624     0.020 0.01632993  1.2247449 0.30806801 #> 723    -0.054 0.03446738 -1.5666989 0.21516994 #> 918     0.143 0.12428864  1.1505476 0.33330784 #> 949    -0.098 0.09846150 -0.9953129 0.39294486 #> 978     0.632 0.13061904  4.8384983 0.01683776 #> 1105    0.156 0.06170899  2.5279945 0.08557441 #> 1542    0.237 0.07361839  3.2193045 0.04861002 #> 1552    0.153 0.15250246  1.0032625 0.38965538 #> 1653    0.246 0.05685068  4.3271249 0.02275586 deviant_tolerance_tidy <- deviant_tolerance_fit |>   map(tidy) |>   list_rbind(names_to = \"id\") |>   mutate(     id = as.factor(id),     term = case_when(       term == \"(Intercept)\" ~ \"intercept\",       term == \"I(age - 11)\" ~ \"slope\"     )   )  deviant_tolerance_abline <- deviant_tolerance_tidy |>   select(id:estimate) |>   pivot_wider(names_from = term, values_from = estimate)  deviant_tolerance_abline #> # A tibble: 16 × 3 #>    id    intercept   slope #>    <fct>     <dbl>   <dbl> #>  1 9         1.90   0.119  #>  2 45        1.14   0.174  #>  3 268       1.54   0.0230 #>  4 314       1.31  -0.0300 #>  5 442       1.58   0.0580 #>  6 514       1.43   0.265  #>  7 569       1.82   0.0490 #>  8 624       1.12   0.0200 #>  9 723       1.27  -0.0540 #> 10 918       1      0.143  #> 11 949       1.73  -0.0980 #> 12 978       1.03   0.632  #> 13 1105      1.54   0.156  #> 14 1542      1.19   0.237  #> 15 1552      1.18   0.153  #> 16 1653      0.954  0.246 transform_centre <- function(subtract) {   new_transform(     \"centre\",     transform = \\(x) x - subtract,     inverse = \\(x) x + subtract   ) }  # Figure 2.5, page 32: deviant_tolerance_empgrowth +   geom_abline(     aes(intercept = intercept, slope = slope),     data = deviant_tolerance_abline   ) +   scale_x_continuous(transform = transform_centre(11)) deviant_tolerance_empgrowth +   stat_smooth(method = \"lm\", se = FALSE)"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-2.html","id":"exploring-differences-in-change-across-people","dir":"Articles","previous_headings":"","what":"2.3 Exploring differences in change across people","title":"Chapter 2: Exploring longitudinal data on change","text":"explored individual changes time, Section 2.3 Singer Willet (2003) continue deviant_tolerance_pp data set demonstrate three strategies exploring interindividual differences change: Plotting entire set individual trajectories together, along average change trajectory entire group. Individual trajectories can either compared one another examine similarities differences changes across people, average change trajectory compare individual change group change. Conducting descriptive analyses key model parameters, estimated intercepts slopes individual change trajectory models. Exploring relationship change time-invariant predictors. relationship can explored plots statistical modelling.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-2.html","id":"plotting-the-entire-set-of-trajectories-together","dir":"Articles","previous_headings":"2.3 Exploring differences in change across people","what":"Plotting the entire set of trajectories together","title":"Chapter 2: Exploring longitudinal data on change","text":"purpose first strategy answer generic questions change, : direction rate change similar different across people? individual change compare group-averaged change trajectory? strategy, Singer Willet (2003) suggest using nonparametric parametric approaches, certain patterns data may somewhat easier interpret using one approach .","code":"deviant_tolerance_grptraj <- map(   list(\"loess\", \"lm\"),   \\(.method) {     deviant_tolerance_pp |>       mutate(method = .method) |>       ggplot(mapping = aes(x = age, y = tolerance)) +         stat_smooth(           aes(linewidth = \"individual\", group = id),           method = .method, se = FALSE, span = .9         ) +         stat_smooth(           aes(linewidth = \"average\"),           method = .method, se = FALSE, span = .9         ) +         scale_linewidth_manual(values = c(2, .25)) +         coord_cartesian(ylim = c(0, 4)) +         facet_wrap(vars(method), labeller = label_both) +         labs(linewidth = \"trajectory\")   } )  # Figure 2.6, page 34: wrap_plots(deviant_tolerance_grptraj) +   plot_layout(guides = \"collect\", axes = \"collect\")"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-2.html","id":"conducting-descriptive-analyses-of-key-model-parameters","dir":"Articles","previous_headings":"2.3 Exploring differences in change across people","what":"Conducting descriptive analyses of key model parameters","title":"Chapter 2: Exploring longitudinal data on change","text":"purpose second strategy answer specific questions behaviour key parameters individual change trajectory models, : average initial status average annual rate change? observed variability initial status annual rate change? relationship initial status annual rate change? strategy, Singer Willet (2003) suggest examining estimated intercepts slopes fitted linear models following three summary statistics: sample mean, summarizes average initial status (intercept) annual rate change (slope) across sample. sample variance standard deviation, summarize amount observed interindividual heterogeneity initial status annual rate change. sample correlation, summarizes strength direction relationship initial status annual rate change. sample mean, variance, standard deviation can computed together tidied model fits saved earlier using combination group_by() summarise() functions dplyr package. sample correlation needs computed separate step, requires additional transformations tidied model fits . use correlate() function corrr package—part tidymodels universe packages—API designed data pipelines mind.","code":"# Table 2.3, page 37: deviant_tolerance_tidy |>   group_by(term) |>   summarise(     mean = mean(estimate),     var = var(estimate),     sd = sd(estimate)   ) #> # A tibble: 2 × 4 #>   term       mean    var    sd #>   <chr>     <dbl>  <dbl> <dbl> #> 1 intercept 1.36  0.0887 0.298 #> 2 slope     0.131 0.0297 0.172 deviant_tolerance_tidy |>   select(id, term, estimate) |>   pivot_wider(names_from = term, values_from = estimate) |>   select(-id) |>   correlate() |>   stretch(na.rm = TRUE, remove.dups = TRUE) #> # A tibble: 1 × 3 #>   x         y          r #>   <chr>     <chr>  <dbl> #> 1 intercept slope -0.448"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-2.html","id":"exploring-the-relationship-between-change-and-time-invariant-predictors","dir":"Articles","previous_headings":"2.3 Exploring differences in change across people","what":"Exploring the relationship between change and time-invariant predictors","title":"Chapter 2: Exploring longitudinal data on change","text":"purpose final strategy answer questions systematic interindividual differences change, : observed (average) initial status (average) annual rate change differ across levels values time-invariant predictors? relationship initial status annual rate change time-invariant predictors? strategy, Singer Willet (2003) suggest using two approaches: Plotting (smoothed) individual growth trajectories, displayed separately groups distinguished important values time-invariant predictors. categorical predictors, level predictor can used. continuous predictors, values can temporarily categorized purpose display. Conducting exploratory analyses relationship change time-invariant predictors, investigating whether estimated intercepts slopes individual change trajectory models vary systematically different predictors.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-2.html","id":"the-plotting-approach","dir":"Articles","previous_headings":"2.3 Exploring differences in change across people > Exploring the relationship between change and time-invariant predictors","what":"The plotting approach","title":"Chapter 2: Exploring longitudinal data on change","text":"plotting approach, can adapt code used earlier plot entire set trajectories together, simply changing variable ’ll facet . facet categorical predictor male continuous predictor exposure, split median purposes display.   examining plots like , Singer Willet (2003) recommend recommend looking systematic patterns trajectories answer questions like: observed trajectories differ across groups? observed differences appear intercepts slopes? observed trajectories groups heterogeneous others? also wished conduct descriptive analyses key model parameters groups use update() function update refit common linear model different subsets data. store model fits subgroup list ’re easier iterate upon together. example, descriptive analysis intercepts slopes males females.","code":"deviant_tolerance_grptraj_by <- map(   list(male = \"male\", exposure = \"exposure\"),   \\(.by) {     deviant_tolerance_pp |>       mutate(         exposure = if_else(exposure < median(exposure), \"low\", \"high\"),         exposure = factor(exposure, levels = c(\"low\", \"high\"))       ) |>       ggplot(aes(x = age, y = tolerance)) +         stat_smooth(           aes(linewidth = \"individual\", group = id),           method = \"lm\", se = FALSE, span = .9         ) +         stat_smooth(           aes(linewidth = \"average\"),           method = \"lm\", se = FALSE, span = .9         ) +         scale_linewidth_manual(values = c(2, .25)) +         coord_cartesian(ylim = c(0, 4)) +         facet_wrap(.by, labeller = label_both) +         labs(linewidth = \"trajectory\")   } )  # Figure 2.7, page 38: wrap_plots(deviant_tolerance_grptraj_by, ncol = 1, guides = \"collect\") tolerance_fit_sex <- list(   male = update(deviant_tolerance_fit, subset = male == 1),   female = update(deviant_tolerance_fit, subset = male == 0) )  tolerance_fit_exposure <- list(   low = update(deviant_tolerance_fit, subset = exposure < 1.145),   high = update(deviant_tolerance_fit, subset = exposure >= 1.145) ) tolerance_fit_sex |>   map(     \\(.fit_sex) {       .fit_sex |>         map(tidy) |>         list_rbind(names_to = \"id\") |>         group_by(term) |>           summarise(             mean = mean(estimate),             sd = sd(estimate)           )     }   ) |>     list_rbind(names_to = \"sex\") #> # A tibble: 4 × 4 #>   sex    term         mean    sd #>   <chr>  <chr>       <dbl> <dbl> #> 1 male   (Intercept) 1.36  0.264 #> 2 male   I(age - 11) 0.167 0.238 #> 3 female (Intercept) 1.36  0.338 #> 4 female I(age - 11) 0.102 0.106"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-2.html","id":"the-exploratory-analysis-approach","dir":"Articles","previous_headings":"2.3 Exploring differences in change across people > Exploring the relationship between change and time-invariant predictors","what":"The exploratory analysis approach","title":"Chapter 2: Exploring longitudinal data on change","text":"exploratory analysis approach, Singer Willett (2003) recommend restricting simplest approaches examine relationship change time-invariant predictors—bivariate scatter plots sample correlations. reasoning restriction twofold: statistical models presented chapter intended descriptive exploratory purposes ; estimates known biases make imperfect measures person’s true initial status true rate change. models soon replaced multilevel model change Chapter 3, better-suited modelling longitudinal data. plotting computations, first need add adolescent’s male exposure values deviant_tolerance_tidy data frame. easily done using left_join() function dplyr package, performs mutating join add columns one data frame another, matching observations based keys. join selection columns person-level deviant_tolerance_pl data set: Specifically, id column, exists data frames thus used joining; two time-invariant predictors male exposure. ’ll also create new sex variable, can use instead male plotting. Now can create bivariate scatter plots. Note use .data pronoun inside call aes()—.data pronoun special construct tidyverse allows us treat character vector variable names environment variables, work expected way arguments use non-standard evaluation. learn .data pronoun, see dplyr package’s Programming dplyr vignette.  Finally, can compute correlation intercepts slopes individual change trajectory models time-invariant predictors male exposure. use cor() function rather corrr::correlate() since just want return correlation value, correlation data frame.","code":"deviant_tolerance_tidy_2 <- deviant_tolerance_tidy |>   left_join(select(deviant_tolerance_pl, id, male, exposure)) |>   mutate(sex = if_else(male == 0, \"female\", \"male\"))  deviant_tolerance_tidy_2 #> # A tibble: 32 × 9 #>    id    term      estimate std.error statistic p.value  male exposure sex    #>    <fct> <chr>        <dbl>     <dbl>     <dbl>   <dbl> <dbl>    <dbl> <chr>  #>  1 9     intercept   1.90      0.252      7.55  0.00482     0     1.54 female #>  2 9     slope       0.119     0.103      1.16  0.331       0     1.54 female #>  3 45    intercept   1.14      0.133      8.58  0.00333     1     1.16 male   #>  4 45    slope       0.174     0.0544     3.20  0.0495      1     1.16 male   #>  5 268   intercept   1.54      0.260      5.90  0.00973     1     0.9  male   #>  6 268   slope       0.0230    0.106      0.216 0.843       1     0.9  male   #>  7 314   intercept   1.31      0.153      8.56  0.00336     0     0.81 female #>  8 314   slope      -0.0300    0.0623    -0.481 0.663       0     0.81 female #>  9 442   intercept   1.58      0.208      7.58  0.00476     0     1.13 female #> 10 442   slope       0.0580    0.0849     0.683 0.543       0     1.13 female #> # ℹ 22 more rows deviant_tolerance_biplot <- map(   list(sex = \"sex\", exposure = \"exposure\"),   \\(.x) {     ggplot(deviant_tolerance_tidy_2, aes(x = .data[[.x]], y = estimate)) +       geom_point() +       facet_wrap(vars(term), ncol = 1, scales = \"free_y\")   } )  # Figure 2.8, page 40: wrap_plots(deviant_tolerance_biplot) +   plot_layout(axes = \"collect\") # Correlation values shown in Figure 2.8, page 40: deviant_tolerance_tidy_2 |>   group_by(term) |>   summarise(     male_cor = cor(estimate, male),     exposure_cor = cor(estimate, exposure)   ) #> # A tibble: 2 × 3 #>   term      male_cor exposure_cor #>   <chr>        <dbl>        <dbl> #> 1 intercept  0.00863        0.232 #> 2 slope      0.194          0.442"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-3.html","id":"what-is-the-purpose-of-the-multilevel-model-for-change","dir":"Articles","previous_headings":"","what":"3.1 What Is the Purpose of the Multilevel Model for Change?","title":"Chapter 3: Introducing the multilevel model for change","text":"Chapter 3 Singer Willet (2003) develop explain multilevel model change using subset data Burchinal, Campbell, Bryant, Wasik, Ramey (1997), measured effect early educational intervention cognitive performance sample African-American children ages 12, 18, 24 months (.e., 1.0, 1.5, 2.0 years). example use early_intervention data set, person-period data frame 309 rows 4 columns: id: Child ID. age: Age years time measurement. treatment: Treatment condition (control = 0, intervention = 1). cognitive_score: Cognitive performance score one two standardized intelligence tests: Bayley Scales Infant Development (Bayley, 1969) 12 18 months, Stanford Binet (Terman & Merrill, 1972) 24 months. Note reasons participant privacy early_intervention data set uses simulated data rather real data used Singer Willet (2003), examples presented article similar identical results presented text. motivate need multilevel model change, begin basic exploration description early_intervention data. Starting age variable, can see child’s cognitive performance measured three occasions ages 1.0, 1.5, 2.0 years, thus early_intervention data uses time-structured design. Next ’ll look time-invariant treatment variable. ’re summarizing time-invariant predictor, ’ll transform data person-level format pivot_wider() function tidyr package summarizing. total 58 children (56.3%) assigned participate early educational intervention, remaining 45 children (43.7%) participating control group. Singer Willet (2003) discuss, kind statistical model needed represent change processes longitudinal data like must include components two levels: level-1 submodel describes individuals change time, can address questions within-person change. level-2 submodel describes changes vary across individuals, can address questions -person differences change. Together, two components form multilevel model (also known linear mixed-effects model mixed model) change.","code":"# Table 3.1, page 48: early_intervention #> # A tibble: 309 × 4 #>    id      age treatment cognitive_score #>    <fct> <dbl>     <dbl>           <dbl> #>  1 1       1           1           106.  #>  2 1       1.5         1            91.7 #>  3 1       2           1            74.2 #>  4 2       1           1           112.  #>  5 2       1.5         1           114.  #>  6 2       2           1           119.  #>  7 3       1           0            90.4 #>  8 3       1.5         0            94.7 #>  9 3       2           0            80.4 #> 10 4       1           1           103.  #> # ℹ 299 more rows measurement_occasions <- unique(early_intervention$age) measurement_occasions #> [1] 1.0 1.5 2.0  early_intervention |>   group_by(id) |>   summarise(all_occasions = identical(age, measurement_occasions)) |>   pull(all_occasions) |>   unique() #> [1] TRUE early_intervention_pl <- pivot_wider(   early_intervention,   names_from = age,   names_prefix = \"age_\",   values_from = cognitive_score )  early_intervention_pl #> # A tibble: 103 × 5 #>    id    treatment age_1 age_1.5 age_2 #>    <fct>     <dbl> <dbl>   <dbl> <dbl> #>  1 1             1 106.     91.7  74.2 #>  2 2             1 112.    114.  119.  #>  3 3             0  90.4    94.7  80.4 #>  4 4             1 103.    101.   93.9 #>  5 5             1 103.     75.0  71.7 #>  6 6             0 106.     96.8  93.5 #>  7 7             1 136.    117.  119.  #>  8 8             0  79.8    69.3  67.5 #>  9 9             1 113.    105.  108.  #> 10 10            1  88.2    87.5  85.3 #> # ℹ 93 more rows early_intervention_pl |>   group_by(treatment) |>   summarise(count = n()) |>   mutate(proportion = count / sum(count)) #> # A tibble: 2 × 3 #>   treatment count proportion #>       <dbl> <int>      <dbl> #> 1         0    45      0.437 #> 2         1    58      0.563"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-3.html","id":"the-level-1-submodel-for-individual-change","dir":"Articles","previous_headings":"","what":"3.2 The Level-1 Submodel for Individual Change","title":"Chapter 3: Introducing the multilevel model for change","text":"Section 3.2 Singer Willet (2003) introduce level-1 component multilevel model change: submodel individual change—also known individual growth model—represents individual change outcome variable expect occur time period study. individual growth model specifies common functional form individual trajectories, Singer Willet (2003) suggest preceding level-1 submodel specification visual inspection empirical growth plots order select parsimonious functional form observed data reasonably come .  Singer Willet (2003) identify several questions helpful answer examining empirical growth plots aid model specification: type population individual growth model generated observed data? population individual growth model linear curvilinear time? Smooth jagged? Continuous disjoint? nonlinearities observed data consistent across individuals? Might due measurement error random error? many measurement occasions ? many? early_intervention data three measurement occasions individual, nonlinearities observed data likely due measurement error random error, Singer Willet (2003) specify simple linear model level-1 submodel: \\[ \\text{cognitive_score}_{ij} = \\pi_{0i} + \\pi_{1i} (\\text{age}_{ij} - 1) + \\epsilon_{ij}, \\] asserts \\(\\text{cognitive_score}_{ij}\\)—true value cognitive_score \\(\\)th child \\(j\\)th time—linear function age measurement occasion, \\(\\text{age}_{ij}\\), deviations linearity observed data time result random error, \\(\\epsilon_{ij}\\). \\(\\text{age}\\) centred \\((\\text{age} - 1)\\) model intercept, \\(\\pi_{0i}\\), represents \\(\\)th child’s true initial status—, true \\(\\text{cognitive_score}\\) value age 1. preferable using \\(\\text{age}\\) centring, intercept model instead represent \\(\\)th child’s true value cognitive_score age 0, meaning: (1) uncentred \\(\\text{age}\\) model must predict beyond temporal limits early_intervention data; (2) assume individual trajectories extend back time birth linearly age. Finally, model slope, \\(\\pi_{1i}\\), represents true rate change \\(\\)th child’s true \\(\\text{cognitive_score}\\) time—case, true annual rate change.","code":"set.seed(567)  # Figure 3.1, page 50: early_intervention |>   filter(id %in% sample(unique(id), size = 8)) |>   ggplot(aes(x = age, y = cognitive_score)) +     geom_point() +     stat_smooth(method = \"lm\", se = FALSE) +     scale_x_continuous(breaks = c(1, 1.5, 2)) +     coord_cartesian(ylim = c(50, 150)) +     facet_wrap(vars(id), ncol = 4, labeller = label_both)"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-3.html","id":"relating-the-level-1-submodel-to-the-exploratory-methods-of-chapter-2","dir":"Articles","previous_headings":"3.2 The Level-1 Submodel for Individual Change","what":"Relating the Level-1 Submodel to the Exploratory Methods of Chapter 2","title":"Chapter 3: Introducing the multilevel model for change","text":"fitting model, find helpful introduce Gelman Hill’s (2006) concepts complete pooling, pooling, partial pooling relation exploratory linear models fit Chapter 2 summarize average individual patterns change time. complete pooling model, group indicators individuals included model, single average trajectory using information individuals fitted. definition, complete pooling model ignores variation individuals, making unsuitable analyzing individual change time. corresponds average trajectory group trajectory plot Section 2.3. pooling model, separate within-person models fit person’s data, resulting individual trajectory individual. However, individual’s fitted trajectory ignores information individuals, pooling models tend overfit data within individual—potentially overstating variation individuals, making interindividual differences initial status rate change look different actually . corresponds individual trajectories group trajectory plot Section 2.3. partial pooling model represents compromise two extremes complete pooling partial pooling, wherein fit single model—individual growth model—takes account information individual, one hand, average individuals, , determine individual’s fitted trajectory. , partial pooling approach regularizes individual’s fitted trajectory—pulling extreme initial statuses rates change towards overall average. can fit level-1 submodel individual change using lmer() function lme4 package. model formula lmer() function takes form response ~ fixed_effects + random_effects. random effects term takes form (varying_effects | group), left side vertical bar defining variable(s) allowed vary across groups, right side grouping variable. Note , order match maximum likelihood method used Singer Willett (2003) chapter, also set REML argument FALSE model fit using full maximum likelihood (FML) estimation rather restricted maximum likelihood (REML). can visualize differences complete pooling, pooling, partial pooling adding fitted trajectory model empirical growth plots random sample children early_intervention data. , ’ll first use augment() function broom broom.mixed packages get predicted values child’s fitted trajectory complete pooling, pooling, partial pooling models. Note augment() generic function, whose methods linear multilevel models available broom broom.mixed packages, respectively. Next, ’ll tidy sample predicted values data. Finally, can plot empirical growth plots randomly sampled children, along fitted trajectory complete pooling, pooling, partial pooling models.  Examining plots, differences complete pooling, pooling, partial pooling become apparent: complete pooling model estimates single average trajectory, ignoring variation individuals, child predicted exact trajectory. pooling model estimates unique trajectory individual closely follows observed data. partial pooling model estimates unique trajectory individual, sometimes closely follows observed data, times lies somewhere -complete pooling pooling trajectories. Conceptually, partial pooling model similar pooling model: approaches model individual change outcome variable expect occur time period study, allowing individuals vary initial status rate change. However, partial pooling model takes account relative amount information individual average individuals, trajectories pulled towards overall average—stronger pull extreme individual’s observed data. case early_intervention data, children’s cognitive performance declined time, apparent average trajectory complete pooling model, entire set trajectories pooling model together. Although rate decline varied across children, showed improvement.  Given , cases show improvement given less weight partial pooling model, estimates pulled strongly towards group average. full extent effect can seen plotting entire set fitted trajectories partial pooling model together, along model’s population average trajectory.","code":"early_intervention_fit_cp <- lm(   cognitive_score ~ I(age - 1), data = early_intervention ) early_intervention_fit_np <- lmList(   cognitive_score ~ I(age - 1) | id,   pool = FALSE,   data = early_intervention ) early_intervention_fit_1 <- lmer(   cognitive_score ~ I(age - 1) + (1 + I(age - 1) | id),   data = early_intervention,   REML = FALSE )  summary(early_intervention_fit_1) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: cognitive_score ~ I(age - 1) + (1 + I(age - 1) | id) #>    Data: early_intervention #>  #>      AIC      BIC   logLik deviance df.resid  #>   2412.7   2435.1  -1200.4   2400.7      303  #>  #> Scaled residuals:  #>      Min       1Q   Median       3Q      Max  #> -2.08234 -0.50220  0.04103  0.53133  2.40849  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr  #>  id       (Intercept) 137.02   11.706         #>           I(age - 1)   53.60    7.321   -0.46 #>  Residual              69.24    8.321         #> Number of obs: 309, groups:  id, 103 #>  #> Fixed effects: #>             Estimate Std. Error t value #> (Intercept)  109.881      1.375   79.92 #> I(age - 1)   -16.913      1.366  -12.38 #>  #> Correlation of Fixed Effects: #>            (Intr) #> I(age - 1) -0.561 # Because the complete pooling model does not have a group indicator for # individuals, we need to manually add the IDs to the predicted values. early_intervention_pred_cp <- early_intervention_fit_cp |>   augment() |>   mutate(model = \"complete_pooling\", id = early_intervention$id) early_intervention_pred_cp #> # A tibble: 309 × 10 #>    cognitive_score `I(age - 1)` .fitted .resid    .hat .sigma .cooksd .std.resid #>              <dbl>     <I<dbl>>   <dbl>  <dbl>   <dbl>  <dbl>   <dbl>      <dbl> #>  1           106.           0     110.   -4.04 0.00809   13.8 3.52e-4     -0.294 #>  2            91.7          0.5   101.   -9.74 0.00324   13.8 8.11e-4     -0.707 #>  3            74.2          1      93.0 -18.7  0.00809   13.8 7.58e-3     -1.36  #>  4           112.           0     110.    2.33 0.00809   13.8 1.17e-4      0.169 #>  5           114.           0.5   101.   12.5  0.00324   13.8 1.33e-3      0.905 #>  6           119.           1      93.0  26.3  0.00809   13.7 1.49e-2      1.91  #>  7            90.4          0     110.  -19.5  0.00809   13.8 8.19e-3     -1.42  #>  8            94.7          0.5   101.   -6.69 0.00324   13.8 3.82e-4     -0.485 #>  9            80.4          1      93.0 -12.6  0.00809   13.8 3.42e-3     -0.916 #> 10           103.           0     110.   -6.77 0.00809   13.8 9.88e-4     -0.492 #> # ℹ 299 more rows #> # ℹ 2 more variables: model <chr>, id <fct>  # Because the no pooling models are separate linear models stored in a list, we # need to apply the augment() call to each model, then bind the predicted values # from each model into a single data set. Here the individual ID for each model # is stored in the name of list entry, which we add to the data frame using the # `names_to` argument of list_rbind(). early_intervention_pred_np <- early_intervention_fit_np |>   map(augment) |>   list_rbind(names_to = \"id\") |>   mutate(model = \"no_pooling\") early_intervention_pred_np #> # A tibble: 309 × 10 #>    id    cognitive_score `I(age - 1)` .fitted .resid  .hat .sigma .cooksd #>    <chr>           <dbl>     <I<dbl>>   <dbl>  <dbl> <dbl>  <dbl>   <dbl> #>  1 1               106.           0     106.  -0.551 0.833    NaN    2.50 #>  2 1                91.7          0.5    90.6  1.10  0.333    NaN    0.25 #>  3 1                74.2          1      74.8 -0.551 0.833    Inf    2.50 #>  4 2               112.           0     112.   0.610 0.833    NaN    2.50 #>  5 2               114.           0.5   115.  -1.22  0.333    NaN    0.25 #>  6 2               119.           1     119.   0.610 0.833    Inf    2.50 #>  7 3                90.4          0      93.5 -3.12  0.833    NaN    2.50 #>  8 3                94.7          0.5    88.5  6.23  0.333    NaN    0.25 #>  9 3                80.4          1      83.5 -3.12  0.833    Inf    2.50 #> 10 4               103.           0     104.  -0.686 0.833    NaN    2.50 #> # ℹ 299 more rows #> # ℹ 2 more variables: .std.resid <dbl>, model <chr>  # Nothing special needs to be done for the partial pooling model, aside from # having the broom.mixed package loaded. early_intervention_pred_pp <- early_intervention_fit_1 |>   augment() |>   mutate(model = \"partial_pooling\") early_intervention_pred_pp #> # A tibble: 309 × 15 #>    cognitive_score `I(age - 1)` id    .fitted .resid  .hat .cooksd .fixed   .mu #>              <dbl>     <I<dbl>> <fct>   <dbl>  <dbl> <dbl>   <dbl>  <dbl> <dbl> #>  1           106.           0   1       103.   3.11  0.439 0.0974   110.  103.  #>  2            91.7          0.5 1        92.6 -0.939 0.276 0.00336  101.   92.6 #>  3            74.2          1   1        82.5 -8.29  0.395 0.535     93.0  82.5 #>  4           112.           0   2       118.  -5.90  0.439 0.352    110.  118.  #>  5           114.           0.5 2       112.   1.42  0.276 0.00765  101.  112.  #>  6           119.           1   2       107.  12.4   0.395 1.20      93.0 107.  #>  7            90.4          0   3        97.7 -7.34  0.439 0.543    110.   97.7 #>  8            94.7          0.5 3        90.7  4.07  0.276 0.0632   101.   90.7 #>  9            80.4          1   3        83.6 -3.21  0.395 0.0803    93.0  83.6 #> 10           103.           0   4       107.  -3.71  0.439 0.139    110.  107.  #> # ℹ 299 more rows #> # ℹ 6 more variables: .offset <dbl>, .sqrtXwt <dbl>, .sqrtrwt <dbl>, #> #   .weights <dbl>, .wtres <dbl>, model <chr>  # Finally, we can bind the predicted values from the models into a single data # frame. early_intervention_preds <- bind_rows(   early_intervention_pred_cp,   early_intervention_pred_np,   early_intervention_pred_pp ) set.seed(333) early_intervention_preds_tidy <- early_intervention_preds |>   select(model, id, cognitive_score, age = `I(age - 1)`, .fitted) |>   mutate(     id = factor(id, levels = unique(id)),     age = as.numeric(age + 1)   ) |>   filter(id %in% sample(unique(id), size = 8))  early_intervention_preds_tidy #> # A tibble: 72 × 5 #>    model            id    cognitive_score   age .fitted #>    <chr>            <fct>           <dbl> <dbl>   <dbl> #>  1 complete_pooling 2               112.    1     110.  #>  2 complete_pooling 2               114.    1.5   101.  #>  3 complete_pooling 2               119.    2      93.0 #>  4 complete_pooling 6               106.    1     110.  #>  5 complete_pooling 6                96.8   1.5   101.  #>  6 complete_pooling 6                93.5   2      93.0 #>  7 complete_pooling 14               88.7   1     110.  #>  8 complete_pooling 14               97.6   1.5   101.  #>  9 complete_pooling 14               81.3   2      93.0 #> 10 complete_pooling 39              111.    1     110.  #> # ℹ 62 more rows ggplot(early_intervention_preds_tidy, aes(x = age, group = id)) +   geom_point(aes(y = cognitive_score)) +   geom_line(aes(y = .fitted, colour = model, group = model), linewidth = .75) +   scale_x_continuous(breaks = c(1, 1.5, 2)) +   scale_colour_brewer(palette = \"Dark2\") +   coord_cartesian(ylim = c(50, 150)) +   facet_wrap(vars(id), nrow = 2, labeller = label_both) # Figure 3.3, page 57: early_intervention |>   ggplot(mapping = aes(x = age, y = cognitive_score)) +     stat_smooth(       aes(linewidth = \"no_pooling\", group = id),       method = \"lm\", se = FALSE, span = .9     ) +     stat_smooth(       aes(linewidth = \"complete_pooling\"),       method = \"lm\", se = FALSE, span = .9     ) +     scale_linewidth_manual(values = c(2, .25)) +     scale_x_continuous(breaks = c(1, 1.5, 2)) +     coord_cartesian(ylim = c(50, 150)) +     labs(linewidth = \"model\") early_intervention_fit_1 |>   augment() |>   select(-cognitive_score) |>   rename(cognitive_score = .fitted, age = `I(age - 1)`) |>   mutate(age = as.numeric(age + 1)) |>   ggplot(aes(x = age, y = cognitive_score)) +     geom_line(aes(linewidth = \"individual\", group = id), colour = \"#3366FF\") +     # We'll use predict() rather than augment() to get the population-level     # predictions, due to some currently bad behaviour in augment() when making     # predictions on new data: https://github.com/bbolker/broom.mixed/issues/141     geom_line(       aes(linewidth = \"average\"),       data = tibble(         age = measurement_occasions,         cognitive_score = predict(           early_intervention_fit_1,           tibble(age = measurement_occasions),           re.form = NA         )       ),       colour = \"#3366FF\"     ) +     scale_linewidth_manual(values = c(2, .25)) +     scale_x_continuous(breaks = c(1, 1.5, 2)) +     coord_cartesian(ylim = c(50, 150)) +     labs(linewidth = \"trajectory\")"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-3.html","id":"the-level-2-submodel-for-systematic-interindividual-differences-in-change","dir":"Articles","previous_headings":"","what":"3.3 The Level-2 Submodel for Systematic Interindividual Differences in Change","title":"Chapter 3: Introducing the multilevel model for change","text":"Section 3.3 Singer Willet (2003) introduce level-2 component multilevel model change—submodel systematic interindividual differences change—defined four specific features: outcomes must individual growth parameters level-1 submodel, \\(\\pi_{0i}\\) \\(\\pi_{1i}\\). must one level-2 submodel level-1 individual growth parameter, must written separate parts. level-2 submodel must specify relationship individual growth parameter level-2 time-invariant predictors. level-2 submodel must allow individuals share common predictor values vary individual change trajectories. level-2 submodel must simultaneously account -group differences individual growth parameters within-group differences change, Singer Willet (2003) suggest preceding level-2 submodel specification visual inspection individual growth trajectories stratified levels time-invariant predictor(s), order identify kind population model give rise observed patterns.  early_intervention data, single time-invariant predictor, Singer Willet (2003) specify following level-2 submodel: \\[ \\begin{align} \\pi_{0i} &= \\gamma_{00} + \\gamma_{01} \\text{treatment}_i + \\zeta_{0i} \\\\ \\pi_{1i} &= \\gamma_{10} + \\gamma_{11} \\text{treatment}_i + \\zeta_{1i}, \\end{align} \\] asserts individual growth parameters level-1 submodel, \\(\\pi_{0i}\\) \\(\\pi_{1i}\\), treated level-2 outcomes linear function \\(\\)th child’s treatment status, \\(\\text{treatment}_i\\). parameters \\(\\gamma_{00}\\) \\(\\gamma_{10}\\) level-2 intercepts; parameters \\(\\gamma_{01}\\) \\(\\gamma_{11}\\) level-2 slopes. Collectively, four level-2 parameters known fixed effects. Finally, parameters \\(\\zeta_{0i}\\) \\(\\zeta_{1i}\\) level-2 residuals allow value individual’s growth parameters scattered around respective population averages. Collectively, two level-2 parameters known random effects, assume bivariate normally distributed mean 0, unknown variances, \\(\\sigma_0^2\\) \\(\\sigma_1^2\\), unknown covariance, \\(\\sigma_{01}\\): \\[ \\begin{align} \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} & \\sim \\operatorname{N} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},   \\begin{bmatrix} \\sigma_0^2 & \\sigma_{01}\\\\ \\sigma_{01} & \\sigma_1^2 \\end{bmatrix} \\end{pmatrix}. \\end{align} \\]","code":"# Figure 3.4, page 59: ggplot(early_intervention, mapping = aes(x = age, y = cognitive_score)) +   stat_smooth(     aes(linewidth = \"individual\", group = id),     method = \"lm\", se = FALSE, span = .9   ) +   stat_smooth(     aes(linewidth = \"average\"),     method = \"lm\", se = FALSE, span = .9   ) +   scale_linewidth_manual(values = c(2, .25)) +   scale_x_continuous(breaks = c(1, 1.5, 2)) +   coord_cartesian(ylim = c(50, 150)) +   facet_wrap(vars(treatment), labeller = label_both) +   labs(linewidth = \"trajectory\") # TODO: Decide whether or not to do the bottom plots."},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-3.html","id":"fitting-the-multilevel-model-for-change-to-data","dir":"Articles","previous_headings":"","what":"3.4 Fitting the Multilevel Model for Change to Data","title":"Chapter 3: Introducing the multilevel model for change","text":"Putting level-1 level-2 submodels together, multilevel model change early_intervention data looks like: \\[ \\begin{alignat}{3} & \\text{Level 1:} \\qquad &   \\text{cognitive_score}_{ij} &=   \\pi_{0i} + \\pi_{1i} (\\text{age}_{ij} - 1) + \\epsilon_{ij} \\\\ & \\text{Level 2:} \\qquad &   \\pi_{0i} &=   \\gamma_{00} + \\gamma_{01} \\text{treatment}_i + \\zeta_{0i} \\\\ &                        &   \\pi_{1i} &=   \\gamma_{10} + \\gamma_{11} \\text{treatment}_i + \\zeta_{1i}. \\end{alignat} \\] fitting model, find helpful substitute level-2 equations level-1, yielding single reduced equation. Although mixed model equation mathematically identical multilevel model equation , substituting equations helpful practice two reasons: makes parameters model easier identify, particularly case interactions level-1 level-2 predictors. format mixed-effects modelling packages R use specify model formula. clarifies statistical model actually fit data software. \\[ \\begin{align} \\text{cog}_{ij} &=   \\pi_{0i} + \\pi_{1i} (\\text{age}_{ij} - 1) + \\epsilon_{ij} \\\\ &=   \\gamma_{00} + \\gamma_{01} \\text{trt}_i   + \\pi_{1i} (\\text{age}_{ij} - 1) + \\epsilon_{ij} + \\zeta_{0i} \\\\ &=   \\gamma_{00} + \\gamma_{01} \\text{trt}_i   + (\\gamma_{10} + \\gamma_{11} \\text{trt}_i + \\zeta_{1i})(\\text{age}_{ij} - 1)   + \\epsilon_{ij} + \\zeta_{0i} \\\\ &=   \\underbrace{     \\gamma_{00} + \\gamma_{01} \\text{trt}_i     + \\gamma_{10}(\\text{age}_{ij} - 1)     + \\gamma_{11} \\text{trt}_i(\\text{age}_{ij} - 1)   }_{\\text{Fixed Effects}}   + \\underbrace{     \\epsilon_{ij} + \\zeta_{0i} + \\zeta_{1i}(\\text{age}_{ij} - 1).   }_{\\text{Random Effects}} \\end{align} \\] can now fit multilevel model change early_intervention data. Based equation , need update level-1 submodel include predictors treatment, interaction treatment age. Alternatively, start scratch, can specify final multilevel model change like .","code":"update(   early_intervention_fit_1,   . ~ . + treatment + treatment:I(age - 1) ) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: cognitive_score ~ I(age - 1) + (1 + I(age - 1) | id) + treatment +   #>     I(age - 1):treatment #>    Data: early_intervention #>       AIC       BIC    logLik  deviance  df.resid  #>  2402.540  2432.407 -1193.270  2386.540       301  #> Random effects: #>  Groups   Name        Std.Dev. Corr  #>  id       (Intercept) 11.564         #>           I(age - 1)   6.754   -0.57 #>  Residual              8.321         #> Number of obs: 309, groups:  id, 103 #> Fixed Effects: #>          (Intercept)            I(age - 1)             treatment   #>              107.822               -20.123                 3.657   #> I(age - 1):treatment   #>                5.702 early_intervention_fit <- lmer(   cognitive_score ~ I(age - 1) * treatment + (1 + I(age - 1) | id),   data = early_intervention,   REML = FALSE )  # Table 3.3, page 69: summary(early_intervention_fit) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: cognitive_score ~ I(age - 1) * treatment + (1 + I(age - 1) |      id) #>    Data: early_intervention #>  #>      AIC      BIC   logLik deviance df.resid  #>   2402.5   2432.4  -1193.3   2386.5      301  #>  #> Scaled residuals:  #>      Min       1Q   Median       3Q      Max  #> -2.04567 -0.48714  0.04639  0.53367  2.32828  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr  #>  id       (Intercept) 133.74   11.564         #>           I(age - 1)   45.61    6.754   -0.57 #>  Residual              69.24    8.321         #> Number of obs: 309, groups:  id, 103 #>  #> Fixed effects: #>                      Estimate Std. Error t value #> (Intercept)           107.822      2.063  52.276 #> I(age - 1)            -20.123      2.023  -9.949 #> treatment               3.657      2.749   1.330 #> I(age - 1):treatment    5.702      2.695   2.116 #>  #> Correlation of Fixed Effects: #>             (Intr) I(g-1) trtmnt #> I(age - 1)  -0.605               #> treatment   -0.750  0.454        #> I(g-1):trtm  0.454 -0.750 -0.605"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-3.html","id":"examining-estimated-fixed-effects","dir":"Articles","previous_headings":"","what":"3.5 Examining Estimated Fixed Effects","title":"Chapter 3: Introducing the multilevel model for change","text":"Section 3.5 Singer Willet (2003) explain two ways interpret fixed effects estimates multilevel model change: Interpreting fixed effects coefficients directly. Plotting fitted change trajectories prototypical individuals.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-3.html","id":"interpreting-fixed-effects-coefficients-directly","dir":"Articles","previous_headings":"3.5 Examining Estimated Fixed Effects","what":"Interpreting fixed effects coefficients directly","title":"Chapter 3: Introducing the multilevel model for change","text":"fixed effects estimates multilevel model change can interpreted directly way regression coefficient. Thus, multilevel model change early_intervention data, fixed effects estimates interpreted follows: \\(\\gamma_{00}\\): model intercept. parameter estimates population average true initial status children control group. \\(\\gamma_{01}\\): coefficient treatment. parameter estimates difference population average true initial status children treatment group children control group. \\(\\gamma_{10}\\): coefficient age. parameter estimates population average annual rate true change children control group. \\(\\gamma_{11}\\): coefficient interaction age treatment. parameter estimates difference population average annual rate true change children treatment group children control group. model, can view estimates summary() function. can also access estimates programmatically using either generic fixef() function return vector estimates, tidy() function broom.mixed package return tibble.","code":"summary(early_intervention_fit) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: cognitive_score ~ I(age - 1) * treatment + (1 + I(age - 1) |      id) #>    Data: early_intervention #>  #>      AIC      BIC   logLik deviance df.resid  #>   2402.5   2432.4  -1193.3   2386.5      301  #>  #> Scaled residuals:  #>      Min       1Q   Median       3Q      Max  #> -2.04567 -0.48714  0.04639  0.53367  2.32828  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr  #>  id       (Intercept) 133.74   11.564         #>           I(age - 1)   45.61    6.754   -0.57 #>  Residual              69.24    8.321         #> Number of obs: 309, groups:  id, 103 #>  #> Fixed effects: #>                      Estimate Std. Error t value #> (Intercept)           107.822      2.063  52.276 #> I(age - 1)            -20.123      2.023  -9.949 #> treatment               3.657      2.749   1.330 #> I(age - 1):treatment    5.702      2.695   2.116 #>  #> Correlation of Fixed Effects: #>             (Intr) I(g-1) trtmnt #> I(age - 1)  -0.605               #> treatment   -0.750  0.454        #> I(g-1):trtm  0.454 -0.750 -0.605 fixef(early_intervention_fit) #>          (Intercept)           I(age - 1)            treatment  #>           107.821683           -20.123396             3.656602  #> I(age - 1):treatment  #>             5.702077 tidy(early_intervention_fit, effects = \"fixed\") #> # A tibble: 4 × 5 #>   effect term                 estimate std.error statistic #>   <chr>  <chr>                   <dbl>     <dbl>     <dbl> #> 1 fixed  (Intercept)            108.        2.06     52.3  #> 2 fixed  I(age - 1)             -20.1       2.02     -9.95 #> 3 fixed  treatment                3.66      2.75      1.33 #> 4 fixed  I(age - 1):treatment     5.70      2.70      2.12"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-3.html","id":"plotting-fitted-change-trajectories-for-prototypical-individuals","dir":"Articles","previous_headings":"3.5 Examining Estimated Fixed Effects","what":"Plotting fitted change trajectories for prototypical individuals","title":"Chapter 3: Introducing the multilevel model for change","text":"Another way interpreting fixed effects plot fitted change trajectories prototypical individuals, using fixed effects estimates make predictions. can using following three-step process: Construct data set prototypical individuals. Predict fitted change trajectories prototypical individuals using fixed effects estimates. Plot fitted change trajectories. Depending complexity multilevel model change, number prototypical individuals wish examine, number ways construct data set prototypical individuals. simplest way construct data set hand using, example, tibble() tribble() functions tibble package. However, often case prototypical individuals simply represent unique combinations different predictor values, often convenient construct data set using expand_grid() crossing() functions tidyr package, expand data frame include possible combinations values. difference functions crossing() wrapper around expand_grid() de-duplicates sorts inputs. early_intervention multilevel model change, two prototypical individuals possible: child treatment group (treatment = 1) child control group (treatment = 0). make predictions using fixed effects estimates, set re.form argument predict() function NA. noted earlier code comment, use predict() rather augment() get predictions, due currently bad behaviour augment.merMod() making predictions new data models (although specific example augment() approach worked well). Finally, can plot fitted change trajectories usual.","code":"prototypical_children <- crossing(treatment = c(0, 1), age = c(1, 1.5, 2)) prototypical_children #> # A tibble: 6 × 2 #>   treatment   age #>       <dbl> <dbl> #> 1         0   1   #> 2         0   1.5 #> 3         0   2   #> 4         1   1   #> 5         1   1.5 #> 6         1   2 prototypical_children <- prototypical_children |>   mutate(     treatment = factor(treatment),     cognitive_score = predict(       early_intervention_fit, newdata = prototypical_children, re.form = NA     )   )  prototypical_children #> # A tibble: 6 × 3 #>   treatment   age cognitive_score #>   <fct>     <dbl>           <dbl> #> 1 0           1             108.  #> 2 0           1.5            97.8 #> 3 0           2              87.7 #> 4 1           1             111.  #> 5 1           1.5           104.  #> 6 1           2              97.1 # Figure 3.5, page 71: ggplot(prototypical_children, aes(x = age, y = cognitive_score)) +   geom_line(aes(linetype = treatment, group = treatment)) +   scale_x_continuous(breaks = c(1, 1.5, 2)) +   coord_cartesian(ylim = c(50, 150))"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-4.html","id":"example-changes-in-adolescent-alcohol-use","dir":"Articles","previous_headings":"","what":"4.1 Example: Changes in adolescent alcohol use","title":"Chapter 4: Doing data analysis with the multilevel model for change","text":"Chapter 4 Singer Willet (2003) delve deeper specification, estimation, interpretation multilevel model change using subset data Curran, Stice, Chassin (1997), measured relation changes alcohol use changes peer alcohol use 3-year period community-based sample Hispanic Caucasian adolescents. example use alcohol_use_1 data set, person-period data frame 246 rows 6 columns: id: Adolescent ID. age: Age years time measurement. child_of_alcoholic: Binary indicator whether adolescent child alcoholic parent. male: Binary indicator whether adolescent male. alcohol_use: Square root summed scores four eight-point items measuring frequency alcohol use. peer_alcohol_use: Square root summed scores two six-point items measuring frequency peer alcohol use. inform specification multilevel model change fit subsequent sections, begin basic exploration description alcohol_use_1 data. Starting age variable, can see adolescent measured three occasions ages 14, 15, 16 years. Next ’ll look time-invariant male child_of_alcoholic variables. ’re summarizing time-invariant predictors, ’ll transform data person-level format pivot_wider() function tidyr package summarizing. total 42 adolescents (51.2%) male 40 female (48.8%), total 37 adolescents (45.1%) children alcoholic parent 45 (54.9%). inform specification level-1 submodel, can look empirical growth plots random sample adolescents usual.  Finally, inform specification level-2 submodel, can look coincident growth trajectories—simply usual individual growth trajectories summarized number individuals trajectory—displayed separately groups distinguished important values time-invariant predictors. look two time-invariant predictors, child_of_alcoholic peer_alcohol_use. peer_alcohol_use continuous variable, split sample mean purpose display. plot coincident growth trajectories, first need summarize, predictor, number individuals trajectory. easiest way count number groups trajectory pattern level time-invariant predictors using person-level data, tidying coincident trajectory summary back person-period format. Afterwards can plot usual, addition linewidth aesthetic coincident trajectory counts. Note plot differs slightly text, unlike Singer Willet (2003) use entire sample instead random sample.  Based exploratory analyses, Singer Willett (2003) posited following multilevel model change alcohol_use_1 data: \\[ \\begin{alignat}{3} & \\text{Level 1:} \\qquad &   \\text{alcohol_use}_{ij} &=   \\pi_{0i} + \\pi_{1i} (\\text{age}_{ij} - 14) + \\epsilon_{ij} \\\\ & \\text{Level 2:} \\qquad &   \\pi_{0i} &=   \\gamma_{00} + \\gamma_{01} \\text{child_of_alcoholic}_i + \\zeta_{0i} \\\\ &                        &   \\pi_{1i} &=   \\gamma_{10} + \\gamma_{11} \\text{child_of_alcoholic}_i + \\zeta_{1i}, \\end{alignat} \\] model parameters follow definitions interpretations discussed Chapter 3.","code":"alcohol_use_1 #> # A tibble: 246 × 6 #>    id      age child_of_alcoholic  male alcohol_use peer_alcohol_use #>    <fct> <dbl>              <dbl> <dbl>       <dbl>            <dbl> #>  1 1        14                  1     0        1.73            1.26  #>  2 1        15                  1     0        2               1.26  #>  3 1        16                  1     0        2               1.26  #>  4 2        14                  1     1        0               0.894 #>  5 2        15                  1     1        0               0.894 #>  6 2        16                  1     1        1               0.894 #>  7 3        14                  1     1        1               0.894 #>  8 3        15                  1     1        2               0.894 #>  9 3        16                  1     1        3.32            0.894 #> 10 4        14                  1     1        0               1.79  #> # ℹ 236 more rows measurement_occasions <- unique(alcohol_use_1$age) measurement_occasions #> [1] 14 15 16  alcohol_use_1 |>   group_by(id) |>   summarise(all_occasions = identical(age, measurement_occasions)) |>   pull(all_occasions) |>   unique() #> [1] TRUE alcohol_use_1_pl <- pivot_wider(   alcohol_use_1,   names_from = age,   names_prefix = \"alcohol_use_\",   values_from = alcohol_use )  alcohol_use_1_pl #> # A tibble: 82 × 7 #>    id    child_of_alcoholic  male peer_alcohol_use alcohol_use_14 alcohol_use_15 #>    <fct>              <dbl> <dbl>            <dbl>          <dbl>          <dbl> #>  1 1                      1     0            1.26            1.73           2    #>  2 2                      1     1            0.894           0              0    #>  3 3                      1     1            0.894           1              2    #>  4 4                      1     1            1.79            0              2    #>  5 5                      1     0            0.894           0              0    #>  6 6                      1     1            1.55            3              3    #>  7 7                      1     0            1.55            1.73           2.45 #>  8 8                      1     1            0               0              0    #>  9 9                      1     1            0               0              1    #> 10 10                     1     0            2               1              1    #> # ℹ 72 more rows #> # ℹ 1 more variable: alcohol_use_16 <dbl> map(   list(male = \"male\", child_of_alcoholic = \"child_of_alcoholic\"),   \\(.x) {     alcohol_use_1_pl |>       group_by(.data[[.x]]) |>       summarise(count = n()) |>       mutate(proportion = count / sum(count))   } ) #> $male #> # A tibble: 2 × 3 #>    male count proportion #>   <dbl> <int>      <dbl> #> 1     0    40      0.488 #> 2     1    42      0.512 #>  #> $child_of_alcoholic #> # A tibble: 2 × 3 #>   child_of_alcoholic count proportion #>                <dbl> <int>      <dbl> #> 1                  0    45      0.549 #> 2                  1    37      0.451 # Figure 4.1, page 77: alcohol_use_1 |>   filter(id %in% c(4, 14, 23, 32, 41, 56, 65, 82)) |>   ggplot(aes(x = age, y = alcohol_use)) +     stat_smooth(method = \"lm\", se = FALSE) +     geom_point() +     coord_cartesian(xlim = c(13, 17), ylim = c(0, 4)) +     facet_wrap(vars(id), ncol = 4, labeller = label_both) alcohol_use_1_pl <- alcohol_use_1_pl |>   mutate(     peer_alcohol_use_split = if_else(       peer_alcohol_use < mean(peer_alcohol_use),       true = \"low\",       false = \"high\"     ),     peer_alcohol_use_split = factor(peer_alcohol_use_split, levels = c(\"low\", \"high\"))   )  alcohol_use_1_pl #> # A tibble: 82 × 8 #>    id    child_of_alcoholic  male peer_alcohol_use alcohol_use_14 alcohol_use_15 #>    <fct>              <dbl> <dbl>            <dbl>          <dbl>          <dbl> #>  1 1                      1     0            1.26            1.73           2    #>  2 2                      1     1            0.894           0              0    #>  3 3                      1     1            0.894           1              2    #>  4 4                      1     1            1.79            0              2    #>  5 5                      1     0            0.894           0              0    #>  6 6                      1     1            1.55            3              3    #>  7 7                      1     0            1.55            1.73           2.45 #>  8 8                      1     1            0               0              0    #>  9 9                      1     1            0               0              1    #> 10 10                     1     0            2               1              1    #> # ℹ 72 more rows #> # ℹ 2 more variables: alcohol_use_16 <dbl>, peer_alcohol_use_split <fct> alcohol_use_1_cotraj <- map(   list(\"child_of_alcoholic\", \"peer_alcohol_use_split\"),   \\(.x) {     # Wrangle     .coincident_trajectories <- alcohol_use_1_pl |>       group_by(.data[[.x]], pick(starts_with(\"alcohol_use\"))) |>       summarise(coincident_trajectories = n(), .groups = \"drop\") |>       mutate(trajectory_id = 1:n(), .before = everything()) |>       pivot_longer(         cols = starts_with(\"alcohol_use\"),         names_to = \"age\",         names_prefix = \"alcohol_use_\",         names_transform = as.integer,         values_to = \"alcohol_use\"       )          # Plot     ggplot(.coincident_trajectories, aes(x = age, y = alcohol_use)) +       stat_smooth(         aes(group = trajectory_id, linewidth = coincident_trajectories),         method = \"lm\",         se = FALSE       ) +       scale_linewidth_continuous(limits = c(1, 22), range = c(.25, 4)) +       coord_cartesian(xlim = c(13, 17), ylim = c(0, 4)) +       facet_wrap(vars(.data[[.x]]), labeller = label_both)   } )  # Figure 4.2, page 79: wrap_plots(alcohol_use_1_cotraj, ncol = 1, guides = \"collect\")"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-4.html","id":"the-composite-specification-of-the-multilevel-model-for-change","dir":"Articles","previous_headings":"","what":"4.2 The composite specification of the multilevel model for change","title":"Chapter 4: Doing data analysis with the multilevel model for change","text":"Section 4.2 Singer Willett (2003) introduce call composite multilevel model change, prematurely introduced Chapter 3 examples mixed model specification. substituting level-2 equations level-1, composite multilevel model change alcohol_use_1 data looks like: \\[ \\text{alcohol_use}_{ij} = \\underbrace{   \\gamma_{00} + \\gamma_{01} \\text{coa}_i   + \\gamma_{10}(\\text{age}_{ij} - 14)   + \\gamma_{11} \\text{coa}_i(\\text{age}_{ij} - 14) }_{\\text{Fixed Effects}} + \\underbrace{   \\epsilon_{ij} + \\zeta_{0i} + \\zeta_{1i}(\\text{age}_{ij} - 14). }_{\\text{Random Effects}} \\]","code":""},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-4.html","id":"methods-of-estimation-revisited","dir":"Articles","previous_headings":"","what":"4.3 Methods of Estimation, Revisited","title":"Chapter 4: Doing data analysis with the multilevel model for change","text":"Section 4.3 Singer Willett (2003) discuss two methods estimation available frequentist multilevel models, must selected fitting model: Generalized least squares (GLS), extension ordinary least-squares estimation allows residuals autocorrelated heteroscedastic. GLS estimates obtained minimizing weighted function residuals. gls() function nlme package can used fit multilevel model change using GLS. Maximum likelihood (ML), general approach limited linear regression models. ML estimates obtained maximizing likelihood function , assumed statistical model, observed data probable. previously demonstrated, lmer() function lme4 package can used fit multilevel model change using ML. generalized least squares maximum likelihood estimation use different procedures fit multilevel model change, estimates may differ fitting model using data; however, normal distribution assumptions required maximum likelihood estimation hold, estimates equivalent. Additionally, maximum likelihood estimation can distinguished two types: full restricted. Singer Willet (2003) explain, full maximum likelihood (FML) likelihood sample data maximized, goodness--fit statistics refer fit entire model (fixed random effects); restricted maximum likelihood (REML) likelihood sample residuals maximized, goodness--fit statistics refer fit random effects. Consequently, statistical tests comparing goodness--fit statistics FML models can used test hypotheses either fixed random effect parameters, whereas REML models can used test hypotheses random effect parameters.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-4.html","id":"first-steps-fitting-two-unconditional-multilevel-models-for-change","dir":"Articles","previous_headings":"","what":"4.4 First Steps: Fitting Two Unconditional Multilevel Models for Change","title":"Chapter 4: Doing data analysis with the multilevel model for change","text":"Section 4.4 Singer Willett (2003) introduce new model building workflow multilevel model change begins fitting two unconditional multilevel models change including substantive predictors: unconditional means model, partitions quantifies total variation outcome variable across individuals without regard time. fit model first determine whether variance components \\(\\epsilon_{ij}\\) \\(\\zeta_{0i}\\) sufficient variation within individuals (level 1) individuals (level 2), respectively, warrant linking outcome variation level predictors. unconditional growth model, partitions quantifies variation outcome variable across individuals time. fit model second determine whether interindividual differences change due outcome variation true initial status, \\(\\zeta_{0i}\\), true rate change, \\(\\zeta_{1i}\\). Together, two models (1) provide valuable baseline can evaluate compare subsequent models include substantive predictors, (2) help establish whether systematic variation outcome variable worth exploring resides.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-4.html","id":"the-unconditional-means-model","dir":"Articles","previous_headings":"4.4 First Steps: Fitting Two Unconditional Multilevel Models for Change","what":"The unconditional means model","title":"Chapter 4: Doing data analysis with the multilevel model for change","text":"unconditional means model intercept-model allows intercept vary across individuals: \\[ \\begin{alignat}{3} & \\text{Level 1:} \\qquad &   \\text{alcohol_use}_{ij} &=   \\pi_{0i} + \\epsilon_{ij} \\\\ & \\text{Level 2:} \\qquad &   \\pi_{0i} &=   \\gamma_{00} + \\zeta_{0i}, \\end{alignat} \\] postulates observed value alcohol_use \\(\\)th adolescent \\(j\\)th time composed within-person deviations, \\(\\epsilon_{ij}\\), person-specific true mean, \\(\\pi_{0i}\\), turn composed -person deviation, \\(\\zeta_{0i}\\), population average true mean, \\(\\gamma_{00}\\). Note unconditional means model lacks temporal predictors, stipulates true change trajectory individual completely flat time, sitting person-specific mean (\\(\\pi_{0i}\\)); true population change trajectory also flat, sitting grand mean (\\(\\gamma_{00}\\)).","code":"# Table 4.1, Model A, page 94-95: model_A <- lmer(   alcohol_use ~ 1 + (1 | id),   data = alcohol_use_1,   REML = FALSE )  summary(model_A) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: alcohol_use ~ 1 + (1 | id) #>    Data: alcohol_use_1 #>  #>      AIC      BIC   logLik deviance df.resid  #>    676.2    686.7   -335.1    670.2      243  #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -1.8865 -0.3076 -0.3067  0.6137  2.8567  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. #>  id       (Intercept) 0.5639   0.7509   #>  Residual             0.5617   0.7495   #> Number of obs: 246, groups:  id, 82 #>  #> Fixed effects: #>             Estimate Std. Error t value #> (Intercept)  0.92195    0.09571   9.633 model_A |>   augment(data = alcohol_use_1) |>   ggplot(aes(x = age, y = .fitted)) +     geom_line(aes(linewidth = \"individual\", group = id), alpha = .35) +     geom_line(       aes(linewidth = \"average\"),       data = tibble(.fitted = fixef(model_A), age = measurement_occasions)     ) +     scale_linewidth_manual(values = c(2, .25)) +     coord_cartesian(xlim = c(13, 17), ylim = c(0, 4)) +     labs(y = \"alcohol_use\", linewidth = \"trajectory\")"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-4.html","id":"the-unconditional-growth-model","dir":"Articles","previous_headings":"4.4 First Steps: Fitting Two Unconditional Multilevel Models for Change","what":"The unconditional growth model","title":"Chapter 4: Doing data analysis with the multilevel model for change","text":"unconditional growth model introduces time-indicator predictor model, allows rate change vary across individuals: \\[ \\begin{alignat}{3} & \\text{Level 1:} \\qquad &   \\text{alcohol_use}_{ij} &=   \\pi_{0i} + \\pi_{1i}(\\text{age}_{ij} - 14) + \\epsilon_{ij} \\\\ & \\text{Level 2:} \\qquad &   \\pi_{0i} &=   \\gamma_{00} + \\zeta_{0i} \\\\ &                        &   \\pi_{1i} &=   \\gamma_{10} + \\zeta_{1i}, \\end{alignat} \\] postulates observed value alcohol_use \\(\\)th adolescent \\(j\\)th time composed within-person deviations, \\(\\epsilon_{ij}\\), true linear change trajectory—linear function true initial status, \\(\\pi_{0i}\\), true rate change, \\(\\pi_{1i}\\)—turn composed -person deviations, \\(\\zeta_{0i}\\) \\(\\zeta_{1i}\\), population average true initial status, \\(\\gamma_{00}\\), population average true rate change, \\(\\gamma_{10}\\), respectively. model Chapter 3’s individual growth model—new name emphasize model includes substantive predictors—can plot trajectories usual.","code":"model_B <- lmer(   alcohol_use ~ I(age - 14) + (I(age - 14) | id),   data = alcohol_use_1,   REML = FALSE )  summary(model_B) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: alcohol_use ~ I(age - 14) + (I(age - 14) | id) #>    Data: alcohol_use_1 #>  #>      AIC      BIC   logLik deviance df.resid  #>    648.6    669.6   -318.3    636.6      240  #>  #> Scaled residuals:  #>      Min       1Q   Median       3Q      Max  #> -2.47999 -0.38401 -0.07553  0.39001  2.50685  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr  #>  id       (Intercept) 0.6244   0.7902         #>           I(age - 14) 0.1512   0.3888   -0.22 #>  Residual             0.3373   0.5808         #> Number of obs: 246, groups:  id, 82 #>  #> Fixed effects: #>             Estimate Std. Error t value #> (Intercept)  0.65130    0.10508   6.198 #> I(age - 14)  0.27065    0.06245   4.334 #>  #> Correlation of Fixed Effects: #>             (Intr) #> I(age - 14) -0.441 model_B |>   augment() |>   select(-alcohol_use) |>   rename(alcohol_use = .fitted, age = `I(age - 14)`) |>   mutate(age = as.numeric(age + 14)) |>   ggplot(aes(x = age, y = alcohol_use)) +     geom_line(aes(linewidth = \"individual\", group = id), colour = \"#3366FF\") +     geom_line(       aes(linewidth = \"average\"),       data = tibble(         age = measurement_occasions,         alcohol_use = predict(           model_B,           tibble(age = measurement_occasions),           re.form = NA         )       ),       colour = \"#3366FF\"     ) +     scale_linewidth_manual(values = c(2, .25)) +     scale_x_continuous(breaks = 13:17) +     coord_cartesian(xlim = c(13, 17), ylim = c(0, 4)) +     labs(linewidth = \"trajectory\")"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-4.html","id":"practical-data-analytic-strategies-for-model-building","dir":"Articles","previous_headings":"","what":"4.5 Practical Data Analytic Strategies for Model Building","title":"Chapter 4: Doing data analysis with the multilevel model for change","text":"Section 4.5 Singer Willet (2003) present data analytic strategy model building, focuses building systematic sequence models , set, address research questions meaningful way. refer sequence taxonomy statistical models, : model taxonomy extends prior model sensible way. Decisions enter, retain, remove predictors based combination logic, theory, prior research, supplemented hypothesis testing comparisons model fit. taxonomy progresses toward “final” model whose interpretation addresses research questions. present strategy one potential analytic path alcohol_use_1 data, research question focused relationship changes adolescent alcohol use child alcoholic parent. first substantive model, Model C, updates unconditional growth model include child_of_alcoholic predictor initial status rate change. Singer Willet (2003) added terms logical first step, given research question. Model D builds upon Model C, controlling effects peer_alcohol_use initial status rate change. Singer Willet (2003) added terms see might explain conditional residual variation initial status rate change Model C. Model E reduces Model D, removing child_of_alcoholic predictor rate change. Singer Willet (2003) removed term based results Models C D, estimated difference rate change alcohol_use children alchoholic nonalcoholic parents practically zero. Model F serves alternative Model E, peer_alcohol_use centred sample mean 1.018 (person-level data set). Singer Willet (2003) centred peer_alcohol_use level-2 intercepts, \\(\\gamma_{00}\\) \\(\\gamma_{10}\\), represent child non-alchoholic parents average value peer_alcohol_use (peer_alcohol_use = 1.018 child_of_alcoholic = 0), rather child non-alchoholic parents whose peers age 14 totally abstinent (peer_alcohol_use = 0 child_of_alcoholic = 0). Finally, Model G serves alternative Model F, child_of_alcoholic also centred sample mean 0.451 (person-level data set). Singer Willet (2003) also centred child_of_alcoholic level-2 intercepts, \\(\\gamma_{00}\\) \\(\\gamma_{10}\\), represent adolescent average values peer_alcohol_use child_of_alcoholic (peer_alcohol_use = 1.018 child_of_alcoholic = 0.451), numerically identical corresponding level-2 intercepts unconditional growth model. make taxonomy statistical models easier work subsequent sections, store models list.","code":"model_C <- update(   model_B,   . ~ . + child_of_alcoholic + I(age - 14):child_of_alcoholic )  summary(model_C) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: alcohol_use ~ I(age - 14) + (I(age - 14) | id) + child_of_alcoholic +   #>     I(age - 14):child_of_alcoholic #>    Data: alcohol_use_1 #>  #>      AIC      BIC   logLik deviance df.resid  #>    637.2    665.2   -310.6    621.2      238  #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -2.5480 -0.3880 -0.1058  0.3602  2.3961  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr  #>  id       (Intercept) 0.4876   0.6983         #>           I(age - 14) 0.1506   0.3881   -0.22 #>  Residual             0.3373   0.5808         #> Number of obs: 246, groups:  id, 82 #>  #> Fixed effects: #>                                Estimate Std. Error t value #> (Intercept)                     0.31595    0.13070   2.417 #> I(age - 14)                     0.29296    0.08423   3.478 #> child_of_alcoholic              0.74321    0.19457   3.820 #> I(age - 14):child_of_alcoholic -0.04943    0.12539  -0.394 #>  #> Correlation of Fixed Effects: #>             (Intr) I(g-14) chld__ #> I(age - 14) -0.460                #> chld_f_lchl -0.672  0.309         #> I(-14):ch__  0.309 -0.672  -0.460 model_D <- update(   model_C,   . ~ . + peer_alcohol_use + I(age - 14):peer_alcohol_use )  summary(model_D) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: alcohol_use ~ I(age - 14) + (I(age - 14) | id) + child_of_alcoholic +   #>     peer_alcohol_use + I(age - 14):child_of_alcoholic + I(age -   #>     14):peer_alcohol_use #>    Data: alcohol_use_1 #>  #>      AIC      BIC   logLik deviance df.resid  #>    608.7    643.7   -294.3    588.7      236  #>  #> Scaled residuals:  #>      Min       1Q   Median       3Q      Max  #> -2.59554 -0.40005 -0.07769  0.46003  2.29373  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr  #>  id       (Intercept) 0.2409   0.4908         #>           I(age - 14) 0.1391   0.3730   -0.03 #>  Residual             0.3373   0.5808         #> Number of obs: 246, groups:  id, 82 #>  #> Fixed effects: #>                                Estimate Std. Error t value #> (Intercept)                    -0.31651    0.14806  -2.138 #> I(age - 14)                     0.42943    0.11369   3.777 #> child_of_alcoholic              0.57917    0.16249   3.564 #> peer_alcohol_use                0.69430    0.11153   6.225 #> I(age - 14):child_of_alcoholic -0.01403    0.12477  -0.112 #> I(age - 14):peer_alcohol_use   -0.14982    0.08564  -1.749 #>  #> Correlation of Fixed Effects: #>             (Intr) I(g-14) chld__ pr_lc_ I(-14):c__ #> I(age - 14) -0.436                                  #> chld_f_lchl -0.371  0.162                           #> peer_lchl_s -0.686  0.299  -0.162                   #> I(-14):ch__  0.162 -0.371  -0.436  0.071            #> I(-14):pr__  0.299 -0.686   0.071 -0.436 -0.162 model_E <- update(   model_D,   . ~ . - I(age - 14):child_of_alcoholic )  summary(model_E) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: alcohol_use ~ I(age - 14) + (I(age - 14) | id) + child_of_alcoholic +   #>     peer_alcohol_use + I(age - 14):peer_alcohol_use #>    Data: alcohol_use_1 #>  #>      AIC      BIC   logLik deviance df.resid  #>    606.7    638.3   -294.4    588.7      237  #>  #> Scaled residuals:  #>      Min       1Q   Median       3Q      Max  #> -2.59554 -0.40414 -0.08352  0.45550  2.29975  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr  #>  id       (Intercept) 0.2409   0.4908         #>           I(age - 14) 0.1392   0.3730   -0.03 #>  Residual             0.3373   0.5808         #> Number of obs: 246, groups:  id, 82 #>  #> Fixed effects: #>                              Estimate Std. Error t value #> (Intercept)                  -0.31382    0.14611  -2.148 #> I(age - 14)                   0.42469    0.10559   4.022 #> child_of_alcoholic            0.57120    0.14623   3.906 #> peer_alcohol_use              0.69518    0.11126   6.249 #> I(age - 14):peer_alcohol_use -0.15138    0.08451  -1.791 #>  #> Correlation of Fixed Effects: #>             (Intr) I(g-14) chld__ pr_lc_ #> I(age - 14) -0.410                       #> chld_f_lchl -0.338  0.000                #> peer_lchl_s -0.709  0.351  -0.146        #> I(-14):pr__  0.334 -0.814   0.000 -0.431 model_F <- update(   model_E,   data = mutate(alcohol_use_1, peer_alcohol_use = peer_alcohol_use - 1.018) )  summary(model_F) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: alcohol_use ~ I(age - 14) + (I(age - 14) | id) + child_of_alcoholic +   #>     peer_alcohol_use + I(age - 14):peer_alcohol_use #>    Data: mutate(alcohol_use_1, peer_alcohol_use = peer_alcohol_use - 1.018) #>  #>      AIC      BIC   logLik deviance df.resid  #>    606.7    638.3   -294.4    588.7      237  #>  #> Scaled residuals:  #>      Min       1Q   Median       3Q      Max  #> -2.59554 -0.40414 -0.08352  0.45550  2.29975  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr  #>  id       (Intercept) 0.2409   0.4908         #>           I(age - 14) 0.1392   0.3730   -0.03 #>  Residual             0.3373   0.5808         #> Number of obs: 246, groups:  id, 82 #>  #> Fixed effects: #>                              Estimate Std. Error t value #> (Intercept)                   0.39387    0.10354   3.804 #> I(age - 14)                   0.27058    0.06127   4.416 #> child_of_alcoholic            0.57120    0.14623   3.906 #> peer_alcohol_use              0.69518    0.11126   6.249 #> I(age - 14):peer_alcohol_use -0.15138    0.08451  -1.791 #>  #> Correlation of Fixed Effects: #>             (Intr) I(g-14) chld__ pr_lc_ #> I(age - 14) -0.336                       #> chld_f_lchl -0.637  0.000                #> peer_lchl_s  0.094  0.000  -0.146        #> I(-14):pr__  0.000  0.001   0.000 -0.431 model_G <- update(   model_F,   data = mutate(     alcohol_use_1,     peer_alcohol_use = peer_alcohol_use - 1.018,     child_of_alcoholic = child_of_alcoholic - 0.451   ) )  summary(model_G) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: alcohol_use ~ I(age - 14) + (I(age - 14) | id) + child_of_alcoholic +   #>     peer_alcohol_use + I(age - 14):peer_alcohol_use #>    Data: mutate(alcohol_use_1, peer_alcohol_use = peer_alcohol_use - 1.018,   #>     child_of_alcoholic = child_of_alcoholic - 0.451) #>  #>      AIC      BIC   logLik deviance df.resid  #>    606.7    638.3   -294.4    588.7      237  #>  #> Scaled residuals:  #>      Min       1Q   Median       3Q      Max  #> -2.59554 -0.40414 -0.08352  0.45550  2.29975  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr  #>  id       (Intercept) 0.2409   0.4908         #>           I(age - 14) 0.1392   0.3730   -0.03 #>  Residual             0.3373   0.5808         #> Number of obs: 246, groups:  id, 82 #>  #> Fixed effects: #>                              Estimate Std. Error t value #> (Intercept)                   0.65148    0.07979   8.165 #> I(age - 14)                   0.27058    0.06127   4.416 #> child_of_alcoholic            0.57120    0.14623   3.906 #> peer_alcohol_use              0.69518    0.11126   6.249 #> I(age - 14):peer_alcohol_use -0.15138    0.08451  -1.791 #>  #> Correlation of Fixed Effects: #>             (Intr) I(g-14) chld__ pr_lc_ #> I(age - 14) -0.436                       #> chld_f_lchl  0.000  0.000                #> peer_lchl_s  0.001  0.000  -0.146        #> I(-14):pr__  0.000  0.001   0.000 -0.431 alcohol_use_1_fits <- list(   `Model A` = model_A,   `Model B` = model_B,   `Model C` = model_C,   `Model D` = model_D,   `Model E` = model_E,   `Model F` = model_F,   `Model G` = model_G )"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-4.html","id":"inspecting-model-summary-and-goodness-of-fit-statistics","dir":"Articles","previous_headings":"4.5 Practical Data Analytic Strategies for Model Building","what":"Inspecting model summary and goodness-of-fit statistics","title":"Chapter 4: Doing data analysis with the multilevel model for change","text":"addition output summary() can return one row tibble model summary goodness--fit statistics using glance() function broom.mixed package. Individual statistics can also returned using generic functions corresponding name (e.g., AIC(), BIC(), deviance(), etc.). Singer Willet (2003) also introduce three pseudo-\\(R^2\\) statistics multilevel model change, can used cautiously quantify much outcome variation “explained” model’s predictors. first statistic, \\(R^2_{y \\hat y}\\), assesses proportion total outcome variation “explained” model’s specific combination predictors, based squared sample correlation observed predicted values. second statistic, \\(R^2_{\\epsilon}\\), assesses proportion within-person variation “explained” time, based proportional decrease within-person residual variance unconditional means model subsequent models. Note way reducing variance component add time-varying predictors level-1 submodel, statistic models fit alcohol_use_1 data. final statistic, \\(R^2_{\\zeta}\\), assesses proportion -person variation “explained” one level-2 predictors, based proportional decrease level-2 residual variance unconditional growth model subsequent models level-2 residual variance component. adding statistics table next section, join together .","code":"glance(model_A) #> # A tibble: 1 × 7 #>    nobs sigma logLik   AIC   BIC deviance df.residual #>   <int> <dbl>  <dbl> <dbl> <dbl>    <dbl>       <int> #> 1   246 0.749  -335.  676.  687.     670.         243 r2_yy <- alcohol_use_1_fits |>    map(     \\(.fit) {       .fit |>         augment() |>         summarise(           r2_yy = cor(alcohol_use, .fixed)^2         )     }   ) |>   list_rbind(names_to = \"model\") r2_e <- alcohol_use_1_fits[2:7] |>    map(     \\(.fit) {       .fit |>         augment() |>         summarise(           r2_e = (sigma(model_A)^2 - sigma(.fit)^2) / sigma(model_A)^2         )     }   ) |>   list_rbind(names_to = \"model\") r2_z <- alcohol_use_1_fits[3:7] |>   map(     \\(.fit) {       zeta <- map(         list(x = model_B, y = .fit),         \\(.fit2) {           .fit2 |>             tidy(effects = \"ran_pars\", scales = \"vcov\") |>             filter(group != \"Residual\" & stringr::str_detect(term, \"^var\"))         }       )              zeta$x |>         left_join(zeta$y, by = c(\"effect\", \"group\", \"term\")) |>         mutate(           r2 = (estimate.x - estimate.y) / estimate.x,           name = c(\"r2_z1\", \"r2_z2\")         ) |>         select(name, r2) |>         pivot_wider(names_from = name, values_from = r2)     }   ) |>   list_rbind(names_to = \"model\") alcohol_use_1_fits_r2 <- r2_yy |>   left_join(r2_e) |>   left_join(r2_z)  alcohol_use_1_fits_r2 #> # A tibble: 7 × 5 #>   model     r2_yy   r2_e  r2_z1    r2_z2 #>   <chr>     <dbl>  <dbl>  <dbl>    <dbl> #> 1 Model A NA      NA     NA     NA       #> 2 Model B  0.0434  0.400 NA     NA       #> 3 Model C  0.150   0.400  0.219  0.00401 #> 4 Model D  0.291   0.400  0.614  0.0799  #> 5 Model E  0.291   0.400  0.614  0.0797  #> 6 Model F  0.291   0.400  0.614  0.0797  #> 7 Model G  0.291   0.400  0.614  0.0797"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-4.html","id":"interpreting-fitted-models","dir":"Articles","previous_headings":"4.5 Practical Data Analytic Strategies for Model Building","what":"Interpreting Fitted Models","title":"Chapter 4: Doing data analysis with the multilevel model for change","text":"systematically compare fitted models—describing happens predictors added removed—Singer Willet (2003) suggest placing side--side table, allows easily inspect compare estimated fixed effects, variance components, goodness--fit statistics one model next. can construct table using modelsummary() function modelsummary package. better match table text, set table output \"gt\" can post-process using gt package.","code":"# This option needs to be set in order to make all the desired goodness-of-fit # statistics available to modelsummary. options(modelsummary_get = \"all\")  # Table 4.1, page 94-95: alcohol_use_1_fits |>   modelsummary(     shape = term + effect + statistic ~ model,     scales = c(\"vcov\", NA), # argument from broom.mixed::tidy()     coef_map = c(       \"(Intercept)\",       \"child_of_alcoholic\",       \"peer_alcohol_use\",       \"I(age - 14)\",       \"I(age - 14):child_of_alcoholic\",       \"I(age - 14):peer_alcohol_use\",       \"var__Observation\",       \"var__(Intercept)\",       \"var__I(age - 14)\",       \"cov__(Intercept).I(age - 14)\"     ),     gof_map = tibble(       raw = c(\"deviance\", \"AIC\", \"BIC\"),       clean = c(\"Deviance\", \"AIC\", \"BIC\"),       fmt = 2     ),     # The R2s need to be transposed to be added to the table columns. Their     # position in the table is set by the `position` attribute.     add_rows = alcohol_use_1_fits_r2 |>       pivot_longer(-model, names_to = \"estimate\") |>       pivot_wider(names_from = model) |>       mutate(effect = \"\", .after = estimate) |>       structure(position = 17:21),     output = \"gt\"   ) |>   tab_row_group(label = \"Goodness-of-Fit\", rows = 17:23) |>   tab_row_group(label = \"Variance Components\", rows = 13:16) |>   tab_row_group(label = \"Fixed Effects\", rows = 1:12) |>   cols_hide(effect)"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-4.html","id":"displaying-prototypical-change-trajectories","dir":"Articles","previous_headings":"4.5 Practical Data Analytic Strategies for Model Building","what":"Displaying Prototypical Change Trajectories","title":"Chapter 4: Doing data analysis with the multilevel model for change","text":"addition numerical summaries, Singer Willett (2003) suggest plotting fitted trajectories prototypical individuals describe results model fitting, prototypical values predictors selected using one following strategies: Choosing substantively interesting values, categorical continuous predictors well-known values. Using range percentiles, continuous predictors without well-known values. Using standard deviations sample mean, continuous predictors without well-known values. Using sample mean, categorical continuous predictors want control . selecting prototypical values predictors, prototypical change trajectories can derived combinations values using usual model predictions. convenience, can use estimate_prediction() function modelbased package make predictions, map2() function purrr package iterate desired model prototypical value combinations. look prototypical change trajectories Models B, C, E. systematically compare prototypical change trajectories, can helpful plot side--side. However, certain predictors present models others, need supply na.value scale_() functions ensure trajectory appears panels predictors present.  Note depending number predictors across different models, may preferable instead create separate plots (later added together using patchwork package).","code":"prototypical_alcohol_use <- alcohol_use_1_fits |>   keep_at(paste0(\"Model \", c(\"B\", \"C\", \"E\"))) |>   map2(     list(       tibble(age = 14:16),       crossing(age = 14:16, child_of_alcoholic = 0:1),       crossing(         age = 14:16, child_of_alcoholic = 0:1, peer_alcohol_use = c(0.655, 1.381)       )     ),     \\(.fit, .data) {       .fit |>         estimate_prediction(data = .data) |>         rename(alcohol_use = Predicted) |>         as_tibble()     }   ) |>   list_rbind(names_to = \"model\") |>   mutate(     child_of_alcoholic = factor(child_of_alcoholic),     peer_alcohol_use = factor(peer_alcohol_use, labels = c(\"low\", \"high\"))   )  prototypical_alcohol_use #> # A tibble: 21 × 8 #>    model     age alcohol_use    SE  CI_low CI_high child_of_alcoholic #>    <chr>   <int>       <dbl> <dbl>   <dbl>   <dbl> <fct>              #>  1 Model B    14       0.651 0.590 -0.511     1.81 NA                 #>  2 Model B    15       0.922 0.589 -0.238     2.08 NA                 #>  3 Model B    16       1.19  0.594  0.0233    2.36 NA                 #>  4 Model C    14       0.316 0.595 -0.857     1.49 0                  #>  5 Model C    14       1.06  0.598 -0.120     2.24 1                  #>  6 Model C    15       0.609 0.593 -0.559     1.78 0                  #>  7 Model C    15       1.30  0.595  0.130     2.48 1                  #>  8 Model C    16       0.902 0.602 -0.284     2.09 0                  #>  9 Model C    16       1.55  0.607  0.351     2.74 1                  #> 10 Model E    14       0.142 0.591 -1.02      1.31 0                  #> # ℹ 11 more rows #> # ℹ 1 more variable: peer_alcohol_use <fct> # Figure 4.3, page 99: prototypical_alcohol_use |>   ggplot(aes(x = age, y = alcohol_use)) +     geom_line(aes(linetype = child_of_alcoholic, colour = peer_alcohol_use)) +     scale_linetype_manual(values = c(2, 6), na.value = 1) +     scale_color_viridis_d(       option = \"G\", begin = .4, end = .7, na.value = \"black\"     ) +     scale_x_continuous(breaks = 13:17) +     coord_cartesian(xlim = c(13, 17), ylim = c(0, 2)) +     facet_wrap(vars(model))"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-4.html","id":"comparing-models-using-deviance-statistics","dir":"Articles","previous_headings":"","what":"4.6 Comparing Models Using Deviance Statistics","title":"Chapter 4: Doing data analysis with the multilevel model for change","text":"Section 4.6 Singer Willett (2003) introduce deviance statistic, quantifies much worse current model fit comparison saturated model fits observed data perfectly comparing log-likelihood statistics two models: \\[ \\text{Deviance} = -2 (LL_\\text{current model} - LL_\\text{saturated model}). \\] Note multilevel model change equation reduces : \\[ \\text{Deviance} = -2LL_\\text{current model}, \\] log-likelihood statistic saturated model always zero. Deviance statistics nested models estimated using identical data can compared using anova() function, computes analysis deviance tables one fitted models. Unfortunately anova() doesn’t accept list input, meta-programming use list models don’t want type model hand. Note default anova() function refit objects class merMod FML comparing models estimated REML prevent common mistake inappropriately comparing REML-fitted models different fixed effects, whose likelihoods directly comparable. REML-fitted models identical fixed effects different random effects, refit argument can set FALSE directly compare REML-fitted models.","code":"with(   alcohol_use_1_fits[1:5],   do.call(anova, map(names(alcohol_use_1_fits[1:5]), as.name)) ) #> Data: alcohol_use_1 #> Models: #> Model A: alcohol_use ~ 1 + (1 | id) #> Model B: alcohol_use ~ I(age - 14) + (I(age - 14) | id) #> Model C: alcohol_use ~ I(age - 14) + (I(age - 14) | id) + child_of_alcoholic + I(age - 14):child_of_alcoholic #> Model E: alcohol_use ~ I(age - 14) + (I(age - 14) | id) + child_of_alcoholic + peer_alcohol_use + I(age - 14):peer_alcohol_use #> Model D: alcohol_use ~ I(age - 14) + (I(age - 14) | id) + child_of_alcoholic + peer_alcohol_use + I(age - 14):child_of_alcoholic + I(age - 14):peer_alcohol_use #>         npar    AIC    BIC  logLik deviance   Chisq Df Pr(>Chisq)     #> Model A    3 676.16 686.67 -335.08   670.16                           #> Model B    6 648.61 669.64 -318.31   636.61 33.5449  3  2.472e-07 *** #> Model C    8 637.20 665.25 -310.60   621.20 15.4085  2  0.0004509 *** #> Model E    9 606.70 638.25 -294.35   588.70 32.4993  1  1.192e-08 *** #> Model D   10 608.69 643.74 -294.35   588.69  0.0126  1  0.9104569     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-4.html","id":"using-wald-statistics-to-test-composite-hypotheses-about-fixed-effects","dir":"Articles","previous_headings":"","what":"4.7 Using Wald Statistics to Test Composite Hypotheses About Fixed Effects","title":"Chapter 4: Doing data analysis with the multilevel model for change","text":"section intentionally left blank.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-4.html","id":"evaluating-the-tenability-of-a-models-assumptions","dir":"Articles","previous_headings":"","what":"4.8 Evaluating the Tenability of a Model’s Assumptions","title":"Chapter 4: Doing data analysis with the multilevel model for change","text":"Section 4.8 Singer Willett (2003) offer strategies checking following assumptions multilevel model change: linear (nonlinear) functional form hypothesized individual change trajectory seems reasonable observed data—appear systematic deviations linearity (nonlinearity) across participants. level-1 level-2 residuals normally distributed. level-1 level-2 residuals equal variances level every predictor.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-4.html","id":"checking-functional-form","dir":"Articles","previous_headings":"4.8 Evaluating the Tenability of a Model’s Assumptions","what":"Checking Functional Form","title":"Chapter 4: Doing data analysis with the multilevel model for change","text":"functional form assumption multilevel model change can assessed inspecting “outcome versus predictors” plots level. level-1, empirical growth plots superimposed individual change trajectories support suitability specified functional form. Empirical growth plots examined individual (several subsamples individuals), looking systematic deviations disconfirm suitability hypothesized individual change trajectory.  level-2, OLS-estimated individual growth parameters plotted substantive predictor confirm suitability specified level-2 relationships. linear models, continuous predictors need assessed categorical predictors always linear.","code":"set.seed(333) alcohol_use_1 |>   filter(id %in% sample(unique(id), size = 16)) |>   ggplot(aes(x = age, y = alcohol_use)) +     stat_smooth(method = \"lm\", se = FALSE) +     geom_point() +     coord_cartesian(xlim = c(13, 17), ylim = c(0, 4)) +     facet_wrap(vars(id), ncol = 4, labeller = label_both) alcohol_use_1_fit_np <- lmList(   alcohol_use ~ I(age - 14) | id,   pool = FALSE,   data = alcohol_use_1 )  alcohol_use_1_est_np <- alcohol_use_1_fit_np |>   map(tidy) |>   list_rbind(names_to = \"id\") |>   select(id:estimate, alcohol_use = estimate) |>   left_join(alcohol_use_1_pl) |>   mutate(child_of_alcoholic = factor(child_of_alcoholic))  alcohol_use_1_ovp <- map(   list(\"child_of_alcoholic\", \"peer_alcohol_use\"),   \\(.x) {     ggplot(alcohol_use_1_est_np, aes(x = .data[[.x]], y = alcohol_use)) +       geom_hline(yintercept = 0, alpha = .25) +       geom_point() +       facet_wrap(vars(term), ncol = 1, scales = \"free_y\")   } )  # Figure 4.4: wrap_plots(alcohol_use_1_ovp) +   plot_layout(axes = \"collect\")"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-4.html","id":"checking-normality","dir":"Articles","previous_headings":"4.8 Evaluating the Tenability of a Model’s Assumptions","what":"Checking Normality","title":"Chapter 4: Doing data analysis with the multilevel model for change","text":"normality assumption multilevel model change can assessed inspecting Q-Q plots level-1 level-2 residuals, also (optionally) statistical tests normality. check_normality() function performance package can perform tasks. plot() method check_normality() can used return Q-Q plots level-1 level-2 residuals.","code":"model_F_normality <- map(   set_names(c(\"fixed\", \"random\")),   \\(.x) check_normality(model_F, effects = .x) )  model_F_normality #> $fixed #> Warning: Non-normality of residuals detected (p = 0.011). #>  #> $random #> OK: Random effects 'id: (Intercept)' appear as normally distributed (p = 0.270). #> Warning: Non-normality for random effects 'id: I(age - 14)' detected (p < .001). # Figure 4.5 (left panels), page 131: plot(model_F_normality$fixed, detrend = FALSE) +   plot(model_F_normality$random) +   plot_layout(widths = c(1/3, 2/3)) &   theme_bw() &   theme(panel.grid = element_blank())"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-4.html","id":"checking-homoscedasticity","dir":"Articles","previous_headings":"4.8 Evaluating the Tenability of a Model’s Assumptions","what":"Checking Homoscedasticity","title":"Chapter 4: Doing data analysis with the multilevel model for change","text":"homoscedasticity assumption multilevel model change can assessed inspecting “residual versus predictors” plots level see residual variability approximately equal every predictor value. level-1 residuals plotted level-1 predictor.  level-2 residuals plotted level-2 predictor(s).","code":"# Figure 4.6 (top panel), page 133: model_F |>   augment(re.form = NA) |>   rename(age = `I(age - 14)`) |>   mutate(age = as.numeric(age + 14)) |>   ggplot(aes(x = age, y = .resid)) +     geom_hline(yintercept = 0, alpha = .25) +     geom_point() +     coord_cartesian(xlim = c(13, 17), ylim = c(-2, 2)) alcohol_use_1_ranef <- model_F |>   ranef() |>   augment() |>   as_tibble() |>   rename(term = variable, id = level, .resid = estimate) |>   left_join(alcohol_use_1_pl) |>   mutate(child_of_alcoholic = factor(child_of_alcoholic))  alcohol_use_1_rvp <- map(   list(\"child_of_alcoholic\", \"peer_alcohol_use\"),   \\(.x) {     ggplot(alcohol_use_1_ranef, aes(x = .data[[.x]], y = .resid)) +       geom_hline(yintercept = 0, alpha = .25) +       geom_point() +       facet_wrap(vars(term), ncol = 1, scales = \"free_y\") +       coord_cartesian(ylim = c(-1, 1))   } )  # Figure 4.6 (bottom panels), page 133: wrap_plots(alcohol_use_1_rvp) + plot_layout(axes = \"collect\")"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-4.html","id":"model-based-estimates-of-the-individual-growth-parameters","dir":"Articles","previous_headings":"","what":"4.9 Model-Based Estimates of the Individual Growth Parameters","title":"Chapter 4: Doing data analysis with the multilevel model for change","text":"Section 4.9 Singer Willett (2003) discuss use model-based estimates display individual growth trajectories, simply partial pooling trajectories previously discussed Chapter 3. begin predicting three types growth trajectories individual: (1) pooling trajectory estimated separate linear models; (2) population average trajectory estimated multilevel model change, condition random effects; (3) model-based trajectory estimated multilevel model change, condition random effects. Now can plot three trajectories.  Similar partial pooling example Chapter 3, notice : population average trajectories stable, varying least across individuals. pooling trajectories least stable, varying across individuals. model-based trajectories fall somewhere population average pooling trajectories, due effects partial pooling. Although typically prefer model-based trajectories multilevel model change, Singer Willet (2003) conclude cautioning model-based trajectories flawed model flawed well—quality depends heavily quality model fit soundness model’s assumptions (given data).","code":"alcohol_use_1_np_pred <- alcohol_use_1_fit_np |>   map(augment) |>   list_rbind(names_to = \"id\") |>   mutate(trajectory = \"no_pooling\")  alcohol_use_1_pp_pred <- list(population_average = NA, model_based = NULL) |>   map(\\(.x) augment(model_F, re.form = .x)) |>   list_rbind(names_to = \"trajectory\")  # For display purposes we will tidy up the prediction data frames and only use # a subset of participants. alcohol_use_1_preds <- alcohol_use_1_np_pred |>   bind_rows(alcohol_use_1_pp_pred) |>   filter(id %in% c(4, 14, 23, 32, 41, 56, 65, 82)) |>   rename(age = `I(age - 14)`) |>   mutate(     trajectory = factor(       trajectory, levels = c(\"population_average\", \"no_pooling\", \"model_based\")     ),     id = factor(id, levels = sort(as.numeric(unique(id)))),     age = as.numeric(age + 14),   ) |>   select(trajectory, id, age, alcohol_use, .fitted)  alcohol_use_1_preds #> # A tibble: 72 × 5 #>    trajectory id      age alcohol_use .fitted #>    <fct>      <fct> <dbl>       <dbl>   <dbl> #>  1 no_pooling 4        14        0      0.378 #>  2 no_pooling 4        15        2      1.24  #>  3 no_pooling 4        16        1.73   2.11  #>  4 no_pooling 14       14        2.83   3.09  #>  5 no_pooling 14       15        3.61   3.09  #>  6 no_pooling 14       16        2.83   3.09  #>  7 no_pooling 23       14        1      0.878 #>  8 no_pooling 23       15        1      1.24  #>  9 no_pooling 23       16        1.73   1.61  #> 10 no_pooling 32       14        1.73   1.75  #> # ℹ 62 more rows # Figure 4.7: ggplot(alcohol_use_1_preds, aes(x = age)) +   geom_point(aes(y = alcohol_use)) +   geom_line(aes(y = .fitted, colour = trajectory)) +   scale_colour_brewer(palette = \"Dark2\") +   scale_y_continuous(breaks = 0:4) +   coord_cartesian(xlim = c(13, 17), ylim = c(-1, 4)) +   facet_wrap(vars(id), nrow = 2, labeller = label_both)"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-5.html","id":"variably-spaced-measurement-occasions","dir":"Articles","previous_headings":"","what":"5.1 Variably Spaced Measurement Occasions","title":"Chapter 5: Treating time more flexibly","text":"Table 5.1, page 141: Figure 5.1, page 143:  Table 5.2, page 145:","code":"reading_scores #> # A tibble: 267 × 5 #>    id     wave age_group   age reading_score #>    <fct> <dbl>     <dbl> <dbl>         <dbl> #>  1 1         1       6.5  6               18 #>  2 1         2       8.5  8.33            35 #>  3 1         3      10.5 10.3             59 #>  4 2         1       6.5  6               18 #>  5 2         2       8.5  8.5             25 #>  6 2         3      10.5 10.6             28 #>  7 3         1       6.5  6.08            18 #>  8 3         2       8.5  8.42            23 #>  9 3         3      10.5 10.4             32 #> 10 4         1       6.5  6               18 #> # ℹ 257 more rows reading_scores |>   filter(id %in% c(4, 27, 31, 33, 41, 49, 69, 77, 87)) |>   pivot_longer(     starts_with(\"age\"), names_to = \"time_structure\", values_to = \"time\"   ) |>   ggplot(aes(x = time, y = reading_score, colour = time_structure)) +     geom_point(alpha = .5) +     stat_smooth(method = \"lm\", se = FALSE, linewidth = .5) +     scale_x_continuous(breaks = 5:12) +     coord_cartesian(xlim = c(5, 12), ylim = c(0, 80)) +     facet_wrap(vars(id)) #> `geom_smooth()` using formula = 'y ~ x' age_group_fit <- lmer(   reading_score ~ I(age_group - 6.5) + (1 + I(age_group - 6.5) | id),   data = reading_scores,   REML = FALSE )  age_fit <- lmer(   reading_score ~ I(age - 6.5) + (1 + I(age - 6.5) | id),   data = reading_scores,   REML = FALSE )  summary(age_group_fit) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: reading_score ~ I(age_group - 6.5) + (1 + I(age_group - 6.5) |   #>     id) #>    Data: reading_scores #>  #>      AIC      BIC   logLik deviance df.resid  #>   1831.9   1853.5   -910.0   1819.9      261  #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -2.6585 -0.5419 -0.1434  0.3872  3.3133  #>  #> Random effects: #>  Groups   Name               Variance Std.Dev. Corr #>  id       (Intercept)        11.046   3.324         #>           I(age_group - 6.5)  4.397   2.097    0.24 #>  Residual                    27.043   5.200         #> Number of obs: 267, groups:  id, 89 #>  #> Fixed effects: #>                    Estimate Std. Error t value #> (Intercept)         21.1629     0.6143   34.45 #> I(age_group - 6.5)   5.0309     0.2956   17.02 #>  #> Correlation of Fixed Effects: #>             (Intr) #> I(g_gr-6.5) -0.316 summary(age_fit) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: reading_score ~ I(age - 6.5) + (1 + I(age - 6.5) | id) #>    Data: reading_scores #>  #>      AIC      BIC   logLik deviance df.resid  #>   1815.9   1837.4   -901.9   1803.9      261  #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -3.0068 -0.4949 -0.1370  0.4096  3.7274  #>  #> Random effects: #>  Groups   Name         Variance Std.Dev. Corr #>  id       (Intercept)   5.107   2.260         #>           I(age - 6.5)  3.301   1.817    0.58 #>  Residual              27.447   5.239         #> Number of obs: 267, groups:  id, 89 #>  #> Fixed effects: #>              Estimate Std. Error t value #> (Intercept)   21.0608     0.5593   37.66 #> I(age - 6.5)   4.5400     0.2606   17.42 #>  #> Correlation of Fixed Effects: #>            (Intr) #> I(age-6.5) -0.287"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-5.html","id":"varying-numbers-of-measurement-occasions","dir":"Articles","previous_headings":"","what":"5.2 Varying Numbers of Measurement Occasions","title":"Chapter 5: Treating time more flexibly","text":"Table 5.3, page 147: Table 5.4, page 149: Figure 5.2, page 150:  Table 5.5, page 154:","code":"dropout_wages |>   filter(id %in% c(206, 332, 1028)) |>   select(id, experience, log_wages, black, highest_grade, unemployment_rate) #> # A tibble: 20 × 6 #>    id    experience log_wages black highest_grade unemployment_rate #>    <fct>      <dbl>     <dbl> <dbl>         <dbl>             <dbl> #>  1 206        1.87      2.03      0            10              9.2  #>  2 206        2.81      2.30      0            10             11    #>  3 206        4.31      2.48      0            10              6.30 #>  4 332        0.125     1.63      0             8              7.1  #>  5 332        1.62      1.48      0             8              9.6  #>  6 332        2.41      1.80      0             8              7.2  #>  7 332        3.39      1.44      0             8              6.20 #>  8 332        4.47      1.75      0             8              5.60 #>  9 332        5.18      1.53      0             8              4.60 #> 10 332        6.08      2.04      0             8              4.30 #> 11 332        7.04      2.18      0             8              3.40 #> 12 332        8.20      2.19      0             8              4.39 #> 13 332        9.09      4.04      0             8              6.70 #> 14 1028       0.004     0.872     1             8              9.3  #> 15 1028       0.035     0.903     1             8              7.4  #> 16 1028       0.515     1.39      1             8              7.3  #> 17 1028       1.48      2.32      1             8              7.4  #> 18 1028       2.14      1.48      1             8              6.30 #> 19 1028       3.16      1.70      1             8              5.90 #> 20 1028       4.10      2.34      1             8              6.9 wages_fit_A <- lmer(   log_wages ~ experience + (1 + experience | id),   data = dropout_wages,   REML = FALSE )  summary(wages_fit_A) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: log_wages ~ experience + (1 + experience | id) #>    Data: dropout_wages #>  #>      AIC      BIC   logLik deviance df.resid  #>   4933.4   4974.0  -2460.7   4921.4     6396  #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -4.2011 -0.5207 -0.0316  0.4406  7.0370  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr  #>  id       (Intercept) 0.054268 0.23296        #>           experience  0.001726 0.04154  -0.30 #>  Residual             0.095105 0.30839        #> Number of obs: 6402, groups:  id, 888 #>  #> Fixed effects: #>             Estimate Std. Error t value #> (Intercept) 1.715604   0.010797  158.90 #> experience  0.045681   0.002342   19.51 #>  #> Correlation of Fixed Effects: #>            (Intr) #> experience -0.565  # Adding new predictors to Model A wages_fit_B <- update(   wages_fit_A,   . ~ . + experience * I(highest_grade - 9) + experience * black )  summary(wages_fit_B) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: log_wages ~ experience + (1 + experience | id) + I(highest_grade -   #>     9) + black + experience:I(highest_grade - 9) + experience:black #>    Data: dropout_wages #>  #>      AIC      BIC   logLik deviance df.resid  #>   4893.8   4961.4  -2436.9   4873.8     6392  #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -4.2691 -0.5181 -0.0335  0.4452  7.0194  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr  #>  id       (Intercept) 0.051746 0.22748        #>           experience  0.001636 0.04045  -0.31 #>  Residual             0.095194 0.30854        #> Number of obs: 6402, groups:  id, 888 #>  #> Fixed effects: #>                                  Estimate Std. Error t value #> (Intercept)                      1.717138   0.012542 136.908 #> experience                       0.049343   0.002632  18.749 #> I(highest_grade - 9)             0.034920   0.007881   4.431 #> black                            0.015395   0.023926   0.643 #> experience:I(highest_grade - 9)  0.001279   0.001723   0.742 #> experience:black                -0.018213   0.005499  -3.312 #>  #> Correlation of Fixed Effects: #>             (Intr) exprnc I(_-9) black  e:I(-9 #> experience  -0.575                             #> I(hghst_-9)  0.071 -0.020                      #> black       -0.523  0.301 -0.020               #> expr:I(_-9) -0.019 -0.003 -0.578  0.011        #> exprnc:blck  0.275 -0.478  0.011 -0.573 -0.023  # Removing predictors from Model B wages_fit_C <- update(   wages_fit_B,   . ~ . - experience:I(highest_grade - 9) - black,   # The model fails to converge with the default optimizer (although the   # estimates are fine). Changing the optimizer achieves convergence.   control = lmerControl(optimizer = \"bobyqa\") )  summary(wages_fit_C) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: log_wages ~ experience + (1 + experience | id) + I(highest_grade -   #>     9) + experience:black #>    Data: dropout_wages #> Control: lmerControl(optimizer = \"bobyqa\") #>  #>      AIC      BIC   logLik deviance df.resid  #>   4890.7   4944.8  -2437.4   4874.7     6394  #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -4.2788 -0.5183 -0.0347  0.4426  7.0170  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr  #>  id       (Intercept) 0.051831 0.22766        #>           experience  0.001647 0.04058  -0.31 #>  Residual             0.095174 0.30850        #> Number of obs: 6402, groups:  id, 888 #>  #> Fixed effects: #>                       Estimate Std. Error t value #> (Intercept)           1.721475   0.010697 160.929 #> experience            0.048847   0.002513  19.435 #> I(highest_grade - 9)  0.038361   0.006433   5.963 #> experience:black     -0.016115   0.004511  -3.572 #>  #> Correlation of Fixed Effects: #>             (Intr) exprnc I(_-9) #> experience  -0.515               #> I(hghst_-9)  0.077 -0.023        #> exprnc:blck -0.036 -0.391 -0.015 prototypical_dropouts <- crossing(   experience = c(0, 12),   highest_grade = c(0, 3) + 9,   black = c(FALSE, TRUE) )  prototypical_dropout_scores <- tibble(   log_wages = predict(     wages_fit_C,     prototypical_dropouts,     re.form = NA   ) )  prototypical_dropouts |>   bind_cols(prototypical_dropout_scores) |>   ggplot(aes(x = experience, y = log_wages, colour = black)) +     geom_line() +     scale_x_continuous(breaks = seq(0, 12, by = 2)) +     coord_cartesian(ylim = c(1.6, 2.4)) +     facet_wrap(vars(highest_grade)) wages_fit_A_subset <- update(   wages_fit_C,   data = dropout_wages_subset ) #> boundary (singular) fit: see help('isSingular')  summary(wages_fit_A_subset) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: log_wages ~ experience + (1 + experience | id) + I(highest_grade -   #>     9) + experience:black #>    Data: dropout_wages_subset #> Control: lmerControl(optimizer = \"bobyqa\") #>  #>      AIC      BIC   logLik deviance df.resid  #>    299.9    328.3   -141.9    283.9      249  #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -2.4109 -0.4754 -0.0290  0.4243  4.2842  #>  #> Random effects: #>  Groups   Name        Variance  Std.Dev. Corr #>  id       (Intercept) 8.215e-02 0.286615      #>           experience  3.526e-06 0.001878 1.00 #>  Residual             1.150e-01 0.339068      #> Number of obs: 257, groups:  id, 124 #>  #> Fixed effects: #>                      Estimate Std. Error t value #> (Intercept)           1.73734    0.04760  36.499 #> experience            0.05161    0.02108   2.449 #> I(highest_grade - 9)  0.04610    0.02447   1.884 #> experience:black     -0.05968    0.03477  -1.716 #>  #> Correlation of Fixed Effects: #>             (Intr) exprnc I(_-9) #> experience  -0.612               #> I(hghst_-9)  0.051 -0.133        #> exprnc:blck -0.129 -0.297  0.023 #> optimizer (bobyqa) convergence code: 0 (OK) #> boundary (singular) fit: see help('isSingular')  # {lme4} does not support the removal of boundary constraints to allow for # negative variance components, so Model B cannot be replicated.  # Remove varying slope for experience wages_fit_C_subset <- update(   wages_fit_A_subset,   . ~ . - (1 + experience | id) + (1 | id) )  summary(wages_fit_C_subset) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula:  #> log_wages ~ experience + I(highest_grade - 9) + (1 | id) + experience:black #>    Data: dropout_wages_subset #> Control: lmerControl(optimizer = \"bobyqa\") #>  #>      AIC      BIC   logLik deviance df.resid  #>    295.9    317.2   -141.9    283.9      251  #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -2.4202 -0.4722 -0.0290  0.4197  4.2439  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. #>  id       (Intercept) 0.08425  0.2903   #>  Residual             0.11480  0.3388   #> Number of obs: 257, groups:  id, 124 #>  #> Fixed effects: #>                      Estimate Std. Error t value #> (Intercept)           1.73734    0.04775  36.383 #> experience            0.05178    0.02093   2.474 #> I(highest_grade - 9)  0.04576    0.02450   1.868 #> experience:black     -0.06007    0.03458  -1.737 #>  #> Correlation of Fixed Effects: #>             (Intr) exprnc I(_-9) #> experience  -0.614               #> I(hghst_-9)  0.051 -0.135        #> exprnc:blck -0.130 -0.294  0.024"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-5.html","id":"time-varying-predictors","dir":"Articles","previous_headings":"","what":"5.3 Time-Varying Predictors","title":"Chapter 5: Treating time more flexibly","text":"Table 5.6, page 161: Table 5.7, page 163: Figure 5.4, page 167:    Table 5.8:","code":"filter(depression_unemployment, id %in% c(7589, 55697, 67641, 65441, 53782)) #> # A tibble: 14 × 4 #>    id    months depression unemployed #>    <fct>  <dbl>      <dbl>      <dbl> #>  1 7589   1.31          36          1 #>  2 7589   5.09          40          1 #>  3 7589  11.8           39          1 #>  4 53782  0.427         22          1 #>  5 53782  4.24          15          0 #>  6 53782 11.1           21          1 #>  7 55697  1.35           7          1 #>  8 55697  5.78           4          1 #>  9 65441  1.08          27          1 #> 10 65441  4.70          15          1 #> 11 65441 11.3            7          0 #> 12 67641  0.329         32          1 #> 13 67641  4.11           9          0 #> 14 67641 10.9           10          0 unemployment_fit_A <- lmer(   depression ~ months + (1 + months | id),   data = depression_unemployment,   REML = FALSE )  summary(unemployment_fit_A) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: depression ~ months + (1 + months | id) #>    Data: depression_unemployment #>  #>      AIC      BIC   logLik deviance df.resid  #>   5145.1   5172.2  -2566.6   5133.1      668  #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -1.9979 -0.5415 -0.1608  0.4454  3.5520  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr  #>  id       (Intercept) 86.8520  9.3194         #>           months       0.3551  0.5959   -0.55 #>  Residual             68.8475  8.2974         #> Number of obs: 674, groups:  id, 254 #>  #> Fixed effects: #>             Estimate Std. Error t value #> (Intercept) 17.66937    0.77557  22.783 #> months      -0.42199    0.08298  -5.086 #>  #> Correlation of Fixed Effects: #>        (Intr) #> months -0.632  # Adding a predictor to Model A unemployment_fit_B <- update(   unemployment_fit_A,   . ~ . + unemployed,   # The model fails to converge with the default optimizer (although the   # estimates are fine). Changing the optimizer achieves convergence.   control = lmerControl(optimizer = \"bobyqa\") )  summary(unemployment_fit_B) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: depression ~ months + (1 + months | id) + unemployed #>    Data: depression_unemployment #> Control: lmerControl(optimizer = \"bobyqa\") #>  #>      AIC      BIC   logLik deviance df.resid  #>   5121.6   5153.2  -2553.8   5107.6      667  #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -1.9610 -0.5357 -0.1192  0.4303  3.6857  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr  #>  id       (Intercept) 93.5189  9.6705         #>           months       0.4647  0.6817   -0.59 #>  Residual             62.3875  7.8986         #> Number of obs: 674, groups:  id, 254 #>  #> Fixed effects: #>             Estimate Std. Error t value #> (Intercept) 12.66559    1.24207  10.197 #> months      -0.20198    0.09332  -2.165 #> unemployed   5.11131    0.98884   5.169 #>  #> Correlation of Fixed Effects: #>            (Intr) months #> months     -0.715        #> unemployed -0.780  0.459  # Adding an interaction term to Model B unemployment_fit_C <- update(   unemployment_fit_B,   . ~ . + months:unemployed )  summary(unemployment_fit_C) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula:  #> depression ~ months + (1 + months | id) + unemployed + months:unemployed #>    Data: depression_unemployment #> Control: lmerControl(optimizer = \"bobyqa\") #>  #>      AIC      BIC   logLik deviance df.resid  #>   5119.0   5155.2  -2551.5   5103.0      666  #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -2.0039 -0.5237 -0.1548  0.4309  3.6961  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr  #>  id       (Intercept) 93.7133  9.6806         #>           months       0.4512  0.6717   -0.60 #>  Residual             62.0311  7.8760         #> Number of obs: 674, groups:  id, 254 #>  #> Fixed effects: #>                   Estimate Std. Error t value #> (Intercept)         9.6167     1.8893   5.090 #> months              0.1620     0.1937   0.837 #> unemployed          8.5291     1.8779   4.542 #> months:unemployed  -0.4652     0.2172  -2.142 #>  #> Correlation of Fixed Effects: #>             (Intr) months unmply #> months      -0.888               #> unemployed  -0.911  0.863        #> mnths:nmply  0.755 -0.878 -0.852   # Note: The original model was fit in SAS, which does not report any issues with # the model given the data; however, other programs (MPlus, SPSS, STATA, R) all # have convergence/singularity problems and it is not possible to get results # that match the textbook. Each of these programs react differently to this # situation, but it is reasonable to conclude the problem is not with the # software, but with this model given the data. unemployment_fit_D <- lmer(   depression ~      unemployed + months:unemployed + (1 + unemployed + months:unemployed | id),   data = depression_unemployment,   REML = FALSE,   control = lmerControl(     # The number of observations is less than the number of random effects     # levels for each term, which makes the random effects variances unidentifiable     # in this model, and ignoring this (as we do here) results in a singular fit.     check.nobs.vs.nRE = \"ignore\"   ) ) #> boundary (singular) fit: see help('isSingular')  summary(unemployment_fit_D) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: depression ~ unemployed + months:unemployed + (1 + unemployed +   #>     months:unemployed | id) #>    Data: depression_unemployment #> Control: lmerControl(check.nobs.vs.nRE = \"ignore\") #>  #>      AIC      BIC   logLik deviance df.resid  #>   5115.3   5160.4  -2547.6   5095.3      664  #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -2.1294 -0.4921 -0.1555  0.4258  3.7778  #>  #> Random effects: #>  Groups   Name              Variance Std.Dev. Corr        #>  id       (Intercept)       45.2572  6.727                #>           unemployed        44.9693  6.706     0.14       #>           unemployed:months  0.7534  0.868     0.11 -0.97 #>  Residual                   59.0189  7.682                #> Number of obs: 674, groups:  id, 254 #>  #> Fixed effects: #>                   Estimate Std. Error t value #> (Intercept)        11.1949     0.7899  14.172 #> unemployed          6.9272     0.9300   7.448 #> unemployed:months  -0.3030     0.1121  -2.703 #>  #> Correlation of Fixed Effects: #>             (Intr) unmply #> unemployed  -0.563        #> unmplyd:mnt -0.074 -0.443 #> optimizer (nloptwrap) convergence code: 0 (OK) #> boundary (singular) fit: see help('isSingular') unemployment_fit_predictions <- tibble(   unemployed = c(0, 0, 1, 1), months = c(3.5, 14, 0, 14) )  map(   list(     model_B = unemployment_fit_B,     model_C = unemployment_fit_C,     model_D = unemployment_fit_D   ),   function(.x) {     unemployment_fit_predictions |>     mutate(       depression = predict(.x, unemployment_fit_predictions, re.form = NA),       unemployed = factor(unemployed, levels = c(1, 0))     ) |>     ggplot(aes(x = months, y = depression, colour = unemployed)) +       geom_line() +       scale_x_continuous(breaks = seq(0, 14, by = 2)) +       coord_cartesian(xlim = c(0, 14), ylim = c(5, 20))   } ) #> $model_B #>  #> $model_C #>  #> $model_D"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-5.html","id":"recentering-the-effect-of-time","dir":"Articles","previous_headings":"","what":"5.4 Recentering the Effect of TIME","title":"Chapter 5: Treating time more flexibly","text":"Table 5.9, page 182: Table 5.10, page 184: Figure 5.5, page 185:","code":"antidepressants |>   select(-c(id, positive_mood, treatment)) |>   mutate(time_3.33 = time - 3.33, time_6.67 = time - 6.67) #> # A tibble: 1,242 × 7 #>     wave   day reading time_of_day  time time_3.33 time_6.67 #>    <dbl> <dbl> <chr>         <dbl> <dbl>     <dbl>     <dbl> #>  1     1     0 8 AM          0     0        -3.33      -6.67 #>  2     2     0 3 PM          0.333 0.333    -3.00      -6.34 #>  3     3     0 10 PM         0.667 0.667    -2.66      -6.00 #>  4     4     1 8 AM          0     1        -2.33      -5.67 #>  5     5     1 3 PM          0.333 1.33     -2.00      -5.34 #>  6     6     1 10 PM         0.667 1.67     -1.66      -5.00 #>  7     7     2 8 AM          0     2        -1.33      -4.67 #>  8     8     2 3 PM          0.333 2.33     -0.997     -4.34 #>  9     9     2 10 PM         0.667 2.67     -0.663     -4.00 #> 10    10     3 8 AM          0     3        -0.33      -3.67 #> # ℹ 1,232 more rows antidepressants_fit_A <- lmer(   positive_mood ~ treatment * time + (1 + time | id),   data = antidepressants,   REML = FALSE )  summary(antidepressants_fit_A) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: positive_mood ~ treatment * time + (1 + time | id) #>    Data: antidepressants #>  #>      AIC      BIC   logLik deviance df.resid  #>  12696.5  12737.4  -6340.2  12680.5     1234  #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -3.2709 -0.4899 -0.1175  0.4241  5.6180  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr  #>  id       (Intercept) 2111.54  45.951         #>           time          63.74   7.984   -0.33 #>  Residual             1229.93  35.070         #> Number of obs: 1242, groups:  id, 64 #>  #> Fixed effects: #>                Estimate Std. Error t value #> (Intercept)     167.463      9.327  17.956 #> treatment        -3.109     12.333  -0.252 #> time             -2.418      1.731  -1.397 #> treatment:time    5.537      2.278   2.431 #>  #> Correlation of Fixed Effects: #>             (Intr) trtmnt time   #> treatment   -0.756               #> time        -0.404  0.305        #> treatmnt:tm  0.307 -0.408 -0.760  antidepressants_fit_B <- lmer(   positive_mood ~ treatment * I(time - 3.33) + (1 + I(time - 3.33) | id),   data = antidepressants,   REML = FALSE,   control = lmerControl(optimizer = \"bobyqa\") )  summary(antidepressants_fit_B) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: positive_mood ~ treatment * I(time - 3.33) + (1 + I(time - 3.33) |   #>     id) #>    Data: antidepressants #> Control: lmerControl(optimizer = \"bobyqa\") #>  #>      AIC      BIC   logLik deviance df.resid  #>  12696.5  12737.4  -6340.2  12680.5     1234  #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -3.2709 -0.4899 -0.1175  0.4241  5.6180  #>  #> Random effects: #>  Groups   Name           Variance Std.Dev. Corr #>  id       (Intercept)    2008.16  44.812        #>           I(time - 3.33)   63.74   7.983   0.25 #>  Residual                1229.93  35.070        #> Number of obs: 1242, groups:  id, 64 #>  #> Fixed effects: #>                          Estimate Std. Error t value #> (Intercept)               159.411      8.763  18.191 #> treatment                  15.328     11.543   1.328 #> I(time - 3.33)             -2.418      1.731  -1.397 #> treatment:I(time - 3.33)    5.537      2.278   2.431 #>  #> Correlation of Fixed Effects: #>             (Intr) trtmnt I(-3.3 #> treatment   -0.759               #> I(tim-3.33)  0.228 -0.173        #> tr:I(-3.33) -0.173  0.221 -0.760  antidepressants_fit_C <- lmer(   positive_mood ~ treatment * I(time - 6.67) + (1 + I(time - 6.67) | id),   data = antidepressants,   REML = FALSE )  summary(antidepressants_fit_C) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: positive_mood ~ treatment * I(time - 6.67) + (1 + I(time - 6.67) |   #>     id) #>    Data: antidepressants #>  #>      AIC      BIC   logLik deviance df.resid  #>  12696.5  12737.4  -6340.2  12680.5     1234  #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -3.2709 -0.4899 -0.1175  0.4241  5.6180  #>  #> Random effects: #>  Groups   Name           Variance Std.Dev. Corr #>  id       (Intercept)    3324.50  57.658        #>           I(time - 6.67)   63.73   7.983   0.66 #>  Residual                1229.93  35.070        #> Number of obs: 1242, groups:  id, 64 #>  #> Fixed effects: #>                          Estimate Std. Error t value #> (Intercept)               151.335     11.546  13.107 #> treatment                  33.821     15.163   2.230 #> I(time - 6.67)             -2.418      1.731  -1.397 #> treatment:I(time - 6.67)    5.537      2.278   2.431 #>  #> Correlation of Fixed Effects: #>             (Intr) trtmnt I(-6.6 #> treatment   -0.761               #> I(tim-6.67)  0.674 -0.513        #> tr:I(-6.67) -0.512  0.670 -0.760 antidepressants_fit_predictions <- tibble(   treatment = c(0, 0, 1, 1), time = c(0, 7, 0, 7) )  antidepressants_fit_predictions |>     mutate(       positive_mood = predict(         antidepressants_fit_C, antidepressants_fit_predictions, re.form = NA       ),       treatment = factor(treatment, levels = c(1, 0))     ) |>     ggplot(aes(x = time, y = positive_mood, colour = treatment)) +       geom_line() +       scale_x_continuous(breaks = seq(0, 7, by = 1)) +       coord_cartesian(ylim = c(140, 190))"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-6.html","id":"discontinuous-individual-change","dir":"Articles","previous_headings":"","what":"6.1 Discontinuous Individual Change","title":"Chapter 6: Modeling Discontinuous and Nonlinear Change","text":"Table 6.1, page 192: Table 6.2, page 203: Table 6.3, page 205: Figure 6.3:","code":"dropout_wages |>   filter(id %in% c(206,2365,4384)) |>   select(id, log_wages, experience, ged, postsecondary_education) |>   mutate(ged_x_experience = ged * experience) #> # A tibble: 22 × 6 #>    id    log_wages experience   ged postsecondary_education ged_x_experience #>    <fct>     <dbl>      <dbl> <dbl>                   <dbl>            <dbl> #>  1 206        2.03       1.87     0                    0                0    #>  2 206        2.30       2.81     0                    0                0    #>  3 206        2.48       4.31     0                    0                0    #>  4 2365       1.78       0.66     0                    0                0    #>  5 2365       1.76       1.68     0                    0                0    #>  6 2365       1.71       2.74     0                    0                0    #>  7 2365       1.74       3.68     0                    0                0    #>  8 2365       2.19       4.68     1                    0                4.68 #>  9 2365       2.04       5.72     1                    1.04             5.72 #> 10 2365       2.32       6.72     1                    2.04             6.72 #> # ℹ 12 more rows wages_fit_A <- lmer(   log_wages ~     experience +     I(highest_grade - 9) +     experience:black +     I(unemployment_rate - 7) +     (1 + experience | id),   data = dropout_wages,   REML = FALSE )  deviance(wages_fit_A) #> [1] 4830.519  wages_fit_B <- update(   wages_fit_A,   . ~ . - (1 + experience | id) + ged + (1 + experience + ged | id),   control = lmerControl(optimizer = \"bobyqa\") )  deviance(wages_fit_B) #> [1] 4805.517  wages_fit_C <- update(   wages_fit_A,   . ~ . + ged,   control = lmerControl(optimizer = \"bobyqa\") )  deviance(wages_fit_C) #> [1] 4818.324  wages_fit_D <- update(   wages_fit_A,   . ~      . -      (1 + experience | id) +      postsecondary_education +      (1 + experience + postsecondary_education | id),   control = lmerControl(optimizer = \"bobyqa\") )  deviance(wages_fit_D) #> [1] 4817.377  wages_fit_E <- update(   wages_fit_A,   . ~ . + postsecondary_education )  deviance(wages_fit_E) #> [1] 4820.706  wages_fit_F <- update(   wages_fit_A,   . ~      . -     (1 + experience | id) +      ged +      postsecondary_education +      (1 + experience + ged + postsecondary_education | id),   control = lmerControl(optimizer = \"bobyqa\") )  deviance(wages_fit_F) #> [1] 4789.354  wages_fit_G <- update(   wages_fit_F,   . ~     . -      (1 + experience + ged + postsecondary_education | id) +     (1 + experience + ged | id) )  deviance(wages_fit_G) #> [1] 4802.688  wages_fit_H <- update(   wages_fit_F,   . ~     . -      (1 + experience + ged + postsecondary_education | id) +     (1 + experience + postsecondary_education | id) )  deviance(wages_fit_H) #> [1] 4812.639  # FIXME: Singular wages_fit_I <- update(   wages_fit_A,   . ~      . -     (1 + experience | id) +     ged +     experience:ged +     (1 + experience + ged + experience:ged | id) ) #> boundary (singular) fit: see help('isSingular')  deviance(wages_fit_I) #> [1] 4793.508  wages_fit_J <- update(   wages_fit_I,   . ~      . -     (1 + experience + ged + experience:ged | id) +     (1 + experience + ged | id) ) #> Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, : #> Model failed to converge with max|grad| = 0.00241207 (tol = 0.002, component 1)  deviance(wages_fit_J) #> [1] 4804.601  anova(wages_fit_A, wages_fit_B, wages_fit_D) #> Data: dropout_wages #> Models: #> wages_fit_A: log_wages ~ experience + I(highest_grade - 9) + experience:black + I(unemployment_rate - 7) + (1 + experience | id) #> wages_fit_B: log_wages ~ experience + I(highest_grade - 9) + I(unemployment_rate - 7) + ged + (1 + experience + ged | id) + experience:black #> wages_fit_D: log_wages ~ experience + I(highest_grade - 9) + I(unemployment_rate - 7) + postsecondary_education + (1 + experience + postsecondary_education | id) + experience:black #>             npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)     #> wages_fit_A    9 4848.5 4909.4 -2415.3   4830.5                          #> wages_fit_B   13 4831.5 4919.5 -2402.8   4805.5 25.002  4  5.027e-05 *** #> wages_fit_D   13 4843.4 4931.3 -2408.7   4817.4  0.000  0                #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 anova(wages_fit_B, wages_fit_C, wages_fit_F, wages_fit_I) #> Data: dropout_wages #> Models: #> wages_fit_C: log_wages ~ experience + I(highest_grade - 9) + I(unemployment_rate - 7) + (1 + experience | id) + ged + experience:black #> wages_fit_B: log_wages ~ experience + I(highest_grade - 9) + I(unemployment_rate - 7) + ged + (1 + experience + ged | id) + experience:black #> wages_fit_F: log_wages ~ experience + I(highest_grade - 9) + I(unemployment_rate - 7) + ged + postsecondary_education + (1 + experience + ged + postsecondary_education | id) + experience:black #> wages_fit_I: log_wages ~ experience + I(highest_grade - 9) + I(unemployment_rate - 7) + ged + (1 + experience + ged + experience:ged | id) + experience:black + experience:ged #>             npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    #> wages_fit_C   10 4838.3 4906.0 -2409.2   4818.3                         #> wages_fit_B   13 4831.5 4919.5 -2402.8   4805.5 12.807  3   0.005074 ** #> wages_fit_F   18 4825.4 4947.1 -2394.7   4789.4 16.163  5   0.006392 ** #> wages_fit_I   18 4829.5 4951.3 -2396.8   4793.5  0.000  0               #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 anova(wages_fit_D, wages_fit_E, wages_fit_F) #> Data: dropout_wages #> Models: #> wages_fit_E: log_wages ~ experience + I(highest_grade - 9) + I(unemployment_rate - 7) + (1 + experience | id) + postsecondary_education + experience:black #> wages_fit_D: log_wages ~ experience + I(highest_grade - 9) + I(unemployment_rate - 7) + postsecondary_education + (1 + experience + postsecondary_education | id) + experience:black #> wages_fit_F: log_wages ~ experience + I(highest_grade - 9) + I(unemployment_rate - 7) + ged + postsecondary_education + (1 + experience + ged + postsecondary_education | id) + experience:black #>             npar    AIC    BIC  logLik deviance   Chisq Df Pr(>Chisq)     #> wages_fit_E   10 4840.7 4908.4 -2410.3   4820.7                           #> wages_fit_D   13 4843.4 4931.3 -2408.7   4817.4  3.3292  3     0.3436     #> wages_fit_F   18 4825.4 4947.1 -2394.7   4789.4 28.0233  5  3.602e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 anova(wages_fit_F, wages_fit_G, wages_fit_H) #> Data: dropout_wages #> Models: #> wages_fit_G: log_wages ~ experience + I(highest_grade - 9) + I(unemployment_rate - 7) + ged + postsecondary_education + (1 + experience + ged | id) + experience:black #> wages_fit_H: log_wages ~ experience + I(highest_grade - 9) + I(unemployment_rate - 7) + ged + postsecondary_education + (1 + experience + postsecondary_education | id) + experience:black #> wages_fit_F: log_wages ~ experience + I(highest_grade - 9) + I(unemployment_rate - 7) + ged + postsecondary_education + (1 + experience + ged + postsecondary_education | id) + experience:black #>             npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)     #> wages_fit_G   14 4830.7 4925.4 -2401.3   4802.7                          #> wages_fit_H   14 4840.6 4935.3 -2406.3   4812.6  0.000  0                #> wages_fit_F   18 4825.4 4947.1 -2394.7   4789.4 23.285  4   0.000111 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 anova(wages_fit_I, wages_fit_J) #> Data: dropout_wages #> Models: #> wages_fit_J: log_wages ~ experience + I(highest_grade - 9) + I(unemployment_rate - 7) + ged + (1 + experience + ged | id) + experience:black + experience:ged #> wages_fit_I: log_wages ~ experience + I(highest_grade - 9) + I(unemployment_rate - 7) + ged + (1 + experience + ged + experience:ged | id) + experience:black + experience:ged #>             npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)   #> wages_fit_J   14 4832.6 4927.3 -2402.3   4804.6                        #> wages_fit_I   18 4829.5 4951.3 -2396.8   4793.5 11.094  4    0.02553 * #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 summary(wages_fit_F) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: log_wages ~ experience + I(highest_grade - 9) + I(unemployment_rate -   #>     7) + ged + postsecondary_education + (1 + experience + ged +   #>     postsecondary_education | id) + experience:black #>    Data: dropout_wages #> Control: lmerControl(optimizer = \"bobyqa\") #>  #>      AIC      BIC   logLik deviance df.resid  #>   4825.4   4947.1  -2394.7   4789.4     6384  #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -4.3462 -0.5133 -0.0387  0.4457  6.9719  #>  #> Random effects: #>  Groups   Name                    Variance Std.Dev. Corr              #>  id       (Intercept)             0.041323 0.20328                    #>           experience              0.001360 0.03688  -0.23             #>           ged                     0.016312 0.12772   0.46  0.62       #>           postsecondary_education 0.003355 0.05792  -0.51 -0.43 -0.53 #>  Residual                         0.093873 0.30639                    #> Number of obs: 6402, groups:  id, 888 #>  #> Fixed effects: #>                           Estimate Std. Error t value #> (Intercept)               1.738574   0.011942 145.587 #> experience                0.041471   0.002797  14.827 #> I(highest_grade - 9)      0.039029   0.006243   6.252 #> I(unemployment_rate - 7) -0.011724   0.001783  -6.576 #> ged                       0.040875   0.021989   1.859 #> postsecondary_education   0.009423   0.005545   1.699 #> experience:black         -0.019620   0.004470  -4.389 #>  #> Correlation of Fixed Effects: #>             (Intr) exprnc I(_-9) I(_-7) ged    pstsc_ #> experience  -0.531                                    #> I(hghst_-9)  0.093 -0.023                             #> I(nmply_-7) -0.368  0.255 -0.045                      #> ged         -0.323  0.122 -0.020  0.055               #> pstscndry_d  0.159 -0.368 -0.021 -0.012 -0.558        #> exprnc:blck -0.059 -0.297 -0.018  0.067  0.007 -0.056 prototypical_dropouts <- tibble(   experience = rep(c(0, 3, 3, 12), times = 4),   highest_grade = rep(c(9, 12), each = 4, times = 2),   black = rep(c(FALSE, TRUE), each = 8),   ged = rep(c(0, 0, 1, 1), times = 4),   unemployment_rate = 7 ) |>   mutate(postsecondary_education = ifelse(ged == 0, 0, experience - 3))  prototypical_dropout_scores <- tibble(   log_wages = predict(     wages_fit_F,     prototypical_dropouts,     re.form = NA   ) )  prototypical_dropouts |>   bind_cols(prototypical_dropout_scores) |>   ggplot(aes(x = experience, y = log_wages, colour = as.logical(black))) +     geom_line() +     scale_x_continuous(breaks = seq(0, 10, by = 2)) +     coord_cartesian(xlim = c(0, 10), ylim = c(1.6, 2.4)) +     facet_wrap(vars(highest_grade))"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-6.html","id":"using-transformations-to-model-nonlinear-individual-change","dir":"Articles","previous_headings":"","what":"6.2 Using Transformations to Model Nonlinear Individual Change","title":"Chapter 6: Modeling Discontinuous and Nonlinear Change","text":"Figure 6.4, page 209:  Figure 6.6, page 212:","code":"alcohol_fit <- lmer(   alcohol_use ~      I(age - 14) * peer_alcohol_use + child_of_alcoholic + (1 + I(age - 14) | id),   data = alcohol_use_1,   REML = FALSE )  prototypical_alcohol_users <- tibble(   age = rep(seq(14, 16, by = .25), times = 4),   child_of_alcoholic = rep(c(0, 1), each = 9, times = 2),   peer_alcohol_use = rep(c(.655, 1.381), each = 18) )  prototypical_alcohol_use <- tibble(   alcohol_use = predict(     alcohol_fit,     prototypical_alcohol_users,     re.form = NA   ) )  prototypical_alcohol_users |>   bind_cols(prototypical_alcohol_use) |>   mutate(alcohol_use = alcohol_use^2) |>   ggplot(aes(x = age, y = alcohol_use, colour = factor(peer_alcohol_use))) +     geom_line() +     facet_wrap(vars(child_of_alcoholic)) +     coord_cartesian(xlim = c(13, 17), ylim = c(0, 3)) ggplot(berkeley, aes(x = age, y = iq)) +   geom_point(data = \\(.x) mutate(.x, group = \"1: untransformed\")) +   geom_point(data = \\(.x) mutate(.x, iq = iq^(2.3), group = \"2: iq^(2.3)\")) +   geom_point(data = \\(.x) mutate(.x, age = age^(1/2.3), group = \"3: age^(1/2.3)\")) +   facet_wrap(vars(group), scales = \"free\")"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-6.html","id":"representing-individual-change-using-a-polynomial-function-of-time","dir":"Articles","previous_headings":"","what":"6.3 Representing Individual Change Using a Polynomial Function of TIME","title":"Chapter 6: Modeling Discontinuous and Nonlinear Change","text":"Figure 6.7, page 218:  Table 6.5, page 221:","code":"# Polynomial formulas for plotting: linear <- formula(y ~ x) quadratic <- formula(y ~ x + I(x^2)) cubic <- formula(y ~ x + I(x^2) + I(x^3)) quartic <- formula(y ~ x + I(x^2) + I(x^3) + I(x^4))  # We need to pmap() over geoms to get facets with unique subsets of data and # smooths since there's no aesthetic mappings for geom_smooth() formulas. participant_params <- list(   list(1, 6, 11, 25, 34, 36, 40, 26),   LETTERS[1:8],   list(quadratic, quadratic, linear, linear, cubic, quartic, cubic, quartic) )  ggplot(mapping = aes(x = grade, y = externalizing_behaviour)) +   {     pmap(       participant_params,       \\(.x, .y, .z) {         # Subset data to a given id and add a case variable for faceting.         participant_data <- externalizing_behaviour |>           filter(id == .x) |>           mutate(participant = .y)                  # Rather than adding the geoms together with + they need to be returned         # as a list of geoms to be added to the plot from within pmap().         list(           geom_point(data = participant_data),           geom_smooth(             data = participant_data,             method = \"lm\", formula = quartic, se = FALSE,             linewidth = .25, linetype = 2           ),           geom_smooth(             data = participant_data,             method = \"lm\", formula = .z, se = FALSE, linewidth = .5           )         )       }     )   } +   scale_x_continuous(breaks = 0:7) +   coord_cartesian(xlim = c(0, 7)) +   facet_wrap(vars(participant), nrow = 2) behaviour_fit_A <- lmer(   externalizing_behaviour ~ 1 + (1 | id),   data = externalizing_behaviour,   REML = FALSE )  summary(behaviour_fit_A) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: externalizing_behaviour ~ 1 + (1 | id) #>    Data: externalizing_behaviour #>  #>      AIC      BIC   logLik deviance df.resid  #>   2016.3   2027.0  -1005.1   2010.3      267  #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -2.6629 -0.5255 -0.1648  0.4803  3.5393  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. #>  id       (Intercept) 87.42    9.350    #>  Residual             70.20    8.379    #> Number of obs: 270, groups:  id, 45 #>  #> Fixed effects: #>             Estimate Std. Error t value #> (Intercept)   12.963      1.484   8.734  behaviour_fit_B <- lmer(   externalizing_behaviour ~ time + (1 + time | id),   data = externalizing_behaviour,   REML = FALSE )  summary(behaviour_fit_B) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: externalizing_behaviour ~ time + (1 + time | id) #>    Data: externalizing_behaviour #>  #>      AIC      BIC   logLik deviance df.resid  #>   2003.7   2025.3   -995.9   1991.7      264  #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -2.9552 -0.4928 -0.1353  0.4207  2.8079  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr  #>  id       (Intercept) 123.526  11.114         #>           time          4.693   2.166   -0.52 #>  Residual              53.718   7.329         #> Number of obs: 270, groups:  id, 45 #>  #> Fixed effects: #>             Estimate Std. Error t value #> (Intercept)  13.2899     1.8358   7.239 #> time         -0.1308     0.4153  -0.315 #>  #> Correlation of Fixed Effects: #>      (Intr) #> time -0.589  behaviour_fit_C <- lmer(   externalizing_behaviour ~     poly(time, 2, raw = TRUE) + (1 + poly(time, 2, raw = TRUE) | id),   data = externalizing_behaviour,   REML = FALSE )  summary(behaviour_fit_C) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: externalizing_behaviour ~ poly(time, 2, raw = TRUE) + (1 + poly(time,   #>     2, raw = TRUE) | id) #>    Data: externalizing_behaviour #>  #>      AIC      BIC   logLik deviance df.resid  #>   1995.8   2031.8   -987.9   1975.8      260  #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -2.2153 -0.4833 -0.1148  0.3781  2.6531  #>  #> Random effects: #>  Groups   Name                       Variance Std.Dev. Corr        #>  id       (Intercept)                107.077  10.348               #>           poly(time, 2, raw = TRUE)1  24.606   4.960   -0.07       #>           poly(time, 2, raw = TRUE)2   1.215   1.102   -0.12 -0.91 #>  Residual                             41.985   6.480               #> Number of obs: 270, groups:  id, 45 #>  #> Fixed effects: #>                            Estimate Std. Error t value #> (Intercept)                  13.970      1.774   7.876 #> poly(time, 2, raw = TRUE)1   -1.151      1.107  -1.040 #> poly(time, 2, raw = TRUE)2    0.204      0.228   0.894 #>  #> Correlation of Fixed Effects: #>               (Intr) p(,2,r=TRUE)1 #> p(,2,r=TRUE)1 -0.322               #> p(,2,r=TRUE)2  0.131 -0.932  # Singular behaviour_fit_D <- lmer(   externalizing_behaviour ~     poly(time, 3, raw = TRUE) + (1 + poly(time, 3, raw = TRUE) | id),   data = externalizing_behaviour,   REML = FALSE ) #> boundary (singular) fit: see help('isSingular')  summary(behaviour_fit_D) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: externalizing_behaviour ~ poly(time, 3, raw = TRUE) + (1 + poly(time,   #>     3, raw = TRUE) | id) #>    Data: externalizing_behaviour #>  #>      AIC      BIC   logLik deviance df.resid  #>   1997.4   2051.3   -983.7   1967.4      255  #>  #> Scaled residuals:  #>      Min       1Q   Median       3Q      Max  #> -2.37389 -0.44738 -0.09901  0.36132  2.52632  #>  #> Random effects: #>  Groups   Name                       Variance Std.Dev. Corr              #>  id       (Intercept)                128.9137 11.3540                    #>           poly(time, 3, raw = TRUE)1 105.2094 10.2572  -0.48             #>           poly(time, 3, raw = TRUE)2  16.3605  4.0448   0.53 -0.97       #>           poly(time, 3, raw = TRUE)3   0.1736  0.4167  -0.68  0.94 -0.98 #>  Residual                             37.8546  6.1526                    #> Number of obs: 270, groups:  id, 45 #>  #> Fixed effects: #>                            Estimate Std. Error t value #> (Intercept)                13.79453    1.91640   7.198 #> poly(time, 3, raw = TRUE)1 -0.35006    2.32076  -0.151 #> poly(time, 3, raw = TRUE)2 -0.23430    1.05656  -0.222 #> poly(time, 3, raw = TRUE)3  0.05844    0.12977   0.450 #>  #> Correlation of Fixed Effects: #>               (Intr) p(,3,r=TRUE)1 p(,3,r=TRUE)2 #> p(,3,r=TRUE)1 -0.510                             #> p(,3,r=TRUE)2  0.449 -0.956                      #> p(,3,r=TRUE)3 -0.445  0.886        -0.978        #> optimizer (nloptwrap) convergence code: 0 (OK) #> boundary (singular) fit: see help('isSingular')"},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-6.html","id":"truly-nonlinear-trajectories","dir":"Articles","previous_headings":"","what":"6.4 Truly Nonlinear Trajectories","title":"Chapter 6: Modeling Discontinuous and Nonlinear Change","text":"Figure 6.8, page 227:  Help wanted Table 6.6, page 231, see: https://github.com/mccarthy-m-g/alda/issues/3 Figure 6.10, page 232:","code":"cognitive_growth |>   filter(id %in% c(1, 4, 6, 7, 8, 11, 12, 15)) |>   ggplot(aes(x = game, y = nmoves)) +   geom_point() +   coord_cartesian(xlim = c(0, 30), ylim = c(0, 25)) +   facet_wrap(vars(id), ncol = 4, labeller = label_both) # TODO after resolving the equations and code for Models A and B."},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-7.html","id":"the-standard-specification-of-the-multilevel-model-for-change","dir":"Articles","previous_headings":"","what":"7.1 The “Standard” Specification of the Multilevel Model for Change","title":"Chapter-7: Examining the Multilevel Model’s Error Covariance Structure","text":"Table 7.1: Table 7.2, page 246:","code":"opposites_naming |>   select(-time) |>   pivot_wider(     names_from = wave,     values_from = opposites_naming_score,     names_prefix = \"opp_\"   ) |>   relocate(baseline_cognitive_score, .after = everything()) |>   head(10) #> # A tibble: 10 × 6 #>    id    opp_1 opp_2 opp_3 opp_4 baseline_cognitive_score #>    <fct> <dbl> <dbl> <dbl> <dbl>                    <dbl> #>  1 1       205   217   268   302                      137 #>  2 2       219   243   279   302                      123 #>  3 3       142   212   250   289                      129 #>  4 4       206   230   248   273                      125 #>  5 5       190   220   229   220                       81 #>  6 6       165   205   207   263                      110 #>  7 7       170   182   214   268                       99 #>  8 8        96   131   159   213                      113 #>  9 9       138   156   197   200                      104 #> 10 10      216   252   274   298                       96 mucog <- mean(opposites_naming$baseline_cognitive_score)  opposites_fit_A <- glmmTMB(   opposites_naming_score ~     time * I(baseline_cognitive_score - mucog) + (time | id),   data = opposites_naming,   REML = TRUE )  summary(opposites_fit_A) #>  Family: gaussian  ( identity ) #> Formula:          opposites_naming_score ~ time * I(baseline_cognitive_score -   #>     mucog) + (time | id) #> Data: opposites_naming #>  #>      AIC      BIC   logLik deviance df.resid  #>   1303.0   1326.6   -643.5   1287.0      136  #>  #> Random effects: #>  #> Conditional model: #>  Groups   Name        Variance  Std.Dev.  Corr  #>  id       (Intercept) 9.001e+02 3.000e+01       #>           time        5.781e-14 2.404e-07 0.99  #>  Residual             3.313e+02 1.820e+01       #> Number of obs: 140, groups:  id, 35 #>  #> Dispersion estimate for gaussian family (sigma^2):  331  #>  #> Conditional model: #>                                          Estimate Std. Error z value Pr(>|z|) #> (Intercept)                              164.3743     5.6870  28.903  < 2e-16 #> time                                      26.9600     1.3759  19.595  < 2e-16 #> I(baseline_cognitive_score - mucog)       -0.1136     0.4619  -0.246 0.805789 #> time:I(baseline_cognitive_score - mucog)   0.4329     0.1117   3.874 0.000107 #>                                              #> (Intercept)                              *** #> time                                     *** #> I(baseline_cognitive_score - mucog)          #> time:I(baseline_cognitive_score - mucog) *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":[]},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-7.html","id":"postulating-an-alternative-error-covariance-structure","dir":"Articles","previous_headings":"","what":"7.3 Postulating an Alternative Error Covariance Structure","title":"Chapter-7: Examining the Multilevel Model’s Error Covariance Structure","text":"Table 7.3, page 258-259: Table 7.4, page 265:","code":"summary(opposites_fit_A) #>  Family: gaussian  ( identity ) #> Formula:          opposites_naming_score ~ time * I(baseline_cognitive_score -   #>     mucog) + (time | id) #> Data: opposites_naming #>  #>      AIC      BIC   logLik deviance df.resid  #>   1303.0   1326.6   -643.5   1287.0      136  #>  #> Random effects: #>  #> Conditional model: #>  Groups   Name        Variance  Std.Dev.  Corr  #>  id       (Intercept) 9.001e+02 3.000e+01       #>           time        5.781e-14 2.404e-07 0.99  #>  Residual             3.313e+02 1.820e+01       #> Number of obs: 140, groups:  id, 35 #>  #> Dispersion estimate for gaussian family (sigma^2):  331  #>  #> Conditional model: #>                                          Estimate Std. Error z value Pr(>|z|) #> (Intercept)                              164.3743     5.6870  28.903  < 2e-16 #> time                                      26.9600     1.3759  19.595  < 2e-16 #> I(baseline_cognitive_score - mucog)       -0.1136     0.4619  -0.246 0.805789 #> time:I(baseline_cognitive_score - mucog)   0.4329     0.1117   3.874 0.000107 #>                                              #> (Intercept)                              *** #> time                                     *** #> I(baseline_cognitive_score - mucog)          #> time:I(baseline_cognitive_score - mucog) *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  # Toeplitz opposites_fit_B <- update(   opposites_fit_A,   . ~ . - (time | id) + toep(time + 0 | id),   dispformula = ~ 0 )  summary(opposites_fit_B) #>  Family: gaussian  ( identity ) #> Formula:          opposites_naming_score ~ time + I(baseline_cognitive_score -   #>     mucog) + (toep(time + 0 | id)) + time:I(baseline_cognitive_score -   #>     mucog) #> Dispersion:                              ~0 #> Data: opposites_naming #>  #>           AIC           BIC        logLik      deviance      df.resid  #>  5.013983e+12  5.013983e+12 -2.506992e+12  5.013983e+12           139  #>  #> Random effects: #>  #> Conditional model: #>  Groups Name Variance Std.Dev. Corr #>  id     time 9.985    3.16          #> Number of obs: 140, groups:  id, 35 #>  #> Conditional model: #>                                            Estimate Std. Error z value Pr(>|z|) #> (Intercept)                               1.644e+02  1.726e-05 9521576   <2e-16 #> time                                      2.696e+01  5.341e-01      50   <2e-16 #> I(baseline_cognitive_score - mucog)      -1.136e-01  1.402e-06  -80993   <2e-16 #> time:I(baseline_cognitive_score - mucog)  4.329e-01  4.338e-02      10   <2e-16 #>                                              #> (Intercept)                              *** #> time                                     *** #> I(baseline_cognitive_score - mucog)      *** #> time:I(baseline_cognitive_score - mucog) *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  # Unstructured opposites_fit_C <- update(   opposites_fit_A,   . ~ . - (time | id) + us(time + 0 | id),   dispformula = ~ 0 )  summary(opposites_fit_C) #>  Family: gaussian  ( identity ) #> Formula:          opposites_naming_score ~ time + I(baseline_cognitive_score -   #>     mucog) + (us(time + 0 | id)) + time:I(baseline_cognitive_score -   #>     mucog) #> Dispersion:                              ~0 #> Data: opposites_naming #>  #>           AIC           BIC        logLik      deviance      df.resid  #>  5.013983e+12  5.013983e+12 -2.506992e+12  5.013983e+12           139  #>  #> Random effects: #>  #> Conditional model: #>  Groups Name Variance Std.Dev. #>  id     time 9.985    3.16     #> Number of obs: 140, groups:  id, 35 #>  #> Conditional model: #>                                            Estimate Std. Error z value Pr(>|z|) #> (Intercept)                               1.644e+02  1.726e-05 9521576   <2e-16 #> time                                      2.696e+01  5.341e-01      50   <2e-16 #> I(baseline_cognitive_score - mucog)      -1.136e-01  1.402e-06  -80993   <2e-16 #> time:I(baseline_cognitive_score - mucog)  4.329e-01  4.338e-02      10   <2e-16 #>                                              #> (Intercept)                              *** #> time                                     *** #> I(baseline_cognitive_score - mucog)      *** #> time:I(baseline_cognitive_score - mucog) *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":[]},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-8.html","id":"the-basics-of-latent-growth-modeling","dir":"Articles","previous_headings":"","what":"8.2 The Basics of Latent Growth Modeling","title":"Chapter 8: Modeling change using covariance structure analysis","text":"Table 8.1, page 282: Table 8.2, page 289, Model : Figure 8.2, Model :  Table 8.2, page 289, Model B: Comparison baseline model Model B: Figure 8.2, Model B:  Table 8.2, page 289, Model C: Figure 8.2, Model C (Model B slope ~ 0*female):  Table 8.2, page 289, Model D: Comparison baseline model Model D: Figure 8.2, Model D:","code":"alcohol_use_2_wide <- alcohol_use_2 |>   pivot_wider(names_from = time, values_from = c(alcohol_use, peer_pressure))  alcohol_use_2_wide #> # A tibble: 1,122 × 8 #>    id    female alcohol_use_0 alcohol_use_1 alcohol_use_2 peer_pressure_0 #>    <fct>  <dbl>         <dbl>         <dbl>         <dbl>           <dbl> #>  1 1          0         0.693         0.288         0.511            0    #>  2 2          0         0             0             0                0    #>  3 3          0         0             0             0                0    #>  4 4          0         0             0.511         0.511            1.10 #>  5 5          0         0.288         0             0.847            0    #>  6 6          0         0             0             0                0    #>  7 7          0         0.288         0.288         0                0    #>  8 8          0         0             0             0                0    #>  9 9          0         0             0.511         0                0    #> 10 10         0         0.511         0.693         1.30             0    #> # ℹ 1,112 more rows #> # ℹ 2 more variables: peer_pressure_1 <dbl>, peer_pressure_2 <dbl>  # Means alcohol_use_2_wide |>   summarise(across(female:peer_pressure_2, mean)) |>   glimpse() #> Rows: 1 #> Columns: 7 #> $ female          <dbl> 0.6122995 #> $ alcohol_use_0   <dbl> 0.2250666 #> $ alcohol_use_1   <dbl> 0.2541351 #> $ alcohol_use_2   <dbl> 0.287923 #> $ peer_pressure_0 <dbl> 0.1771944 #> $ peer_pressure_1 <dbl> 0.2904569 #> $ peer_pressure_2 <dbl> 0.3470381  # Covariances cov(select(alcohol_use_2_wide, -c(id, female))) #>                 alcohol_use_0 alcohol_use_1 alcohol_use_2 peer_pressure_0 #> alcohol_use_0      0.13558718    0.07775260    0.06526470      0.06586967 #> alcohol_use_1      0.07775260    0.15528121    0.08186386      0.04479710 #> alcohol_use_2      0.06526470    0.08186386    0.18075945      0.03988182 #> peer_pressure_0    0.06586967    0.04479710    0.03988182      0.17399159 #> peer_pressure_1    0.06404875    0.09647876    0.06580980      0.07158186 #> peer_pressure_2    0.06008199    0.07433086    0.13197010      0.07071309 #>                 peer_pressure_1 peer_pressure_2 #> alcohol_use_0        0.06404875      0.06008199 #> alcohol_use_1        0.09647876      0.07433086 #> alcohol_use_2        0.06580980      0.13197010 #> peer_pressure_0      0.07158186      0.07071309 #> peer_pressure_1      0.26190160      0.11180554 #> peer_pressure_2      0.11180554      0.28901177 # Model A: Unconditional model model_A <- (\"   # Intercept and slope with fixed coefficients   intercept =~ 1*alcohol_use_0 + 1*alcohol_use_1 + 1*alcohol_use_2   slope     =~ 0*alcohol_use_0 + .75*alcohol_use_1 + 1.75*alcohol_use_2 \")  model_A_fit <- growth(   model_A, data = alcohol_use_2_wide, estimator = \"ml\", mimic = \"Mplus\" )  summary(model_A_fit) #> lavaan 0.6.17 ended normally after 32 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                         8 #>  #>   Number of observations                          1122 #>   Number of missing patterns                         1 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.048 #>   Degrees of freedom                                 1 #>   P-value (Chi-square)                           0.826 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Observed #>   Observed information based on                Hessian #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   intercept =~                                         #>     alcohol_use_0     1.000                            #>     alcohol_use_1     1.000                            #>     alcohol_use_2     1.000                            #>   slope =~                                             #>     alcohol_use_0     0.000                            #>     alcohol_use_1     0.750                            #>     alcohol_use_2     1.750                            #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   intercept ~~                                         #>     slope            -0.012    0.005   -2.727    0.006 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|) #>     intercept         0.226    0.011   21.106    0.000 #>     slope             0.036    0.007    4.898    0.000 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|) #>    .alcohol_use_0     0.048    0.006    7.550    0.000 #>    .alcohol_use_1     0.076    0.004   17.051    0.000 #>    .alcohol_use_2     0.077    0.010    7.756    0.000 #>     intercept         0.087    0.007   12.253    0.000 #>     slope             0.020    0.005    3.795    0.000 fitMeasures(model_A_fit, c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\")) #>  chisq     df pvalue    cfi  rmsea  #>  0.048  1.000  0.826  1.000  0.000 lay <- get_layout(   NA, \"intercept\", NA,  \"slope\", NA,   \"alcohol_use_0\", NA,  \"alcohol_use_1\", NA, \"alcohol_use_2\",   rows = 2 )  graph_sem(model_A_fit, layout = lay) # Model B: Adding female as a time-invariant predictor model_B <- (\"   # Intercept and slope with fixed coefficients   intercept =~ 1*alcohol_use_0 + 1*alcohol_use_1 + 1*alcohol_use_2   slope =~ 0*alcohol_use_0 + .75*alcohol_use_1 + 1.75*alcohol_use_2      # Regressions   intercept ~ female   slope ~ female \")  model_B_fit <- growth(   model_B, data = alcohol_use_2_wide, estimator = \"ml\", mimic = \"Mplus\" )  summary(model_B_fit) #> lavaan 0.6.17 ended normally after 33 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        10 #>  #>   Number of observations                          1122 #>   Number of missing patterns                         1 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 1.545 #>   Degrees of freedom                                 2 #>   P-value (Chi-square)                           0.462 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Observed #>   Observed information based on                Hessian #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   intercept =~                                         #>     alcohol_use_0     1.000                            #>     alcohol_use_1     1.000                            #>     alcohol_use_2     1.000                            #>   slope =~                                             #>     alcohol_use_0     0.000                            #>     alcohol_use_1     0.750                            #>     alcohol_use_2     1.750                            #>  #> Regressions: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   intercept ~                                          #>     female           -0.042    0.022   -1.912    0.056 #>   slope ~                                              #>     female            0.008    0.015    0.522    0.602 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|) #>  .intercept ~~                                         #>    .slope            -0.012    0.005   -2.661    0.008 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|) #>    .intercept         0.251    0.017   14.653    0.000 #>    .slope             0.031    0.012    2.640    0.008 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|) #>    .alcohol_use_0     0.049    0.006    7.616    0.000 #>    .alcohol_use_1     0.075    0.004   17.036    0.000 #>    .alcohol_use_2     0.077    0.010    7.789    0.000 #>    .intercept         0.086    0.007   12.191    0.000 #>    .slope             0.019    0.005    3.740    0.000 fitMeasures(model_B_fit, c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\")) #>  chisq     df pvalue    cfi  rmsea  #>  1.545  2.000  0.462  1.000  0.000 # Baseline for Model B (not shown in table) model_B_baseline <- (\"   # Intercept and slope with fixed coefficients   intercept =~ 1*alcohol_use_0 + 1*alcohol_use_1 + 1*alcohol_use_2   slope =~ 0*alcohol_use_0 + .75*alcohol_use_1 + 1.75*alcohol_use_2      # Regressions   intercept ~ 0*female   slope ~ 0*female      alcohol_use_0 ~ 0*1   alcohol_use_1 ~ 0*1   alcohol_use_2 ~ 0*1 \")  model_B_baseline_fit <- growth(   model_B_baseline, data = alcohol_use_2_wide, estimator = \"ml\", mimic = \"Mplus\" )  anova(model_B_baseline_fit, model_B_fit) #>  #> Chi-Squared Difference Test #>  #>                      Df    AIC    BIC  Chisq Chisq diff    RMSEA Df diff #> model_B_fit           2 2577.9 2628.1 1.5447                             #> model_B_baseline_fit  4 2577.7 2617.9 5.3665     3.8218 0.028493       2 #>                      Pr(>Chisq) #> model_B_fit                     #> model_B_baseline_fit     0.1479 lay <- get_layout(   NA, NA, \"female\", NA, NA,   NA, \"intercept\", NA,  \"slope\", NA,   \"alcohol_use_0\", NA,  \"alcohol_use_1\", NA, \"alcohol_use_2\",   rows = 3 )  graph_sem(model_B_fit, layout = lay) # Model C: Model B but with slope fixed to zero model_C <- (\"   # Intercept and slope with fixed coefficients   intercept =~ 1*alcohol_use_0 + 1*alcohol_use_1 + 1*alcohol_use_2   slope =~ 0*alcohol_use_0 + .75*alcohol_use_1 + 1.75*alcohol_use_2      # Regressions   intercept ~ female   slope ~ 0*female \")  model_C_fit <- growth(   model_C, data = alcohol_use_2_wide, estimator = \"ml\", mimic = \"Mplus\" )  summary(model_C_fit) #> lavaan 0.6.17 ended normally after 32 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                         9 #>  #>   Number of observations                          1122 #>   Number of missing patterns                         1 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 1.817 #>   Degrees of freedom                                 3 #>   P-value (Chi-square)                           0.611 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Observed #>   Observed information based on                Hessian #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   intercept =~                                         #>     alcohol_use_0     1.000                            #>     alcohol_use_1     1.000                            #>     alcohol_use_2     1.000                            #>   slope =~                                             #>     alcohol_use_0     0.000                            #>     alcohol_use_1     0.750                            #>     alcohol_use_2     1.750                            #>  #> Regressions: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   intercept ~                                          #>     female           -0.037    0.019   -1.885    0.059 #>   slope ~                                              #>     female            0.000                            #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|) #>  .intercept ~~                                         #>    .slope            -0.012    0.005   -2.667    0.008 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|) #>    .intercept         0.248    0.016   15.525    0.000 #>    .slope             0.036    0.007    4.898    0.000 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|) #>    .alcohol_use_0     0.049    0.006    7.609    0.000 #>    .alcohol_use_1     0.075    0.004   17.036    0.000 #>    .alcohol_use_2     0.077    0.010    7.801    0.000 #>    .intercept         0.086    0.007   12.194    0.000 #>    .slope             0.019    0.005    3.739    0.000 fitMeasures(model_C_fit, c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\")) #>  chisq     df pvalue    cfi  rmsea  #>  1.817  3.000  0.611  1.000  0.000 graph_sem(model_C_fit, layout = lay) # Model D: Adding peer_pressure as a time-varying predictor model_D <- (\"   # Intercept and slope with fixed coefficients   alc_intercept =~ 1*alcohol_use_0 + 1*alcohol_use_1 + 1*alcohol_use_2   alc_slope =~ 0*alcohol_use_0 + .75*alcohol_use_1 + 1.75*alcohol_use_2      peer_intercept =~ 1*peer_pressure_0 + 1*peer_pressure_1 + 1*peer_pressure_2   peer_slope =~ 0*peer_pressure_0 + .75*peer_pressure_1 + 1.75*peer_pressure_2      # Regressions   alc_intercept ~ start(.8)*peer_intercept + start(.08)*peer_slope   alc_slope ~ start(-.1)*peer_intercept + start(.6)*peer_slope      # Time-varying covariances   alcohol_use_0 ~~ peer_pressure_0   alcohol_use_1 ~~ peer_pressure_1   alcohol_use_2 ~~ peer_pressure_2      # Fix intercepts to zero   alcohol_use_0 ~ 0*1   alcohol_use_1 ~ 0*1   alcohol_use_2 ~ 0*1   peer_pressure_0 ~ 0*1   peer_pressure_1 ~ 0*1   peer_pressure_2 ~ 0*1  \")  model_D_fit <- growth(   model_D, data = alcohol_use_2_wide, estimator = \"ml\", mimic = \"Mplus\" )  summary(model_D_fit) #> lavaan 0.6.17 ended normally after 72 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        23 #>  #>   Number of observations                          1122 #>   Number of missing patterns                         1 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                11.557 #>   Degrees of freedom                                 4 #>   P-value (Chi-square)                           0.021 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Observed #>   Observed information based on                Hessian #>  #> Latent Variables: #>                     Estimate  Std.Err  z-value  P(>|z|) #>   alc_intercept =~                                      #>     alcohol_use_0      1.000                            #>     alcohol_use_1      1.000                            #>     alcohol_use_2      1.000                            #>   alc_slope =~                                          #>     alcohol_use_0      0.000                            #>     alcohol_use_1      0.750                            #>     alcohol_use_2      1.750                            #>   peer_intercept =~                                     #>     peer_pressur_0     1.000                            #>     peer_pressur_1     1.000                            #>     peer_pressur_2     1.000                            #>   peer_slope =~                                         #>     peer_pressur_0     0.000                            #>     peer_pressur_1     0.750                            #>     peer_pressur_2     1.750                            #>  #> Regressions: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   alc_intercept ~                                      #>     peer_intercept    0.799    0.103    7.781    0.000 #>     peer_slope        0.080    0.184    0.438    0.661 #>   alc_slope ~                                          #>     peer_intercept   -0.143    0.076   -1.884    0.060 #>     peer_slope        0.577    0.193    2.990    0.003 #>  #> Covariances: #>                     Estimate  Std.Err  z-value  P(>|z|) #>  .alcohol_use_0 ~~                                      #>    .peer_pressur_0     0.011    0.006    1.773    0.076 #>  .alcohol_use_1 ~~                                      #>    .peer_pressur_1     0.034    0.005    7.324    0.000 #>  .alcohol_use_2 ~~                                      #>    .peer_pressur_2     0.037    0.010    3.663    0.000 #>   peer_intercept ~~                                     #>     peer_slope         0.001    0.007    0.166    0.868 #>  .alc_intercept ~~                                      #>    .alc_slope         -0.006    0.005   -1.249    0.212 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|) #>    .alcohol_use_0     0.000                            #>    .alcohol_use_1     0.000                            #>    .alcohol_use_2     0.000                            #>    .peer_pressur_0    0.000                            #>    .peer_pressur_1    0.000                            #>    .peer_pressur_2    0.000                            #>    .alc_intercept     0.067    0.016    4.252    0.000 #>    .alc_slope         0.008    0.015    0.564    0.573 #>     peer_intercept    0.188    0.012   15.743    0.000 #>     peer_slope        0.096    0.010    9.922    0.000 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|) #>    .alcohol_use_0     0.048    0.006    7.553    0.000 #>    .alcohol_use_1     0.076    0.004   17.165    0.000 #>    .alcohol_use_2     0.076    0.010    7.819    0.000 #>    .peer_pressur_0    0.106    0.011    9.790    0.000 #>    .peer_pressur_1    0.171    0.009   19.713    0.000 #>    .peer_pressur_2    0.129    0.018    7.325    0.000 #>    .alc_intercept     0.042    0.007    5.649    0.000 #>    .alc_slope         0.009    0.005    1.697    0.090 #>     peer_intercept    0.070    0.010    6.729    0.000 #>     peer_slope        0.028    0.009    3.214    0.001 fitMeasures(model_D_fit, c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\")) #>  chisq     df pvalue    cfi  rmsea  #> 11.557  4.000  0.021  0.996  0.041 # Baseline for Model D (not shown in table) model_D_baseline <- (\"   # Intercepts and slopes with fixed coefficients   alc_intercept =~ 1*alcohol_use_0 + 1*alcohol_use_1 + 1*alcohol_use_2   alc_slope =~ 0*alcohol_use_0 + .75*alcohol_use_1 + 1.75*alcohol_use_2      peer_intercept =~ 1*peer_pressure_0 + 1*peer_pressure_1 + 1*peer_pressure_2   peer_slope =~ 0*peer_pressure_0 + .75*peer_pressure_1 + 1.75*peer_pressure_2      # Regressions   alc_intercept ~ 0*peer_intercept + 0*peer_slope   alc_slope ~ 0*peer_intercept + 0*peer_slope      # Time-varying covariances   alcohol_use_0 ~~ peer_pressure_0   alcohol_use_1 ~~ peer_pressure_1   alcohol_use_2 ~~ peer_pressure_2      alcohol_use_0 ~ 0*1   alcohol_use_1 ~ 0*1   alcohol_use_2 ~ 0*1   peer_pressure_0 ~ 0*1   peer_pressure_1 ~ 0*1   peer_pressure_2 ~ 0*1 \")  model_D_baseline_fit <- growth(   model_D_baseline, data = alcohol_use_2_wide, estimator = \"ml\", mimic = \"Mplus\" )  anova(model_D_baseline_fit, model_D_fit) #>  #> Chi-Squared Difference Test #>  #>                      Df    AIC    BIC   Chisq Chisq diff   RMSEA Df diff #> model_D_fit           4 6120.5 6236.1  11.557                            #> model_D_baseline_fit  8 6443.6 6539.1 342.648     331.09 0.26997       4 #>                      Pr(>Chisq)     #> model_D_fit                         #> model_D_baseline_fit  < 2.2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 lay <- get_layout(   \"peer_pressure_0\", NA,  \"peer_pressure_1\", NA, \"peer_pressure_2\",   NA, \"peer_intercept\", NA,  \"peer_slope\", NA,   NA, \"alc_intercept\", NA,  \"alc_slope\", NA,   \"alcohol_use_0\", NA,  \"alcohol_use_1\", NA, \"alcohol_use_2\",   rows = 4 )  graph_sem(model_D_fit, layout = lay)"},{"path":[]},{"path":[]},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-9.html","id":"should-you-conduct-a-survival-analysis-the-whether-and-when-test","dir":"Articles","previous_headings":"","what":"9.1 Should You Conduct a Survival Analysis? The “Whether” and “When” Test","title":"Chapter 9: ","text":"","code":"alcohol_relapse #> # A tibble: 89 × 3 #>    id    weeks censor #>    <fct> <dbl>  <dbl> #>  1 1     0.714      0 #>  2 2     0.714      0 #>  3 3     1.14       0 #>  4 4     1.43       0 #>  5 5     1.71       0 #>  6 6     1.71       0 #>  7 7     2.14       0 #>  8 8     2.71       0 #>  9 9     3.86       0 #> 10 10    4.14       0 #> # ℹ 79 more rows teachers #> # A tibble: 3,941 × 3 #>    id    years censor #>    <fct> <dbl>  <dbl> #>  1 1         1      0 #>  2 2         2      0 #>  3 3         1      0 #>  4 4         1      0 #>  5 5        12      1 #>  6 6         1      0 #>  7 7        12      1 #>  8 8         1      0 #>  9 9         2      0 #> 10 10        2      0 #> # ℹ 3,931 more rows suicide_ideation #> # A tibble: 391 × 4 #>    id     time censor   age #>    <fct> <dbl>  <dbl> <dbl> #>  1 1        16      0    18 #>  2 2        10      0    19 #>  3 3        16      0    19 #>  4 4        20      0    22 #>  5 6        15      0    22 #>  6 7        10      0    19 #>  7 8        22      1    22 #>  8 9        22      1    22 #>  9 10       15      0    20 #> 10 11       10      0    19 #> # ℹ 381 more rows"},{"path":[]},{"path":"https://mccarthy-m-g.github.io/alda/articles/chapter-9.html","id":"censoring-how-complete-are-the-data-on-event-occurrence","dir":"Articles","previous_headings":"","what":"9.3 Censoring: How Complete Are the Data on Event Occurrence?","title":"Chapter 9: ","text":"Figure 9.1:","code":"ggplot(teachers, aes(x = years)) +   geom_bar() +   geom_text(aes(label = after_stat(count)), stat = \"count\", vjust = -.5) +   scale_x_continuous(breaks = 1:12) +   coord_cartesian(ylim = c(0, 550)) +   facet_wrap(vars(censor), nrow = 2, labeller = label_both)"},{"path":"https://mccarthy-m-g.github.io/alda/articles/longitudinal-data-organization.html","id":"longitudinal-data-formats","dir":"Articles","previous_headings":"","what":"Longitudinal data formats","title":"Longitudinal data organization","text":"Longitudinal data can organized two distinct formats: person-level, wide, multivariate, format person one row data multiple columns containing data measurement occasion. person-period, long, univariate, format person one row data measurement occasion. R functions expect data person-period format visualization analysis, ’s easy convert longitudinal data set one format .","code":"glimpse(deviant_tolerance_pl) #> Rows: 16 #> Columns: 8 #> $ id           <fct> 9, 45, 268, 314, 442, 514, 569, 624, 723, 918, 949, 978, … #> $ tolerance_11 <dbl> 2.23, 1.12, 1.45, 1.22, 1.45, 1.34, 1.79, 1.12, 1.22, 1.0… #> $ tolerance_12 <dbl> 1.79, 1.45, 1.34, 1.22, 1.99, 1.67, 1.90, 1.12, 1.34, 1.0… #> $ tolerance_13 <dbl> 1.90, 1.45, 1.99, 1.55, 1.45, 2.23, 1.90, 1.22, 1.12, 1.2… #> $ tolerance_14 <dbl> 2.12, 1.45, 1.79, 1.12, 1.67, 2.12, 1.99, 1.12, 1.00, 1.9… #> $ tolerance_15 <dbl> 2.66, 1.99, 1.34, 1.12, 1.90, 2.44, 1.99, 1.22, 1.12, 1.2… #> $ male         <dbl> 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0 #> $ exposure     <dbl> 1.54, 1.16, 0.90, 0.81, 1.13, 0.90, 1.99, 0.98, 0.81, 1.2… glimpse(deviant_tolerance_pp) #> Rows: 80 #> Columns: 5 #> $ id        <fct> 9, 9, 9, 9, 9, 45, 45, 45, 45, 45, 268, 268, 268, 268, 268, … #> $ age       <dbl> 11, 12, 13, 14, 15, 11, 12, 13, 14, 15, 11, 12, 13, 14, 15, … #> $ tolerance <dbl> 2.23, 1.79, 1.90, 2.12, 2.66, 1.12, 1.45, 1.45, 1.45, 1.99, … #> $ male      <dbl> 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, … #> $ exposure  <dbl> 1.54, 1.54, 1.54, 1.54, 1.54, 1.16, 1.16, 1.16, 1.16, 1.16, …"},{"path":"https://mccarthy-m-g.github.io/alda/articles/longitudinal-data-organization.html","id":"converting-between-formats","dir":"Articles","previous_headings":"Longitudinal data formats","what":"Converting between formats","title":"Longitudinal data organization","text":"convert person-level data set person-period format use dplyr::pivot_longer(): convert person-period data set person-level format use dplyr::pivot_wider():","code":"pivot_longer(   deviant_tolerance_pl,   cols = starts_with(\"tolerance_\"),   names_to = \"age\",   names_pattern = \"([[:digit:]]+)\",   names_transform = as.integer,   values_to = \"tolerance\" ) #> # A tibble: 80 × 5 #>    id     male exposure   age tolerance #>    <fct> <dbl>    <dbl> <int>     <dbl> #>  1 9         0     1.54    11      2.23 #>  2 9         0     1.54    12      1.79 #>  3 9         0     1.54    13      1.9  #>  4 9         0     1.54    14      2.12 #>  5 9         0     1.54    15      2.66 #>  6 45        1     1.16    11      1.12 #>  7 45        1     1.16    12      1.45 #>  8 45        1     1.16    13      1.45 #>  9 45        1     1.16    14      1.45 #> 10 45        1     1.16    15      1.99 #> # ℹ 70 more rows pivot_wider(   deviant_tolerance_pp,   names_from = age,   names_prefix = \"tolerance_\",   values_from = tolerance ) #> # A tibble: 16 × 8 #>    id     male exposure tolerance_11 tolerance_12 tolerance_13 tolerance_14 #>    <fct> <dbl>    <dbl>        <dbl>        <dbl>        <dbl>        <dbl> #>  1 9         0     1.54         2.23         1.79         1.9          2.12 #>  2 45        1     1.16         1.12         1.45         1.45         1.45 #>  3 268       1     0.9          1.45         1.34         1.99         1.79 #>  4 314       0     0.81         1.22         1.22         1.55         1.12 #>  5 442       0     1.13         1.45         1.99         1.45         1.67 #>  6 514       1     0.9          1.34         1.67         2.23         2.12 #>  7 569       0     1.99         1.79         1.9          1.9          1.99 #>  8 624       1     0.98         1.12         1.12         1.22         1.12 #>  9 723       0     0.81         1.22         1.34         1.12         1    #> 10 918       0     1.21         1            1            1.22         1.99 #> 11 949       1     0.93         1.99         1.55         1.12         1.45 #> 12 978       1     1.59         1.22         1.34         2.12         3.46 #> 13 1105      1     1.38         1.34         1.9          1.99         1.9  #> 14 1542      0     1.44         1.22         1.22         1.99         1.79 #> 15 1552      0     1.04         1            1.12         2.23         1.55 #> 16 1653      0     1.25         1.11         1.11         1.34         1.55 #> # ℹ 1 more variable: tolerance_15 <dbl>"},{"path":"https://mccarthy-m-g.github.io/alda/articles/longitudinal-data-organization.html","id":"adding-discrete-time-indicators-to-person-period-data","dir":"Articles","previous_headings":"","what":"Adding discrete time indicators to person-period data","title":"Longitudinal data organization","text":"add discrete time indicators person-period data set first create temporary copy time variable column ones, use dplyr::pivot_wider():","code":"deviant_tolerance_pp |>   mutate(     temp_age = age,     temp_dummy = 1   ) |>   pivot_wider(     names_from = temp_age,     names_prefix = \"age_\",     values_from = temp_dummy,     values_fill = 0   ) #> # A tibble: 80 × 10 #>    id      age tolerance  male exposure age_11 age_12 age_13 age_14 age_15 #>    <fct> <dbl>     <dbl> <dbl>    <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> #>  1 9        11      2.23     0     1.54      1      0      0      0      0 #>  2 9        12      1.79     0     1.54      0      1      0      0      0 #>  3 9        13      1.9      0     1.54      0      0      1      0      0 #>  4 9        14      2.12     0     1.54      0      0      0      1      0 #>  5 9        15      2.66     0     1.54      0      0      0      0      1 #>  6 45       11      1.12     1     1.16      1      0      0      0      0 #>  7 45       12      1.45     1     1.16      0      1      0      0      0 #>  8 45       13      1.45     1     1.16      0      0      1      0      0 #>  9 45       14      1.45     1     1.16      0      0      0      1      0 #> 10 45       15      1.99     1     1.16      0      0      0      0      1 #> # ℹ 70 more rows"},{"path":"https://mccarthy-m-g.github.io/alda/articles/longitudinal-data-organization.html","id":"adding-contiguous-periods-to-person-level-survival-data","dir":"Articles","previous_headings":"","what":"Adding contiguous periods to person-level survival data","title":"Longitudinal data organization","text":"add contiguous periods person-level data use dplyr::reframe():","code":"first_sex |>   # In order to add the event indicator, the time variable needs a different   # name in the person-level data from the name we want to use in `reframe()`.   # This is a temporary variable so it doesn't matter what the name is.   rename(grades = grade) |>   group_by(id) |>   reframe(     grade = 1:max(grades),     event = if_else(grade == grades & censor == 0, 1, 0),     # To keep predictors from the person-level data, simply list them. If there     # are many predictors it might be more convenient to use     # `dplyr::left_join()` after `reframe()`.     parental_transition,     parental_antisociality   ) #> # A tibble: 1,902 × 5 #>    id    grade event parental_transition parental_antisociality #>    <fct> <int> <dbl>               <dbl>                  <dbl> #>  1 1         1     0                   0                  1.98  #>  2 1         2     0                   0                  1.98  #>  3 1         3     0                   0                  1.98  #>  4 1         4     0                   0                  1.98  #>  5 1         5     0                   0                  1.98  #>  6 1         6     0                   0                  1.98  #>  7 1         7     0                   0                  1.98  #>  8 1         8     0                   0                  1.98  #>  9 1         9     1                   0                  1.98  #> 10 2         1     0                   1                 -0.545 #> # ℹ 1,892 more rows"},{"path":"https://mccarthy-m-g.github.io/alda/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Michael McCarthy. Author, maintainer.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"McCarthy M (2024). alda: Data Applied longitudinal data analysis: Modeling change event occurrence. R package version 0.0.0.9000, https://github.com/mccarthy-m-g/alda, https://mccarthy-m-g.github.io/alda/.","code":"@Manual{,   title = {alda: Data for Applied longitudinal data analysis: Modeling change and event occurrence},   author = {Michael McCarthy},   year = {2024},   note = {R package version 0.0.0.9000, https://github.com/mccarthy-m-g/alda},   url = {https://mccarthy-m-g.github.io/alda/}, }"},{"path":"https://mccarthy-m-g.github.io/alda/index.html","id":"alda","dir":"","previous_headings":"","what":"Data for Applied longitudinal data analysis: Modeling change and event occurrence","title":"Data for Applied longitudinal data analysis: Modeling change and event occurrence","text":"package contains 31 data sets provided Singer Willet (2003) book, Applied longitudinal data analysis: Modeling change event occurrence, suitable longitudinal mixed effects modelling, longitudinal structural equation modelling, survival analysis. data sets package real data real studies; however, modified Singer Willet (2003) illustration statistical methods, may match results original studies. eleven data sets longitudinal mixed effects modelling: ?deviant_tolerance: Adolescent tolerance deviant behaviour (Chapter 2) ?early_intervention: Early educational interventions cognitive performance (Chapter 3) ?alcohol_use_1: Adolescent peer alcohol use (Chapters 4 6) ?reading_scores: Peabody Individual Achievement Test reading scores (Chapter 5) ?dropout_wages: High school dropout labour market experiences (Chapters 5 6) ?depression_unemployment: Unemployment depression (Chapter 5) ?antidepressants: Antidepressant medication positive mood (Chapter 5) ?berkeley: Berkeley Growth Study (Chapter 6) ?externalizing_behaviour: Externalizing behaviour children (Chapter 6) ?cognitive_growth: Cognitive growth children (Chapter 6) ?opposites_naming: Opposites naming task (Chapter 7) one data set longitudinal structural equation modelling: ?alcohol_use_2: Adolescent alcohol consumption peer pressure (Chapter 8) twenty data sets survival analysis: ?teachers: Years special education teacher turnover (Chapters 9 10) ?cocaine_relapse_1: Weeks cocaine relapse treatment (Chapter 10) ?first_sex: Age first sexual intercourse (Chapters 10 11) ?suicide_ideation: Age first suicide ideation (Chapter 10) ?congresswomen: House Representatives tenure (Chapter 10) ?tenure: Years academic tenure (Chapter 12) ?first_depression_1: Age first depression (Chapter 12) ?first_arrest: Age first juvenile arrest (Chapter 12) ?math_dropout: Math course history (Chapter 12) ?honking: Time horn honking (Chapter 13) ?alcohol_relapse: Weeks alcohol relapse treatment (Chapter 13) ?judges: Supreme Court justice tenure (Chapters 13 15) ?first_depression_2: Age first depression (Chapter 13) ?health_workers: Length health worker employment (Chapter 13) ?rearrest: Days inmate recidivism (Chapters 14 15) ?first_cocaine: Age first cocaine use (Chapter 15) ?cocaine_relapse_2: Days cocaine relapse abstinence (Chapter 15) ?psychiatric_discharge: Days psychiatric hospital discharge (Chapter 15) ?physicians: Physician career history (Chapter 15) ?monkeys: Piagetian monkeys (Chapter 15)","code":""},{"path":"https://mccarthy-m-g.github.io/alda/index.html","id":"vignettes-and-articles","dir":"","previous_headings":"","what":"Vignettes and articles","title":"Data for Applied longitudinal data analysis: Modeling change and event occurrence","text":"one vignette tips tricks working longitudinal data: vignette(\"longitudinal-data-organization\") fourteen articles package documentation website demonstrating recreate examples textbook R: Chapter 2: Exploring longitudinal data change Chapter 3: Introducing multilevel model change Chapter 4: data analysis multilevel model change Chapter 5: Treating time flexibly Chapter 6: Modeling discontinuous nonlinear change Chapter 7: Examining multilevel model’s error covariance structure Chapter 8: Modeling change using covariance structure analysis Chapter 9: framework investigating event occurrence Chapter 10: Describing discrete-time event occurrence data Chapter 11: Fitting basic discrete-time hazard models Chapter 12: Extending discrete-time hazard model Chapter 13: Describing continuous-time event occurrence data Chapter 14: Fitting Cox regression model Chapter 15: Extending Cox regression model","code":""},{"path":"https://mccarthy-m-g.github.io/alda/index.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"Data for Applied longitudinal data analysis: Modeling change and event occurrence","text":"See https://mccarthy-m-g.github.io/alda/ also installed package: help(package = \"alda\").","code":""},{"path":"https://mccarthy-m-g.github.io/alda/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Data for Applied longitudinal data analysis: Modeling change and event occurrence","text":"Singer, J. D., & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change event occurrence. Oxford University Press, USA. https://doi.org/10.1093/acprof:oso/9780195152968.001.0001","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/alcohol_relapse.html","id":null,"dir":"Reference","previous_headings":"","what":"Weeks to alcohol relapse after treatment — alcohol_relapse","title":"Weeks to alcohol relapse after treatment — alcohol_relapse","text":"subset data Cooney colleagues (1991) measuring weeks first \"heavy drinking\" day sample 89 recently treated alcoholics. Individuals followed two years (around 104.286 weeks) relapsed.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/alcohol_relapse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Weeks to alcohol relapse after treatment — alcohol_relapse","text":"","code":"alcohol_relapse"},{"path":"https://mccarthy-m-g.github.io/alda/reference/alcohol_relapse.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Weeks to alcohol relapse after treatment — alcohol_relapse","text":"person-level data frame 89 rows 3 columns: id Participant ID. weeks Number weeks first \"heavy drinking\" day censor Censoring status.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/alcohol_relapse.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Weeks to alcohol relapse after treatment — alcohol_relapse","text":"Cooney, N. L., Kadden, R. M., Litt, M. D., & Getter, H. (1991). Matching alcoholics coping skills interactional therapies: Two-year follow-results. Journal Consulting Clinical Psychology, 59, 598–601. https://doi.org/10.1037/0022-006X.59.4.598","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/alcohol_use_1.html","id":null,"dir":"Reference","previous_headings":"","what":"Adolescent and peer alcohol use — alcohol_use_1","title":"Adolescent and peer alcohol use — alcohol_use_1","text":"subset data Curran, Stice, Chassin (1997) measuring relation changes alcohol use changes peer alcohol use 3-year period community-based sample Hispanic Caucasian adolescents.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/alcohol_use_1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adolescent and peer alcohol use — alcohol_use_1","text":"","code":"alcohol_use_1"},{"path":"https://mccarthy-m-g.github.io/alda/reference/alcohol_use_1.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Adolescent and peer alcohol use — alcohol_use_1","text":"person-period data frame 246 rows 6 columns: id Participant ID. age years. child_of_alcoholic Binary indicator whether adolescent child alcoholic parent. male Binary indicator whether adolescent male. alcohol_use Square root summed scores four eight-point items measuring frequency alcohol use. peer_alcohol_use Square root summed scores two six-point items measuring frequency peer alcohol use.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/alcohol_use_1.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Adolescent and peer alcohol use — alcohol_use_1","text":"Curran, P. J., Stice, E., & Chassin, L. (1997). relation adolescent peer alcohol use: longitudinal random coefficients model. Journal Consulting Clinical Psychology, 65, 130–140. https://doi.org/10.1037//0022-006x.65.1.130","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/alcohol_use_2.html","id":null,"dir":"Reference","previous_headings":"","what":"Adolescent alcohol consumption and peer pressure — alcohol_use_2","title":"Adolescent alcohol consumption and peer pressure — alcohol_use_2","text":"Data Barnes, Farrell, Banerjee (1994) measuring relation changes alcohol use changes peer pressure use alcohol sample 1122 Black White adolescents tracked beginning seventh grade end eighth grade.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/alcohol_use_2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adolescent alcohol consumption and peer pressure — alcohol_use_2","text":"","code":"alcohol_use_2"},{"path":"https://mccarthy-m-g.github.io/alda/reference/alcohol_use_2.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Adolescent alcohol consumption and peer pressure — alcohol_use_2","text":"person-period data frame 3366 rows 5 columns: id Participant ID. time Time measurement. female Binary indicator whether adolescent female. alcohol_use Natural logarithm averaged scores three six-point items measuring frequency beer, wine, liquor consumption, respectively. peer_pressure Natural logarithm six-point item measuring frequency friends offered alcoholic drinks past month.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/alcohol_use_2.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Adolescent alcohol consumption and peer pressure — alcohol_use_2","text":"Barnes, G. M., Farrell, M. P., & Banerjee, S. (1994). Family influences alcohol abuse problem behaviors among black white adolescents general population sample. Journal Research Adolescence, 4, 183–201. https://doi.org/10.1207/s15327795jra0402_2","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/alcohol_use_2.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Adolescent alcohol consumption and peer pressure — alcohol_use_2","text":"Barnes, Farrell, Banerjee (1994) report sample 699 adolescents; however, note ongoing longitudinal study, likely explains sample size discrepancy data used Singer Willett (2003).","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/antidepressants.html","id":null,"dir":"Reference","previous_headings":"","what":"Antidepressant medication and positive mood — antidepressants","title":"Antidepressant medication and positive mood — antidepressants","text":"subset data Tomarken, Shelton, Elkins, Anderson (1997) measuring relation changes positive mood supplemental antidepressant medication course week sample 73 men women already receiving nonpharmacological therapy depression.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/antidepressants.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Antidepressant medication and positive mood — antidepressants","text":"","code":"antidepressants"},{"path":"https://mccarthy-m-g.github.io/alda/reference/antidepressants.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Antidepressant medication and positive mood — antidepressants","text":"person-period data frame 1242 rows 8 columns: id Participant ID. wave Wave measurement. day Day measurement. reading Time day measurement. time_of_day Time day measurement expressed numerically (0 morning readings; 0.33 afternoon readings; 0.67 evening readings). time Time measurement expressed combination day time_of_day. positive_mood Positive mood score. treatment Treatment condition (placebo pills = 0, antidepressant pills = 1).","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/antidepressants.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Antidepressant medication and positive mood — antidepressants","text":"Tomarken, . J., Shelton, R. C., Elkins, L., & Anderson, T. (1997). Sleep deprivation anti-depressant medication: Unique effects positive negative affect. Poster session presented 9th annual meeting American Psychological Society, Washington, DC.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/berkeley.html","id":null,"dir":"Reference","previous_headings":"","what":"Berkeley Growth Study — berkeley","title":"Berkeley Growth Study — berkeley","text":"subset data Berkeley Growth Study measuring changes IQ single girl followed childhood older adulthood (Bayley, 1935).","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/berkeley.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Berkeley Growth Study — berkeley","text":"","code":"berkeley"},{"path":"https://mccarthy-m-g.github.io/alda/reference/berkeley.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Berkeley Growth Study — berkeley","text":"person-period data frame 18 rows 2 columns: age Age girl years. iq IQ score.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/berkeley.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Berkeley Growth Study — berkeley","text":"Bayley, N. (1935). development motor abilities first three years. Monographs Society Research Child Development, 1.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/cocaine_relapse_1.html","id":null,"dir":"Reference","previous_headings":"","what":"Weeks to cocaine relapse after treatment — cocaine_relapse_1","title":"Weeks to cocaine relapse after treatment — cocaine_relapse_1","text":"subset data Hall, Havassy, Wasserman's (1990) measuring number weeks relapse cocaine use sample 104 former addicts released -patient treatment program. -patients followed 12 weeks used cocaine 7 consecutive days.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/cocaine_relapse_1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Weeks to cocaine relapse after treatment — cocaine_relapse_1","text":"","code":"cocaine_relapse_1"},{"path":"https://mccarthy-m-g.github.io/alda/reference/cocaine_relapse_1.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Weeks to cocaine relapse after treatment — cocaine_relapse_1","text":"person-level data frame 104 rows 4 columns: id -patient ID. weeks number weeks -patient's release relapse cocaine use. censor Censoring status. needle Binary indicator whether cocaine ever used intravenously.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/cocaine_relapse_1.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Weeks to cocaine relapse after treatment — cocaine_relapse_1","text":"Hall, S. M., Havassy, B. E., & Wasserman, D. . (1990). Commitment abstinence acute stress relapse alcohol, opiates, nicotine. Journal Consulting Clinical Psychology, 58, 175–181. https://doi.org/10.1037//0022-006x.58.2.175","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/cocaine_relapse_2.html","id":null,"dir":"Reference","previous_headings":"","what":"Days to cocaine relapse after abstinence — cocaine_relapse_2","title":"Days to cocaine relapse after abstinence — cocaine_relapse_2","text":"subset unpublished data Hall, Havassy, Wasserman (1990) measuring relation number days relapse cocaine use several predictors might associated relapse sample 104 newly abstinent cocaine users recently completed abstinence-oriented treatment program. Former cocaine users followed 12 weeks post-treatment used cocaine 7 consecutive days. Self-reported abstinence confirmed interview absence cocaine urine specimens.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/cocaine_relapse_2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Days to cocaine relapse after abstinence — cocaine_relapse_2","text":"","code":"cocaine_relapse_2"},{"path":"https://mccarthy-m-g.github.io/alda/reference/cocaine_relapse_2.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Days to cocaine relapse after abstinence — cocaine_relapse_2","text":"person-period data frame 1248 rows 7 columns: id Participant ID. days Number days relapse cocaine use censoring. Relapse defined 4 days cocaine use week preceding interview. Study dropouts lost participants coded relapsing cocaine use, number days relapse coded occurring week last follow-interview attended. censor Censoring status (0 = relapsed, 1 = censored). needle Binary indicator whether cocaine ever used intravenously. base_mood Total score positive mood subscales (Activity Happiness) Mood Questionnaire (Ryman, Biersner, & LaRocco, 1974), taken intake interview last week treatment. item used five point Likert score (ranging 0 = , 4 = extremely). followup Week follow-interview. mood Total score positive mood subscales (Activity Happiness) Mood Questionnaire (Ryman, Biersner, & LaRocco, 1974), taken follow-interviews week post-treatment. item used five point Likert score (ranging 0 = , 4 = extremely).","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/cocaine_relapse_2.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Days to cocaine relapse after abstinence — cocaine_relapse_2","text":"Hall, S. M., Havassy, B. E., & Wasserman, D. . (1990). Commitment abstinence acute stress relapse alcohol, opiates, nicotine. Journal Consulting Clinical Psychology, 58, 175–181. https://doi.org/10.1037//0022-006x.58.2.175","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/cocaine_relapse_2.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Days to cocaine relapse after abstinence — cocaine_relapse_2","text":"Hall, Havassy, Wasserman (1990) measured time relapse weeks, days; however, use data illustrate imputation strategies, Singer Willett (2003) converted weekly relapse information days, jittered event times, effectively converting discrete-time continuous-time. Additionally, Hall, Havassy, Wasserman (1990) report following cocaine users study, thus, appears unpublished data.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/cocaine_relapse_2.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Days to cocaine relapse after abstinence — cocaine_relapse_2","text":"Ryman, D. H., Biersner, R. J., & La Rocco, J. M. (1974). Reliabilities validities Mood Questionnaire. Psychological Reports, 35, 479-484. https://doi.org/10.2466/pr0.1974.35.1.479","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/cognitive_growth.html","id":null,"dir":"Reference","previous_headings":"","what":"Cognitive growth in children — cognitive_growth","title":"Cognitive growth in children — cognitive_growth","text":"Data Tivnan (1980) measuring changes cognitive growth three-week period sample 17 first second-graders.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/cognitive_growth.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cognitive growth in children — cognitive_growth","text":"","code":"cognitive_growth"},{"path":"https://mccarthy-m-g.github.io/alda/reference/cognitive_growth.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Cognitive growth in children — cognitive_growth","text":"person-period data frame 445 rows 4 columns: id Child ID. game Game number. child played maximum 27 games. nmoves number moves completed making catastrophic error. read Score unnamed standardized reading test.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/cognitive_growth.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Cognitive growth in children — cognitive_growth","text":"Tivnan, T. (1980). Improvements performance cognitive tasks: acquisition new skills elementary school children. Unpublished doctoral dissertation. Harvard University, Graduate School Education.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/congresswomen.html","id":null,"dir":"Reference","previous_headings":"","what":"House of Representatives tenure — congresswomen","title":"House of Representatives tenure — congresswomen","text":"Data measuring long 168 women elected U.S. House Representatives 1919 1996 remained office. Representatives followed eight terms 1998.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/congresswomen.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"House of Representatives tenure — congresswomen","text":"","code":"congresswomen"},{"path":"https://mccarthy-m-g.github.io/alda/reference/congresswomen.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"House of Representatives tenure — congresswomen","text":"person-level data frame 168 rows 5 columns: id Participant ID. name Representative name. time Number terms office. censor Censoring status. democrat Party affiliation.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/depression_unemployment.html","id":null,"dir":"Reference","previous_headings":"","what":"Unemployment and depression — depression_unemployment","title":"Unemployment and depression — depression_unemployment","text":"subset data Ginexi colleagues (2000) measuring changes depressive symptoms job loss sample 254 recently unemployed men women.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/depression_unemployment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unemployment and depression — depression_unemployment","text":"","code":"depression_unemployment"},{"path":"https://mccarthy-m-g.github.io/alda/reference/depression_unemployment.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Unemployment and depression — depression_unemployment","text":"person-period data frame 674 rows 4 columns: id Participant ID. months Months since job loss. depression Center Epidemiologic Studies' Depression (CES-D) scale score (Radloff, 1977) unemployed Binary indicator whether participant unemployed time interview.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/depression_unemployment.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Unemployment and depression — depression_unemployment","text":"Ginexi, E. M., Howe, G. W., & Caplan, R. D. (2000). Depression control beliefs relation reemployment: directions effect? Journal Occupational Health Psychology, 5, 323–336. https://doi.org/10.1037//1076-8998.5.3.323","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/depression_unemployment.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Unemployment and depression — depression_unemployment","text":"Radloff, L. S. (1977). CES-D scale: self report major depressive disorder scale research general population. Applied Psychological Measurement, 1, 385–401. https://doi.org/10.1177/014662167700100306","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/deviant_tolerance.html","id":null,"dir":"Reference","previous_headings":"","what":"Adolescent tolerance of deviant behaviour — deviant_tolerance_pp","title":"Adolescent tolerance of deviant behaviour — deviant_tolerance_pp","text":"subset data National Youth Survey (NYS) measuring tolerance deviant behaviour adolescents time (Raudenbush & Chan, 1992).","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/deviant_tolerance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adolescent tolerance of deviant behaviour — deviant_tolerance_pp","text":"","code":"deviant_tolerance_pp  deviant_tolerance_pl"},{"path":[]},{"path":"https://mccarthy-m-g.github.io/alda/reference/deviant_tolerance.html","id":"deviant-tolerance-pp","dir":"Reference","previous_headings":"","what":"deviant_tolerance_pp","title":"Adolescent tolerance of deviant behaviour — deviant_tolerance_pp","text":"person-period data frame 80 rows 5 columns: id Participant ID. age Adolescent age years. tolerance Average score across 9-item scale assessing attitudes favourable deviant behaviour. item used four point scale (1 = wrong, 2 = wrong, 3 = little bit wrong, 4 = wrong ). male Binary indicator whether adolescent male. exposure Average score across 9-item scale assessing level exposure deviant peers. item used five point Likert score (ranging 0 = none, 4 = ).","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/deviant_tolerance.html","id":"deviant-tolerance-pl","dir":"Reference","previous_headings":"","what":"deviant_tolerance_pl","title":"Adolescent tolerance of deviant behaviour — deviant_tolerance_pp","text":"person-level data frame 16 rows 8 columns: id Participant ID. tolerance_11, tolerance_12, tolerance_13, tolerance_14, tolerance_15, Average score across 9-item scale assessing attitudes favourable deviant behaviour ages 11, 12, 13, 14, 15. item used four point scale (1 = wrong, 2 = wrong, 3 = little bit wrong, 4 = wrong ). male Binary indicator whether adolescent male. exposure Average score across 9-item scale assessing level exposure deviant peers. item used five point Likert score (ranging 0 = none, 4 = ).","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/deviant_tolerance.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Adolescent tolerance of deviant behaviour — deviant_tolerance_pp","text":"Raudenbush, S. W., & Chan, W. S. (1992). Growth curve analysis accelerated longitudinal designs. Journal Research Crime Delinquency, 29, 387–411. https://doi.org/10.1177/0022427892029004001","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/deviant_tolerance.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Adolescent tolerance of deviant behaviour — deviant_tolerance_pp","text":"Raudenbush Chan (1992) comment exposure time-varying predictor original study; however, Singer Willett (2003) provide exposure time-invariant predictor.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/dropout_wages.html","id":null,"dir":"Reference","previous_headings":"","what":"High school dropout labour market experiences — dropout_wages","title":"High school dropout labour market experiences — dropout_wages","text":"subset data National Longitudinal Study Youth tracking labour market experiences male high school dropouts (Murnane, Boudett, & Willett, 1999).","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/dropout_wages.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"High school dropout labour market experiences — dropout_wages","text":"","code":"dropout_wages  dropout_wages_subset"},{"path":[]},{"path":"https://mccarthy-m-g.github.io/alda/reference/dropout_wages.html","id":"dropout-wages","dir":"Reference","previous_headings":"","what":"dropout_wages","title":"High school dropout labour market experiences — dropout_wages","text":"person-period data frame 6402 rows 9 columns: id Participant ID. log_wages Natural logarithm wages. experience Labour force experience years, tracked dropouts' first day work. ged Binary indicator whether dropout obtained GED. postsecondary_education Binary indicator whether dropout obtained post-secondary education. black Binary indicator whether dropout black. hispanic Binary indicator whether dropout hispanic. highest_grade Highest grade completed. unemployment_rate Unemployment rate local geographic area.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/dropout_wages.html","id":"dropout-wages-subset","dir":"Reference","previous_headings":"","what":"dropout_wages_subset","title":"High school dropout labour market experiences — dropout_wages","text":"person-period data frame 257 rows 5 columns: id Participant ID. log_wages Natural logarithm wages. experience Labour force experience years, tracked dropouts' first day work. black Binary indicator whether dropout black. highest_grade Highest grade completed.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/dropout_wages.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"High school dropout labour market experiences — dropout_wages","text":"Murnane, R. J., Boudett, K. P., & Willett, J. B. (1999). male dropouts benefit obtaining GED, postsecondary education, training? Evaluation Review, 23, 475–502. https://doi.org/10.1177/0193841x9902300501","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/early_intervention.html","id":null,"dir":"Reference","previous_headings":"","what":"Early educational intervention and cognitive performance — early_intervention","title":"Early educational intervention and cognitive performance — early_intervention","text":"Simulated data Burchinal, Campbell, Bryant, Wasik, Ramey (1997) measuring effect early educational intervention cognitive performance sample African-American children ages 12, 18, 24 months.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/early_intervention.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Early educational intervention and cognitive performance — early_intervention","text":"","code":"early_intervention"},{"path":"https://mccarthy-m-g.github.io/alda/reference/early_intervention.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Early educational intervention and cognitive performance — early_intervention","text":"person-period data frame 309 rows 4 columns: id Child ID. age Age years time measurement. treatment Treatment condition (control = 0, intervention = 1). cognitive_score Cognitive performance score one two standardized intelligence tests: Bayley Scales Infant Development (Bayley, 1969) 12 18 months, Stanford Binet (Terman & Merrill, 1972) 24 months.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/early_intervention.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Early educational intervention and cognitive performance — early_intervention","text":"Burchinal, M. R., Campbell, F. ., Bryant, D. M., Wasik, B. H., & Ramey, C. T. (1997). Early intervention mediating processes cognitive performance children low income African American families. Child Development, 68, 935–954. https://doi.org/10.2307/1132043","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/early_intervention.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Early educational intervention and cognitive performance — early_intervention","text":"request researchers, Singer Willet (2003) provide data Burchinal, Campbell, Bryant, Wasik, Ramey's (1997) order ensure privacy study's participants. However, data provided simulated similar statistical properties study order match estimates figures presented text best possible.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/early_intervention.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Early educational intervention and cognitive performance — early_intervention","text":"Bayley, N. (1969). Bayley Scales Infant Development. New York: Psychological Corp. Terman, L. M., & Merrill, N. Q. (1972). Stanford-Binet Intelligence Scale: 1972 Norms Editions. Boston: Houghton Mifflin.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/externalizing_behaviour.html","id":null,"dir":"Reference","previous_headings":"","what":"Externalizing behaviour in children — externalizing_behaviour","title":"Externalizing behaviour in children — externalizing_behaviour","text":"subset data Keiley, Bates, Dodge, Pettit (2000) measuring changes externalizing behaviour sample 45 children tracked first sixth grade.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/externalizing_behaviour.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Externalizing behaviour in children — externalizing_behaviour","text":"","code":"externalizing_behaviour"},{"path":"https://mccarthy-m-g.github.io/alda/reference/externalizing_behaviour.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Externalizing behaviour in children — externalizing_behaviour","text":"person-period data frame 270 rows 5 columns: id Child ID. time Time measurement. externalizing_behaviour Sum scores Achenbach's (1991) Child Behavior Checklist. Scores range 0 68 female Binary indicator whether adolescent female. grade Grade year.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/externalizing_behaviour.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Externalizing behaviour in children — externalizing_behaviour","text":"Keiley, M. K., Bates, J. E., Dodge, K. ., & Pettit, G. S. (2000). cross-domain growth analysis: Externalizing internalizing behavior 8 years childhood. Journal Abnormal Child Psychology, 28, 161–179. https://doi.org/10.1023%2Fa%3A1005122814723","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/externalizing_behaviour.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Externalizing behaviour in children — externalizing_behaviour","text":"Achenbach, T. M. (1991). Manual Child Behavior Checklist 4–18 1991 Profile. Burlington, VT: University Vermont Press.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/first_arrest.html","id":null,"dir":"Reference","previous_headings":"","what":"Age of first juvenile arrest — first_arrest","title":"Age of first juvenile arrest — first_arrest","text":"Data Keiley Martin (2002) measuring effect child abuse risk first juvenile arrest sample 1553 adolescents aged 8 18. Adolescents followed age 18 arrested.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/first_arrest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Age of first juvenile arrest — first_arrest","text":"","code":"first_arrest"},{"path":"https://mccarthy-m-g.github.io/alda/reference/first_arrest.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Age of first juvenile arrest — first_arrest","text":"person-period data frame 15834 rows 7 columns: id Participant ID. time Age first juvenile arrest. censor Censoring status. abused Binary indicator whether adolescent abused. black Binary indicator whether adolescent black. period Age record corresponds . event Binary indicator whether adolescent arrested.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/first_arrest.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Age of first juvenile arrest — first_arrest","text":"Keiley, M. K., & Martin, N. C. (2002). Child abuse, neglect, juvenile delinquency: “new” statistical approaches can inform understanding “old” questions—reanalysis Widon, 1989. Manuscript submitted publication.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/first_cocaine.html","id":null,"dir":"Reference","previous_headings":"","what":"Age of first cocaine use — first_cocaine","title":"Age of first cocaine use — first_cocaine","text":"Data Burton colleagues (1996) measuring relation age first cocaine use drug-use history random sample 1658 white American men. Age first cocaine use drug-use history determined two interviews eleven years apart (1974 1985).","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/first_cocaine.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Age of first cocaine use — first_cocaine","text":"","code":"first_cocaine"},{"path":"https://mccarthy-m-g.github.io/alda/reference/first_cocaine.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Age of first cocaine use — first_cocaine","text":"person-level data frame 1658 rows 15 columns: id Participant ID. used_cocaine_age Age first cocaine use. censor Censoring status. birth_year  early_marijuana_use Binary indicator whether marijuana used age 17. used_marijuana Binary indicator whether participant used marijuana study period. used_marijuana_age Age participant first used marijuana. sold_marijuana Binary indicator whether participant sold marijuana study period. sold_marijuana_age Age participant first sold marijuana. early_drug_use Binary indicator whether drugs used age 17. used_drugs Binary indicator whether participant used drugs study period. used_drugs_age Age participant first used drugs. sold_drugs Binary indicator whether participant sold drugs study period. sold_drugs_age Age participant first sold drugs. rural Binary indicator whether participant lived rural area.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/first_cocaine.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Age of first cocaine use — first_cocaine","text":"Burton, R. P. D., Johnson, R. J., Ritter, C., & Clayton. R. R. (1996). effects role socialization initiation cocaine use: event history analysis adolescence middle adulthood. Journal Health Social Behavior, 37, 75–90. https://doi.org/10.2307/2137232","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/first_depression_1.html","id":null,"dir":"Reference","previous_headings":"","what":"Age of first depression — first_depression_1","title":"Age of first depression — first_depression_1","text":"subset data Wheaton, Rozell, Hall (1997) measuring relation age first depressive episode several childhood adult traumatic stressors random sample 1393 adults living metropolitan Toronto, Ontario. Age first depressive episode traumatic stressors determined structured interview.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/first_depression_1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Age of first depression — first_depression_1","text":"","code":"first_depression_1"},{"path":"https://mccarthy-m-g.github.io/alda/reference/first_depression_1.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Age of first depression — first_depression_1","text":"person-period data frame 36997 rows 11 columns: id Participant ID. onset Age first depressive episode. censor Censoring status. interview_age Age time interview. female Binary indicator whether adult female. siblings Number siblings. bigfamily Binary indicator whether adult five siblings. period Age record corresponds . depressive_episode Binary indicator whether adult experienced depressive episode. parental_divorce Binary indicator whether adult's parents divorced previous age. parental_divorce_now Binary indicator whether adult's parents divorced current period.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/first_depression_1.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Age of first depression — first_depression_1","text":"Wheaton, B., Roszell, P., & Hall, K. (1997). impact twenty childhood adult traumatic stressors risk psychiatric disorder. . H. Gotlib & B. Wheaton (Eds.), Stress adversity life course: Trajectories turning points (pp. 50–72). New York: Cambridge University Press. https://doi.org/10.1017/CBO9780511527623.003","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/first_depression_2.html","id":null,"dir":"Reference","previous_headings":"","what":"Age of first depression — first_depression_2","title":"Age of first depression — first_depression_2","text":"Data Sorenson, Rutter, Aneshensel (1991) measuring age first depressive episode sample 2974 adults. Age first depressive episode measured asking respondents whether , , age first experienced depressive episode.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/first_depression_2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Age of first depression — first_depression_2","text":"","code":"first_depression_2"},{"path":"https://mccarthy-m-g.github.io/alda/reference/first_depression_2.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Age of first depression — first_depression_2","text":"person-level data frame 2974 rows 3 columns: id Participant ID. age years. censor Censoring status.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/first_depression_2.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Age of first depression — first_depression_2","text":"Sorenson, S. B., Rutter, C. M., & Aneshensel, C. S. (1991). Depression community: investigation age onset. Journal Consulting Clinical Psychology, 59, 541546. https://doi.org/10.1037/0022-006X.59.4.541","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/first_sex.html","id":null,"dir":"Reference","previous_headings":"","what":"Age of first sexual intercourse — first_sex","title":"Age of first sexual intercourse — first_sex","text":"subset data Capaldi, Crosby, Stoolmiller's (1996) measuring grade year first sexual intercourse sample 180 -risk heterosexual adolescent males. Adolescent males followed Grade 7 Grade 12 reported sexual intercourse first time.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/first_sex.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Age of first sexual intercourse — first_sex","text":"","code":"first_sex"},{"path":"https://mccarthy-m-g.github.io/alda/reference/first_sex.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Age of first sexual intercourse — first_sex","text":"person-level data frame 180 rows 5 columns: id Participant ID. grade Grade year first sexual intercourse. censor Censoring status. parental_transition Binary indicator whether adolescent experienced parental transition (parents separated repartnered). parental_antisociality Composite score across four indicators measuring parents' level antisocial behaviour child's formative years.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/first_sex.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Age of first sexual intercourse — first_sex","text":"Capaldi, D. M., Crosby, L., & Stoolmiller, M. (1996). Predicting timing first sexual intercourse -risk adolescent males. Child Development, 67, 344–359. https://doi.org/10.2307/1131818","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/first_sex.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Age of first sexual intercourse — first_sex","text":"Capaldi, Crosby, Stoolmiller's (1996) original sample consisted 182 adolescent males applying exclusion criteria analysis; Singer Willett (2003) excluded additional two males data reported anal intercourse another male.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/health_workers.html","id":null,"dir":"Reference","previous_headings":"","what":"Length of health worker employment — health_workers","title":"Length of health worker employment — health_workers","text":"subset data Singer colleagues (1998) measuring length employment sample 2074 health care workers hired community migrant health centres. Health care workers followed 33 months termination employment.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/health_workers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Length of health worker employment — health_workers","text":"","code":"health_workers"},{"path":"https://mccarthy-m-g.github.io/alda/reference/health_workers.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Length of health worker employment — health_workers","text":"person-level data frame 2074 rows 3 columns: id Participant ID. weeks Number weeks termination employment. censor Censoring status.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/health_workers.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Length of health worker employment — health_workers","text":"Singer, J. D., Davidson, S., Graham, S., & Davidson, H. S. (1998). Physician retention community migrant health centers: stays long? Medical Care, 38, 11981213. https://doi.org/10.1097/00005650-199808000-00008","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/honking.html","id":null,"dir":"Reference","previous_headings":"","what":"Time to horn honking — honking","title":"Time to horn honking — honking","text":"subset data Diekmann colleagues (1996) measuring time horn honking sample 57 motorists purposefully blocked green light Volkswagen Jetta busy intersection near centre Munich, West Germany two busy afternoons (Sunday Monday) 1998. Motorists followed honked horns took alternative action (beaming changing lanes).","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/honking.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Time to horn honking — honking","text":"","code":"honking"},{"path":"https://mccarthy-m-g.github.io/alda/reference/honking.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Time to horn honking — honking","text":"person-level data frame 57 rows 3 columns: id Participant ID. seconds Number seconds horn honking alternative action. censor Censoring status.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/honking.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Time to horn honking — honking","text":"Diekmann, ., Jungbauer-Gans, M., Krassnig, H., & Lorenz, S. (1996). Social status aggression: field study analyzed survival analysis. Journal Social Psychology, 136, 761–768. https://doi.org/10.1080/00224545.1996.9712252","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/judges.html","id":null,"dir":"Reference","previous_headings":"","what":"Supreme Court justice tenure — judges","title":"Supreme Court justice tenure — judges","text":"Data Zorn Van Winkle (2000) long 107 justices appointed U.S. Supreme Court 1789 1980 remained position.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/judges.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Supreme Court justice tenure — judges","text":"","code":"judges"},{"path":"https://mccarthy-m-g.github.io/alda/reference/judges.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Supreme Court justice tenure — judges","text":"person-level data frame 109 rows 7 columns: id Justice ID. tenure Time retirement death years. dead Binary indicator whether justice died. retired Binary indicator whether justice retired. left_appointment Binary indicator whether justice left appointment. appointment_age Age time appointment. appointment_year Year appointment.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/judges.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Supreme Court justice tenure — judges","text":"Zorn, C. J., & van Winkle, S. R. (2000). competing risks model Supreme Court vacancies, 1780–1992. Political Behavior, 22, 145–166.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/math_dropout.html","id":null,"dir":"Reference","previous_headings":"","what":"Math course History — math_dropout","title":"Math course History — math_dropout","text":"Data Graham (1997) measuring relation mathematics course-taking gender identity sample 3790 tenth grade high school students. Students followed 5 terms (eleventh grade, twelfth grade, first three semesters college) stopped enrolling mathematics courses.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/math_dropout.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Math course History — math_dropout","text":"","code":"math_dropout"},{"path":"https://mccarthy-m-g.github.io/alda/reference/math_dropout.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Math course History — math_dropout","text":"person-period data frame 9558 rows 6 columns: id Participant ID. last_term term student stopped enrolling mathematics courses. woman Binary indicators whether student identified woman. censor Censoring status. term Term record corresponds . event Binary indicator whether student stopped enrolling mathematics courses given term.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/math_dropout.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Math course History — math_dropout","text":"Graham, S. E. (1997). exodus mathematics: ? Unpublished doctoral dissertation. Harvard University, Graduate School Education.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/monkeys.html","id":null,"dir":"Reference","previous_headings":"","what":"Piagetian monkeys — monkeys","title":"Piagetian monkeys — monkeys","text":"Data Ha, Kimpo, Sackett (1997) measuring age first demonstration object recognition sample 123 pigtailed macaques. Monkeys followed 37 days demonstrated classic Piagetian stage development known object recognition.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/monkeys.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Piagetian monkeys — monkeys","text":"","code":"monkeys"},{"path":"https://mccarthy-m-g.github.io/alda/reference/monkeys.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Piagetian monkeys — monkeys","text":"person-level data frame 123 rows 7 columns: id Monkey ID. sessions Number sessions monkey completed demonstrating object recognition. initial_age Age initial testing days. end_age Age end testing days. censor Censoring status. birth_weight Decile equivalent monkey's birth weight comparison colony-wide sex-specific standards. female Binary indicator whether adolescent female.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/monkeys.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Piagetian monkeys — monkeys","text":"Ha, J. C., Kimpo, C. L., & Sackett, G. P. (1997). Multiple-spell, discrete-time survival analysis developmental data: Object concept pigtailed macaques. Developmental Psychology, 33, 1054–1059. https://doi.org/10.1037//0012-1649.33.6.1054","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/opposites_naming.html","id":null,"dir":"Reference","previous_headings":"","what":"Opposites naming Task — opposites_naming","title":"Opposites naming Task — opposites_naming","text":"Artificial data created Willett (1988) measuring changes performance hypothetical \"opposites naming\" task four week period sample 35 people.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/opposites_naming.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Opposites naming Task — opposites_naming","text":"","code":"opposites_naming"},{"path":"https://mccarthy-m-g.github.io/alda/reference/opposites_naming.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Opposites naming Task — opposites_naming","text":"person-period data frame 140 rows 5 columns: id Participant ID. time Time measurement. wave Wave measurement. opposites_naming_score Score \"opposites naming\" task. baseline_cognitive_score Baseline score standardized instrument assessing general cognitive skill.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/opposites_naming.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Opposites naming Task — opposites_naming","text":"Willett, J. B. (1988). Questions answers measurement change. E. Rothkopf (Ed.), Review research education (1988–89) (pp. 345–422). Washington, DC: American Educational Research Association. https://doi.org/10.3102/0091732X015001345","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/physicians.html","id":null,"dir":"Reference","previous_headings":"","what":"Physician career history — physicians","title":"Physician career history — physicians","text":"subset data Singer colleagues (1998) measuring length employment sample 812 physicians hired community migrant health centres. Physicians followed 33 months termination employment. measurement window began January 1, 1990, ended September 30, 1992.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/physicians.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Physician career history — physicians","text":"","code":"physicians"},{"path":"https://mccarthy-m-g.github.io/alda/reference/physicians.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Physician career history — physicians","text":"person-level data frame 812 rows 8 columns: id Participant ID. start_date Date hire. end_date Date departure. entry Number years since hire physician worked entering measurement window. exit Number years physician worked departure. censor Censoring status. part_time Binary indicator whether physician worked part time. age Age time hire.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/physicians.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Physician career history — physicians","text":"Singer, J. D., Davidson, S., Graham, S., & Davidson, H. S. (1998). Physician retention community migrant health centers: stays long? Medical Care, 38, 11981213. https://doi.org/10.1097/00005650-199808000-00008","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/psychiatric_discharge.html","id":null,"dir":"Reference","previous_headings":"","what":"Days to psychiatric hospital discharge — psychiatric_discharge","title":"Days to psychiatric hospital discharge — psychiatric_discharge","text":"subset data Foster (2000) measuring relation number days discharge psychiatric hospital type treatment plan sample 174 adolescents emotional behavioural problems.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/psychiatric_discharge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Days to psychiatric hospital discharge — psychiatric_discharge","text":"","code":"psychiatric_discharge"},{"path":"https://mccarthy-m-g.github.io/alda/reference/psychiatric_discharge.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Days to psychiatric hospital discharge — psychiatric_discharge","text":"person-level data frame 174 rows 4 columns: id Participant ID. days Number days discharge. censor Censoring status. treatment_plan Binary indicator whether patient traditional coverage plan (0) innovative coverage plan (1).","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/psychiatric_discharge.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Days to psychiatric hospital discharge — psychiatric_discharge","text":"Foster, E. M. (2000). continuum care reduce inpatient length stay? Evaluation Program Planning, 23, 53–65. https://doi.org/10.1016/S0149-7189(99)00037-3","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/reading_scores.html","id":null,"dir":"Reference","previous_headings":"","what":"Peabody Individual Achievement Test reading scores — reading_scores","title":"Peabody Individual Achievement Test reading scores — reading_scores","text":"subset data Children National Longitudinal Study Youth measuring changes reading subtest Peabody Individual Achievement Test (PIAT) sample 89 African-American children ages 6, 8, 10.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/reading_scores.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Peabody Individual Achievement Test reading scores — reading_scores","text":"","code":"reading_scores"},{"path":"https://mccarthy-m-g.github.io/alda/reference/reading_scores.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Peabody Individual Achievement Test reading scores — reading_scores","text":"person-period data frame 267 rows 5 columns: id Participant ID. wave Wave measurement. age_group Expected age measurement occassion. age Age years time measurement. reading_score Reading score reading subtest Peabody Individual Achievement Test (PIAT).","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/reading_scores.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Peabody Individual Achievement Test reading scores — reading_scores","text":"US Bureau Labor Statistics. National Longitudinal Survey Youth (Children NLSY). https://www.bls.gov/nls/nlsy79-children.htm","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/rearrest.html","id":null,"dir":"Reference","previous_headings":"","what":"Days to inmate recidivism — rearrest","title":"Days to inmate recidivism — rearrest","text":"Data Henning Frueh (1996) measuring measuring days rearrest sample 194 inmates recently released medium security prison. Inmates followed three years rearrested.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/rearrest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Days to inmate recidivism — rearrest","text":"","code":"rearrest"},{"path":"https://mccarthy-m-g.github.io/alda/reference/rearrest.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Days to inmate recidivism — rearrest","text":"person-level data frame 194 rows 7 columns: id Participant ID. days Number days rearrest. months Number months rearrest, scale \"average\" month (30.4375 days). censor Censoring status. personal Committed person-related crime property Binary indicator whether inmate committed property crime. age Centred age time release.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/rearrest.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Days to inmate recidivism — rearrest","text":"Henning, K. R., & Frueh, B. C. (1996). Cognitive-behavioral treatment incarcerated offenders: evaluation Vermont Department Corrections' cognitive self-change program. Criminal Justice Behavior, 23, 523–541. https://doi.org/10.1177/0093854896023004001","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/suicide_ideation.html","id":null,"dir":"Reference","previous_headings":"","what":"Age of first suicide ideation — suicide_ideation","title":"Age of first suicide ideation — suicide_ideation","text":"subset data Bolger colleagues (1989) measuring age first suicide ideation sample 391 undergraduate students aged 16 22. Age first suicide ideation measured two-item survey asking respondents \"ever thought committing suicide?\" , \"age thought first occur ?\"","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/suicide_ideation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Age of first suicide ideation — suicide_ideation","text":"","code":"suicide_ideation"},{"path":"https://mccarthy-m-g.github.io/alda/reference/suicide_ideation.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Age of first suicide ideation — suicide_ideation","text":"person-level data frame 391 rows 4 columns: id Participant ID. time Reported age first suicide ideation. censor Censoring status. age Participant age time survey.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/suicide_ideation.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Age of first suicide ideation — suicide_ideation","text":"Bolger, N., Downey, G., Walker, E., & Steininger, P. (1989). onset suicide ideation childhood adolescence. Journal Youth Adolescence, 18, 175–189. https://doi.org/10.1007/BF02138799","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/teachers.html","id":null,"dir":"Reference","previous_headings":"","what":"Years to special education teacher turnover — teachers","title":"Years to special education teacher turnover — teachers","text":"subset data Singer (1993) measuring many years 3941 newly hired special educators Michigan stayed teaching 1972 1978. Teachers followed 13 years stopped teaching state.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/teachers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Years to special education teacher turnover — teachers","text":"","code":"teachers"},{"path":"https://mccarthy-m-g.github.io/alda/reference/teachers.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Years to special education teacher turnover — teachers","text":"person-level data frame 3941 rows 3 columns: id Teacher ID. years number years teacher's dates hire departure Michigan public schools. censor Censoring status.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/teachers.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Years to special education teacher turnover — teachers","text":"Singer, J. D. (1992). special educators' careers special? Results 13-Year Longitudinal Study. Exceptional Children, 59, 262–279. https://doi.org/10.1177/001440299305900309","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/tenure.html","id":null,"dir":"Reference","previous_headings":"","what":"Years to academic tenure — tenure","title":"Years to academic tenure — tenure","text":"Data Gamse Conger (1997) measuring number years receiving tenure sample 260 semifinalists fellowship recipients National Academy Education–Spencer Foundation PostDoctoral Fellowship Program took academic job earning doctorate. Academics followed nine years received tenure.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/tenure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Years to academic tenure — tenure","text":"","code":"tenure"},{"path":"https://mccarthy-m-g.github.io/alda/reference/tenure.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Years to academic tenure — tenure","text":"person-level data frame 260 rows 3 columns: id Participant ID. years Number years receiving tenure. censor Censoring status.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/reference/tenure.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Years to academic tenure — tenure","text":"Gamse, B. C., & Conger, D. (1997). evaluation Spencer post-doctoral dissertation program. Cambridge, MA: Abt Associates.","code":""},{"path":"https://mccarthy-m-g.github.io/alda/news/index.html","id":"alda-0009000","dir":"Changelog","previous_headings":"","what":"alda 0.0.0.9000","title":"alda 0.0.0.9000","text":"Added NEWS.md file track changes package.","code":""}]
